## Aven Zitzelberger's Lab Notebook

### Project Description:
You will work to develop and run experiments within a Psychophysics Lab. The lab is a sensor embedded room (many cameras, microphones, temperature sensors, etc.) where individual activity is characterized and streamed to a centralized server for research and analysis. For this project specifically, you will build (or aquire) an Electroencephalograph (EEG) headset and extend this repository to live stream EEG data to the sensor data hub. You will then develop machine learning models that use the EEG data stream to predict human activities as measured by the other sensors.

### Contact Information
* Mohammad Ghassemi, ghassem3@msu.edu, 617-599-6010
* Aven Zitzelberger, zitzelbe@msu.edu, 248-404-5522, 919 East Shaw Ln. (East Holmes Hall)


### Specific Tasks:

1. [Due June 1st] Student will review and familiarize himself with the existing Ghassemi lab data-hub codebase. Student will ensure they can launch the data hub, stream data to the database, and visualize data in the front end.
2. [Due July 1st] Stream EEG data from the Open BCI device to the datahub, and visualize data from EEG and other sensors concurrently.
3. [Due August 1st] Collect data on proof of concept visual stimulus task (TBD), and train machine learning model to reverse engineer visual stimulus from the raw EEG data.


Additional tasks will be added at a future date.

### Project Expectations:
By working on this project you are agreeing to abide by the following expectations:

1. You will keep a daily log of your activities in this notebook. Your update should be 1-2 sentences that outlines what has been accomplished that day, and must be commited by the end of the day. 
2. You will provide a weekly email update, every Monday, to Dr. Ghassemi detailing 
    * What was accomplished the previous week,
    * Any issues you faced, 
    * What you plan to accomplish in the coming week.
3. You will keep all code, data and any other project-related items within this repository in neat and professional condition; this includes: 
    1. keeping the code commented, 
    2. naming scripts in a way that reflects their functionality, 
    3. making regular code commits with meaningful commit messages and,
    4. organizing the contents of the project into a logical directory structure.

### Daily Updates

##### Sept 3rd, 2021:

​	Finally got it. I still don't know the cause of the error, but my guess is that it has something to do with the fact that SoundFile creates a new file object with each call to sf.write(), which may cause issues with opening it multiple times. Instead, the solution was to create a single SoundFile object first and then in the callback function use file.write() instead. This got rid of the error, but I still had to rewrite my OutputBytes object to include seek() and tell() to satisfy SoundFile's standards for what constitutes a valid file-like object. I just did this by extending the BytesIO object from the built-in io module. This came with a bug that took me awhile to locate where the read() operation wasn't returning anything - turns out I just had to add a seek(0) beforehand because I am now using BytesIO read() rather than getvalue().

​	The next step is to transmit this data to the browser to be played. First, I need the browser to have access to the database column with the audio data, then I will need a way to play it simultaneously with the video.

##### Sept 2nd, 2021:

​	I haven't actually tested the Pi stream on my new Wifi network before, so I did that this morning. Worked perfectly! This is a great sign since I have previously only ever been able test on my wifi network in Ohio, so now I know I didn't just make everything work only for that network. I don't know how I could have done that in the first place, but there is a lot I don't understand so who knows.
​	After testing the various streams (Video, Sense Hat, Synth EEG, Test Stream 1), I added some better error messages to the streams that don't have a Transformed analyzer assigned to them. This is because all Bokeh streams get the dropdown menus, but that doesn't mean there is a Transform analyzer running that can process the data. If this occurs, an error is shown in the console log of the browser and the dropdown menus are not populated with functions.

​	I then wrote a new AudioStreamer class similar to the code I wrote yesterday. The first problem I ran into was a question of how to write the data to the database. The audio is being sampled at 44100 Hz, but of course I don't get timestamps for every single one. Rather I get a block of data with a timestamp for the beginning of that block. My temporary solution is just to create a timestamp array that is the same size as the audio data with the given timestamp in every entry - this gets around the requirement that all data columns have the same length. Eventually I think I want to create a new format to store data that allows this 'chunking'. For now, at least, I just want a way to transfer the data to see that it works. 
​	The next problem is that this timestamp is completely arbitrary and not synchronized with the current epoch time. To get around this I had to calculate the difference between the current relative time and the time that the data block was taken, then subtract this difference from the current time measured by time.time(). This gives the epoch time that the block was taken.
​	Next problem is how to play this data in the browser. Sounddevice outputs the raw data as numpy arrays, but I need a way to compress, store, and play back this data in a browser. Currently I am attempting to use the SoundFile package to do this, writing to a file-like object, but there is a myriad of errors that seem to be nonexistent for anyone else on the internet. For example, when trying to open by BytesIO object with SoundFile, I get ```RuntimeError: Error opening <_io.BytesIO object at 0xb45127e0>: Unspecified internal error.```, which of course tells me nothing and also seems to be a nonexistent problem when I tried searching it. You know you're screwed when the third search result for an error is just the source code. I'm done for tonight, but tomorrow I think I'll just try the wave module instead. If that doesn't work, there's always scipy.io.wavfile.

##### September 1st 2021:

​	After speaking with Dr. Ghassemi this Monday, he wants me to get audio streaming working for the project because the ICASSP conference has a focus on audio processing. I have put this off in the past because I hate dealing with audio drivers. And sure enough, as soon as I started trying I ran into a plethora of issues. 
​	First, I started by using the pyaudio python library along with a lavalier mic that I bought on amazon. First I made sure the mic worked on my PC, then wrote some basic code to record audio from it to also run on my PC. It worked perfectly. However, when trying the exact same code on the Raspi, I got a screen full of errors. Most of the errors mentioned ALSA, which is about what I expected, but what I didn't expect was errors relating to X11 forwarding of the ssh session. What???? I googled in vein to try to find a solution to these, but I was getting absolutely nowhere. Some sources (like this one https://www.raspberrypi.org/forums/viewtopic.php?f=66&t=241814) suggest adding an environment variable PA_ALSA_PLUGHW=1 in an attempt to silence the error outputs, but this didn't help me because unlike the issue being reported in this thread, my recording wasn't even working. 
​	At some point I got frustrated and decided to instead try a different python package - sounddevice. I repeated the process of  testing first on my PC, and again it worked perfectly. I also was able to run it on the Pi (yes!), and I did not see any ALSA-related errors. However, I was still getting the X11-forwarding error stating that the connection to the X11 server was refused. Again, googling did not help as I literally cannot find another instance of this happening ANYWHERE. How am I the only one with this issue???? Clearly this is a problem with the fact that I am using an ssh session, but there is no reason it should be even trying to establish an X11 server, because I am not playing sound, only recording it. To test this I hooked up the Pi to my monitor and ran the program directly from command line, and it worked just fine. While this was a relief, I still needed to fix the ssh issue. I went into the Putty settings and disabled X11 forwarding, but then every attempt to ssh into the Pi from then after resulted in a frozen prompt. Restarting the Pi and my PC, and re-enabling X11 did not fix it. I am quite literally still unable to ssh into that Pi. SIGH. So I went and booted up a different Pi to try again, and it just worked for that one somehow. I am beyond trying to understand why at this point, I just need it to work. On the bright side, using sounddevice instead of pyaudio does have some benefits, most notably the fact that it outputs directly to numpy arrays which is perfect.

​	Next I need to turn this toy script into a continual recording loop that can be started and stopped externally. Sounddevice provides a method for doing this, so I rewrote it in that format only to discover that it now does not work on the raspberry Pi in this mode. It works this way on my PC, but when the same script is run cddat	on the Raspi it returns no audio. I know for sure that this time it is not a problem with ssh because I also tried it directly from the Pi itself and got the same result.
​	Finally, I tried the callback method that is also used in the docs for sounddevice and got it to work on the Pi. Next step is to integrate this code into the application, which should be easy assuming that the multiprocessing doesn't get in the way. Then after that, I need to figure out how to play the sound in the browser. That, I predict, will be much harder.

##### August 20th, 2021:

​	I've been doing a lot of brainstorming on a way to rewrite the interactive display in such a way the reduces the amount of programming knowledge that would be required. Firstly, the necessity of writing a single python loop to read data from an external device is, at the moment, unavoidable. I have written various examples to help someone do this, but because one of our goals is to make this app compatible with anything that can interface with python, that step is necessary.
​	However, the server-side of this has been giving me headaches because I keep getting the nagging feeling that it can be done without writing code, or even a config file. Because of this, I would like to start designing an interactive interface on the website that does this instead, and here's my idea:
​	A section of the website dedicated to displaying all the data columns currently being streamed to the database will be displayed. Each data column should have the option to assign that column to a "display" (a separate webpage to view the data, like I we have now). When a "display" is created and named, it should appear in a dropdown menu for each data column to choose to be assigned to (or multiple displays too, that would be nice). Once a "display" is created, an html page should be selected to render it - this means either my pre-written page that displays Bokeh plots, or something custom made like my video stream html and associated JS code. Then, if the page was the bokeh template, an actual bokeh layout object needs to be specified in a similar way. Then, if the right data columns in the database are assigned to the page with their intended bokeh layout, one can view the page and see exactly what I have implemented now, which is the fully functional and interactive bokeh design of the live streams.
​	I really like the idea of being able to dynamically change which bokeh plots each data column gets displayed in rather than writing everything in python before starting the server and hoping that you did it right. It also means that someone with no understanding of programming would be able to configure the data visualization component completely independently and intuitively.
​	This is probably going to take a lot of work, and since I am going to be spending the next week moving to a new apartment I probably won't be able to make any progress just yet, but I want to get this idea written down so I can keep workshopping it.

​	I found a really cool online tool to write html/css/js and get live feedback without having to go through all the trouble of deploying the server each time: https://jsitor.com/
It works really well and I have not had any problems with it so far - I'm using it to write the webpage that I described above. I'm making slow progress but it's turning out pretty well so far.

##### August 19th, 2021:

​	I think I just needed some sleep - I found the problem. Even though the Nginx config redirects traffic from port 80 to port 443 for SSL, that redirect only works for browsers that know how to respond to redirects. SocketIO doesn't use redirects, so it has to start right off the bat by using port 443. That was the problem. It's a little frustrating that this wasn't the error that was thrown (the version number thing from yesterday), but I guess I can't expect it to be that easy.

​	Another thing I've been working on is re-organizing the method by which streams are created. A couple days ago (forgot to mention) I rewrote the run_analyzers.py file and moved the customization section to the server directory. The interface is now constructed using an Interface() class object that defines the group pages that are to be used by the Flask app. This method is a bit clunky, I feel, as it relies on this file being in the location that it is and not imported from somewhere else. This is because the Flask app imports the interface object from that particular file. I tried to reduce this configuration down to a single file rather than separately between interface config and stream config, but ultimately I decided against it because the stream config technically could be for a completely different device. I intentionally made the streamers and analyzers for the server modular like this to allow for the future possibility of hosting the site and the analysis on separate machines.

##### August 16-18th, 2021:

​	On Monday, Dr. Ghassemi instructed me to work on making the website more secure using SSL. The first step involved getting Nginx functioning as a reverse-proxy server for the Flask app. It took me a lot of documentation reading but I was able to get it working, serving static files directly and passing everything else to the Flask server, which is no longer hosted publicly but rather on localhost:5000. The site is now serving on the typical port 80 instead. A resource that really helped me is Miguel Grinberg's presentation at PyCon in 2016: https://www.youtube.com/watch?v=tdIIJuPh3SI

​	The next step was to get it SSL encrypted, but I soon discovered that in order to do that we needed to purchase a domain name. After talking with Dr. Ghassemi, we ended up with "signalstream.org". I used CertBot to validate the SSL certificates and used the toy example that Miguel made for his presentation (https://github.com/miguelgrinberg/flack) as a template to create the Nginx config with SSL (although it was actually syntactically invalid so I had to debug it, which was a pain since I've never used it before this week). There were a couple of config options that seemed to break everything, so those are commented out for now and I'll ask Dr. Ghassemi about them later.

​	After that, I quickly ran into some issues with socketIO conflicting with the SSL connection. First, there was some modifications that I had to make to the Flask app initialization that I would have never known about if it had not been for this git thread: https://github.com/miguelgrinberg/Flask-SocketIO/issues/1047
I also had to go around to various places in my code to make sure that I was using the correct prefix for the site as well as the domain. Evidently socketIO can't connect using the IP address alone.
My current problem is that socketIO is throwing an error indicating that there are some mismatched version numbers, but it doesn't tell me where. Going to work on that tomorrow.

##### August 12th, 13th, 2021:

​	I have now implemented a file upload, however I am not yet convinced it is safe. I have disabled it for now until I can consult Dr. Ghassemi on Monday about it. I added a button on the index page that opens a dialogue with a file submission form. A file can be selected from the computer and uploaded to the server. Basic filename verification is in place.
​	I initially implemented the file upload via a regular HTTP form submission, but I was having trouble preventing the form from reloading the page every time. Using preventDefault() didn't help because that prevented the entire form form submitting at all. After looking around for a bit, the consensus online seems to be just to do it manually with Ajax. I redid it using Ajax and a JS FormData object, and that seemed to do the trick, though I had a bunch more problems to deal with. It turns out that when making the AJax request, the contentType and processData headers MUST be false in order for a file upload to work. Took me forever to figure out that this is the case, and I still don't know why - only that I get errors if they aren't there.
​	After implementing it this way, I decided that I wasn't happy with it because I have been trying to move various server functions to SocketIO instead of HTTP requests because then it makes logging errors and other messages much much easier on the server end. I was able to figure out how to extract the file contents from the FormData object, and was able to send that along with the file name through a socketIO request. This works great and is a lot cleaner on the server side.

##### August 11th, 2021:

​	I went ahead and implemented that idea I had yesterday about plotting both the original and transformed data in the same plot. I think the result is really cool, and it also got me to so some organizing regarding the CustomJS callback functions that I use for the other streams. 

​	This also lead to me FINALLY fixing this really annoying issue where the Bokeh plots on the EEG and ECG Filtered stream would zoom in until the data was no longer visible and throw a bunch of JS errors. It turns out it was due to the way that I originally solved the plot-sliding issue like last year. The basic idea is that Bokeh plots, by default, update their axes when new data is added. This means that I can't slide the axes to smooth out the incoming data chunks, so I needed to find a way to disable the automatic adjustment. The recommended way (from the Bokeh forums) is to just define a bokeh.models.Range1d with some arbitrary values for your axes. This disables the auto-adjust feature, and then the axes can be manually changed whenever needed. However, when I first did this I just set the Range1d start and end values to (0,0), which apparently throws errors in some very specific circumstances. So now I just set it to (0,1) instead. Fixed!

​	Alright so at this point I've got the first two bullet points completed - python files stored in the local/pipelines directory can be selected in a stream using a basic dropdown interface. The next step is less important as it isn't required for a live demonstration, but I'd like to be able to upload python files. I am nervous about this, however, because if someone managed to get access to it they could obviously execute arbitrary code on the server which would be super duper bad. So before I do that I want to do some research on the best ways to prevent access to pages with Flask and if there are any serious security concerns.

​	I have implemented a better authentication method using Flask's SECRET_KEY, so I believe it should be secure. However, I am still not convinced, so I'll ask Dr. Ghassemi what his thoughts are.

##### August 10th, 2021:

​	So I ran across a really annoying problem with python today. It started when I was working on a way to import the arbitrary functions stored in files. I'm using an exec() statement to dynamically import the functions from the different python files given by the user. However, there seems to be some VERY inconsistent behavior with the exec function, though it took me awhile to figure out that it was unrelated to the import statement itself. The behavior I am expecting is that variables created and modified inside the exec statement are available outside the exec statement. In fact, this is exactly the intended behavior, and it does work that way IF the exec statement is in the global scope. However, it does not work that way if the exec statement is in a local scope, for some unknown reason. In this case, there seems to be a problem where the index of local variables is not updated properly, and one must manually update it by hand in order to capture the changes made from inside the exec statement. Moreover, this behavior changes depending on whether a call to locals() happens before or after it is accessed outside the exec function, and even changes depending on whether the code is being run in debug mode.
​	I have indeed been searching online about this, and there are many answers regarding how the exec function can be passed global and local variables, but none explain the behavior that I am seeing. I am completely baffled.

​	All in all, I've managed to get the transfer of information about which pipelined functions have been activated and in what order organized on the server and the browser, and the information is stored in the database under it's associated group. My task for tomorrow will be to make the Bokeh plot transition from reading the raw data to the transformed data after a transformation has been applied. This is different from my EEG and ECG plots because they never actually read the raw data, only the filtered data from the FilterAnalyzer. This, however, should be applicable to any data stream regardless of where the input data is coming from, so the plot should be able to switch between the original input and the transformed data.

​	An additional idea I had was to have the original data AND the transformed data plotted on the same graph, with the original data less visible. I think this would be really good to showcase the transformations in action. In order to do this, I will need to change the Bokeh layout code to include this IF the appropriate data column contains data - the problem is that this makes creating Bokeh layout more complicated for someone else who might want to do something similar. However with the current deadline for the IEEE paper I think it's best to make it function first, then work on organization afterward.

##### August 9th, 2021:

​	After talking with Dr. Ghassemi today, we've decided that the best use of time for the next two weeks is to implement the following feature:
​	We want the ability to upload an arbitrary file containing python code to the browser. The files should then be able to be selected during a stream and act as a black box through which data is sent through with an Analyzer. This would allow any arbitrary algorithm (for example a trained ML network) to act on the incoming data. In order to accomplish this, I've laid out some goals for myself this week:

- Create an analyzer that reads functions from files in a particular directory which it passes the data through.
- Add the ability to select these files in any order from the browser.
- Add the ability to upload arbitrary files to this directory from the browser.
- Make absolutely certain that these files don't have sudo privileges. That would be bad.

Immediately a couple things came up. First, what if import statements are used in this arbitrary file? Do those need to be confined to the inside of the function? If that is the case, then the module would need to be loaded in every single time a data chunk needs to be fed through it. What would be best is to load all modules in beforehand to skip this, though I'll worry about that later. 

​	For the rest of the day I worked on the interface to work with this structure. I wanted a way to add as many pipelined functions as needed, but in order to do that I had to keep track of the order in which they appear on the screen. This is then saved to the database which will be loaded back to the browser window every time after (I'll work on the backend for that tomorrow).

##### August 5th, 2021:

​	The goal for today will be to make a new feature for the app that reads data from a given Redis database and feeds the whole thing into a black box algorithm (will eventually be ML training). There are a lot of things that would be useful for this - for example, the ability to trim datasets to get rid of unhelpful stuff at the beginning or end. However given the time constraint I think those may have to wait, and I can trim the data manually through the redis CLI. 
​	I am also struggling to figure out how I want to implement this feature - should it be activated by a button in the control center? Eventually, yes, but right now I think that might require too many different options, like which data column to target, what algorithm to feed it though, a mechanism to prevent further action while running, etc... Maybe it's just better to make it a standalone script for now? I think that might be the quickest way for the time being. I'll get started on that first.

##### August 4th, 2021:

​	Today I am testing a long streaming session with just the ECG with the OpenBCI GUI to see if the square wave appears. I am doing this to try to see what the origin of the issue is - whether it's a bug in the Cyton or my app.

​	Also Dr. Ghassemi asked me to make some PPT slides for him to present to showcase the app, including a video demo. This motivated me to refine the smoothing feature on the EEG and ECG streams. I was able to make the data from each channel scroll much smoother, but at the cost of seeing when chunks of data are replaced. I didn't want to spend too much time on it as it will become obsolete once I switch to a Bokeh server.

##### August 3rd, 2021:

​	Today and yesterday I've just been collecting data. I've had a few hiccups here and there, and they seem to be related to sudden disconnections to the server or Raspis. I don't think it's an issue with the app itself because every time it's happened, I have also lost my ssh connection at the same time, so it's likely a network thing. What I still need to figure out, though, is how to recover from it automatically. The problem is that I still don't know what's going on. I checked the logs afterward and there were no errors, just a sudden disconnect. 

​	However, since I am only streaming EKG, EEG, and Sense Hat data, the resulting memory used is much smaller than before. I am currently at 90 minutes of data and using 800MB. 

​	At about the 100 minute mark I checked the EEG, and it was doing the square wave thing again. I don't know how long it had been doing it - maximum 20 minutes. I restarted the streams, and that seemed to fix it. Still don't know the cause of this, but I do not think it has anything to do with the signal analysis. I would like to verify this behavior with the OpenBCI GUI, but then I can't do both EEG and EKG at once.

​	Another thing I noticed is that I sorely need the ability to load the set Filter settings between sessions into the browser. The analyzer keeps the settings as long as the python process remains active, but the Bokeh layout doesn't load the widget values.

​	Roughly 45 minutes into the next dataset, I noticed that the FP1 EEG channel went dark. I figured maybe the gel had dried out, so I added some more but it didn't help. It also wasn't getting any noise so I'm not sure what the deal was. I'm going to leave it for now and see if it comes back when I restart the Cyton for the next data set. After 90 minutes in this second dataset, more channels went dark: T7, C4, P3, O1. I will stop at 2 hours and restart the stream.

​	After restarting the stream (which cycles the Cyton board's streaming states), the channels all came back. This is another thing I would like to check using the OpenBCI GUI to see if it is a problem related to my particular setup.

​	After 40 mins into the 3rd dataset, the square waves came back, this time on the ECG. Ugh. I restarted again, and after 15 minutes the mysterious sudden-stop thing happened. Again, nothing in the logs and the ssh sessions were terminated at the same time. This is very frustrating.

​	Overall, I have collected about 6 hours of data today. I think I'll call it for now and continue tomorrow. I want to try to add some better logging to maybe catch what is happening with these sudden stops, so we will see how that goes.



##### July 31st, 2021:

​	Today I started collecting real data with EEG and ECG - I've decided not to do video for this first data set. I had my grandma inject the electrode gel into the head cap, but it took a bit of work to get the amount of gel right so that the connections were solid. 
​	Most of the problems that I see are just related to noise in the signals - when I use my computer mouse I see intermittent high voltage spikes in the EEG data. I believe this is most likely electrical signals from the mouse, not anything that my brain is doing. Although, turning my head does not produce any discernable noise, probably due to the fact that the electrode cap is very well secured. It is also quite comfortable too, which is great. I see no problem with wearing it for a number of hours, though I suppose we will have to see.
​	Another anomaly I saw was that at one point the Cyton board started transmitting a square wave through all channels, and I had to restart it. This happened once independently for each board - I have no idea what is causing it.

​	I got finally got about 45 minutes into the stream when my family decided that it would be a great time to start a family zoom call and completely saturate the upload bandwidth of the network. Great. I guess this was just a preliminary test stream.

​	I was not able to collect any more data today - the network continued to be slow for the rest of the night. I left the cap on for over 7 hours, though, and noticed that the wet electrodes dried out considerable after about 4-5 hours. I'm not sure how this would affect the EEG, but I will try again tomorrow.

##### July 30th, 2021

​	Finally was able to fix the downsampling issue. I still haven't yet implemented a way for the database to know the exact sample rate of a dataset and downsample accordingly because I think there is too much to analyze - for example, the EKG data cannot be downsampled as much or risk losing the waveform as I just discovered. But the EEG data, on the other hand, while using the same sample rate, can be downsampled further because the visual inspection of the data does not require high resolution of the high frequencies in the FFT. I think the best way to handle this might be to add an interactive choice of how much downsampling to apply, though maybe with a global cap to prevent overwhelming the database and browser. Maybe further down the line, a better solution might be to allow zooming in on portions of the dataset, which could then be retrieved at higher resolutions. Implementing this would be tricky, because it requires information from the Bokeh plot to be transmitted back to the server, which (at the moment) I don't believe is even possible. It remains to be seen, however, if that is something I could do with a Bokeh server, though I suspect not. If it turns out not to be possible in either case, the only thing I can think of would be to add some buttons that interact with the SocketIO in the browser to transmit different sets of data at different resolutions when paused, but I have a gut feeling that that would be super jank and not very stable.

​	Now I'm just sitting back and passively collecting data. I've thought about doing a sample where I look at different colors and press the appropriate button on the Pi, though I don't know how well that will turn out. I suppose we shall see.

##### July 29th, 2021: 

​	I added a method to round the floats in write_snapshot() to the same precision as the other write methods. I don't think this will have as much of an impact since those value will still be stored as a comma-separated string, but it will at the very least reduce the string size. 

​	I am now collecting another 20-minute data set to test the changed I've made on the real EEG and ECG streams. One thing I'm noticing is that the float truncating has actually eliminated the small-value noise that occurs when a signal is railed, and the data comes through as just all 0, which is an unexpected bonus. 

​	I went ahead and added another status display on the control page to show the total memory used by the database. This will be useful in making sure that we can manually stop the stream when it gets too big.

​	I started to collect another dataset, but somewhere around 30 minutes, the streamers stopped transmitting. When I issued the "Stop" command from the browser, the browser's Socketio connection disconnected. Upon reloading the page, the SocketIO connection returned, but the same thing happened. This never stopped. Also, my ssh connections to the server and the Pi's dropped, so I wasn't able to see any error messages. This prompted me to create a log file on the server to dump all errors to if this happens again. I have no idea what caused this or how to go about troubleshooting it - I think I just have to wait for it to happen again and review the logs this time.

​	I also had another idea on how to restructure the Streamer interaction with the Analyzers when updating and connecting to the server. Awhile back I was having trouble with this organization and just decided to have the streamers send a notification whenever they connect or update, and all analyzers are notified and they have to check to see if the connected streamer was one of their targets. Instead of this, I could have each streamer (and analyzer) join a room uniquely identified by the group and name of that stream. Then any analyzers that target that stream can just join the room and get immediate updates whenever that target updates its info. I believe this would make the organization MUCH nicer and avoid the potential timing problems that the current method introduces. I will not try to implement this now, I just want to make note of it.

​	While recording another data set, I noticed the total-memory-used metric is slightly disingenuous, as it displays the total memory usage of Redis alone out of the total memory available in the system. Importantly the memory used does not include that of any other processes, like the many python processes running alongside. This means that the total memory "available for the Redis server" is less than that displayed. To fix this, should we display the total memory used on the system as a whole, not just by Redis? Or maybe display Redis's memory usage, but only display the memory not being used by other processes? Not a big deal, but something to tweak in the future.

​	I was able to record about an hour of data before my internet connection did the thing again. Looks like I'm done for the night. The hour of playback only took about 1.4GB, so I think the optimizations I've implemented have heavily payed off. Not quite as much as I'd like, though.
​	While reviewing the data, I noticed in playback that the waveform if the ECG was not discernable due to the downsampling I implemented. I may need to revisit that and adjust how much downsampling is performed. I'll start working on that tomorrow.

##### July 28th, 2021:

​	Alright I finally got the Sense Hat button working. The trick was to use the sense.stick.get_events() method rather than setting trigger methods like I was trying to do yesterday. get_events() just gets a list of all events taken since the method was last called, which I can then parse and write to the database. The joystick can be pressed in 5 different ways: Up, Down, Left, Right, and Middle (just pressed head-on). This data along with timestamps of when the button was pressed and a color-code is sent to the server and stored. In the browser, I wrote a quick Bokeh plot that displays the times of each button press and the color associated with that button press. Seems to work well enough, though the hover tooltip is annoying to use because the data keeps shifting as time moves forward. Also the total size of the time-window that the Bokeh plot uses is based on the number of data points, not a fixed length of time. This isn't a problem for consistently sampled data, but for the button presses it looks a little wonky. There doesn't seem to be a way to use an AjaxDataSource to fix the x-range to a specific time window without updating it constantly in the browser, so I just decided not to mess with it. Instead I just added a constant invisible line which takes data from the other Sense Hat sensor data to keep the spacing consistent.

​	I also fixed an unrelated bug where the EEG Fourier Analyzer would send twice the amount of data to the Headplot stream if the stream was started and stopped. The data for the headplot is insignificant, thought, to it was more for the sake of clarity.

​	Next I wanted to do some testing with regard to the Redis integer and float data type to see if using the method that Dr. Ghassemi suggested on Monday will make a difference for Redis. I did a quick test comparing the space of an integer vs a float, and it looks like Redis dynamically changes the memory used based on the size and float value, though the float does appear to be roughly 5-10 bytes larger than an integer, depending on the precision. The space used by an integer (1 to 5 digits) was 48 bytes, while the space used for a float with 1 decimal place was 53 bytes. Increasing the precision of the float to 5 decimal places increased the memory usage to 57 bytes.

​	After this I collected a baseline for file size - I ran a recording session with an EEG, Sense Hat, Test Group, Synth EEG, and Video for 20 minutes, 3 times. I wrote down the recorded memory usage of each stream in order to have a point of comparison for making optimizations. I discovered that while the EEG data was taking up a significant amount of memory, the largest was actually the video data. This confused me for awhile because I know for a fact that the raw size of the video data in bytes is smaller than that of the EEG data - I measure it out myself not too long ago when investigating network speed issues. However after researching for awhile, I believe the reason is because Redis heavily optimizes numerical data, and cannot optimize the pure bytes data that the video is stored in, resulting in it taking up much more space. As a temporary solution, I will cut the framerate of the video in half down to 10fps, and reduce the resolution from 300x300 to 200x200. Theoretically, this will reduce the size of the video feed by about 80%.

​	After collecting two more 20-minute data sets with the optimizations in place, everything was as expected. The video stream was indeed reduced by about 70-80%, and all the numerical streams were reduced by ~20%. However, one thing that I found is that this method does not at all affect the FFT data due to the method by which I am storing it in Redis. The FFT data is not the same as the time-series data, but rather has data points which contain an entire spectrum of data rather than a single value. Because of this, I am storing the numerical data as a comma-separated string, and that entire string comes with an associated time stamp. The problem with this is that redis cannot optimize this for the same reason it cannot optimize the video data. The result is that those values are still stored as their full floating-point value strings in redis, and was not reduced when I recorded the memory usages statistics. I have two options for this: I could round the floats before they are stored as a string in redis, or find a way to store them in redis as time-series data so each point can be an integer
​	Tomorrow I will start working on that then see if I can collect more than 2.5 hours of total data.

##### July 27th, 2021:

​	To begin, I added another button to the control center: "Wipe." This manually wipes the contents of the current database file, if live. I need this to clear the current database in the event that the automatic wipe fails, as was the case for the large database file I created yesterday. I also added some clarity to the log messages on the browser, differentiating between the 4 different log levels: log, info, warn, and error. Each has a different color and font weight.

​	I then went to try and configure the Raspberry Pi's Sense Hat to transmit the data from its joystick as a method of categorization, but I'm having a lot of trouble getting it to work with my multithreaded and multiprocessed system. For some reason the event triggers just don't go through to the separate process that the SenseStreamer is running on. I've been working on this for awhile, but I think I have an idea to try tomorrow.

##### July 26th, 2021:

​	I added a hover tool to the EEG headplots that shows which channel each coordinate corresponds to - will be helpful in determining how to correctly rewire the electrode cap. 

​	I am also having trouble with the two USB receivers switching around which device port number they are assigned to, meaning that occasionally the EEG and ECG data will be swapped and feeding data to the wrong database column. After looking online, there seem to be methods of hard-coding a device's serial number or other identifiers to a specific device ports, but that won't work for any given usb device. What I need is a way to just have the Pi keep the same port numbers each time it reboots. Not yet sure how to do this, but for now it's easy enough to just unplug and re-plug the usb devices in the right order.

​	I then began re-wiring the Cyton board to the electrode cap by looking at which headplot reactions when I plugged in the connection. Once I did that I decided I might as well test a long stream, so I went for about 2.5 hours. When I ended the stream, the process of saving the file to disk took over a minute and the gunicorn worker timed out. I believe that gunicorn automatically starts another worker. However, this shouldn't affect the file save, and indeed it did save the file in it's entirety. The other problem I had was loading the file into memory - it took almost 45 minutes. Once loaded, it was taking up 7 GB of the available 8 GB on the server, and during the time it was loading the server itself was completely unresponsive. This is a big problem.
​	I have a few ideas: First, Dr. Ghassemi offered some advice today on reducing data size. It may be possible to store the numerical data as a multiplied integer, then later convert back to floats. If redis does indeed use integer types, this would be a great way to reduce size and read times. I will definitely try this out first. Another idea I have is to break up streaming sessions, preferably automatically. Maybe after some amount of time has passed, I could automate the process of saving the current file to disk, wiping it, and starting up a completely new database instance in it's place. This would mean that you could only review one chunk at a time, but it would probably resolve all the issues I'm having for the time being. However this would of course introduce blank periods in the data collection at pre-determined times, which isn't ideal. Worst case is that I just do this manually, and try to not collect streams for over an hour long each. The problem with this splitting method is that it will make reading the entire data set afterword more annoying.

​	I will start working on Dr. Ghassemi's idea tomorrow - the first step will be to choose the right precision to cap the data to. I'll need to figure out what integer type Redis uses and make the most of that. Then I will write a couple methods in my Database class to convert incoming and outgoing data. Once I do that, I should collect another 2.5 hours of data and see how the sizes compare.

##### July 24th, 2021:

​	I've waiting until my upload speed clears up to start testing the live stream, and I've found some bugs in it since I rewrote my database classes and worked on playback. I noticed that the EEG fourier transform would give a database response error when adjusting the fourier time window. I'm really glad I looked into this one because it turns out there were two issue: First, my function to validate a redis timestamp when writing data did not account for the case where an incoming timestamp was *lower* than the last written timestamp. I hadn't predicted that this could ever happen, but that is now fixed. The second issue is that my analyzer class was producing timestamps that were lower when updating the time window, which of course shouldn't happen. I discovered that this is because the timestamp that I was assigning to it was taken from the last element of the Filtered data list, which was read using the "count" parameter in my read_data method. In read_data, if the count method is specified, I have to use XREVRANGE to get the specified number of most recent datapoints. However I didn't realize that XREVRANGE actually returns a reversed list, so I was actually getting the *first* timestamp that was read. This meant that if I updated the fourier time window (or more precisely, *increased* it), the number of data points to be read for the fourier transform would increase, potentially reading farther back in the filtered data, and then giving that farther back timestamp as the timestamp for the FFT. Once I solved both of the issues, everything worked fine. I'm really glad I found this problem the way I did, because otherwise I have no idea how I would have caught it.

​	I tried on the full EEG + ECG setup today, and quickly realized that the EEG channel names did not at all match up to the electrode cap's setup. I will have to rewire it, which is going to be annoying since the only way I have to know which Cyton pin connects to which channel is to touch it and see which one makes the most noise. I want to implement some hover-text on the headplots to make this a little easier - will work on that tomorrow.

##### July 23rd, 2021:

​	Started today by fixing a potential issue with the video stream playback (again). I noticed that if I loaded the video stream part way into the streaming session, it would grab *all* the previous video data from the session and send it to the browser, causing it to lag while trying to load 20 minutes of data. Now the max_time is one of the first things that I implemented when writing the read_data methods, so I couldn't figure out why this was happening. After debugging, I finally found out that it was because I was using the real-world time since the last data point was read, and checking if that time exceeded the maximum allowed. However, I also always read the first point of a stream to get a reference point in order to know where in the playback to read to. This means that I was reading the first point, then immediately reading data up to where the stream currently is with very little real-world time passing. In order to fix this, I instead needed to calculate the redis-timestamp difference between the last data point read, not the real-world time difference. Because I'm using the last-read timestamp elsewhere in the code, this was easy to obtain and it completely fixed the issue. Also note that I scaled up the max_time with the playback speed, which is really only possible because of the downsampling I implemented yesterday.

​	I also realized that with the downsampling reducing the amount of data transferred to the browser, I don't even need to lock the database read operations anymore. However they aren't hurting anything, so I left them in incase it became important in the future. I can think of a situation in which the Analyzers are trying to read from the database faster than it can deliver, and this would prevent locking it up in that case.

​	My next task is to test this all with real EEG and ECG data. I booted up a Pi and tried to pull the git repo, but found it was corrupted so I deleted the whole thing and re-cloned it. This wiped the virtual env as well, so I had to re-run the setup script. It really annoyed me how long it takes to build Brainflow from source, so I did some digging to see it there was a way to avoid re-building it if the repository was up to date. I may have gotten a little side tracked.

​	I've been trying to wait for a good time to collect some testing EEG data but the upload speed of my internet has refused to go up all night. ugh.



##### July 22nd, 2021:

​	I started by working on a method to lock database reads to one at a time per stream. I used a threading lock to serialize access to individual streams, and only for the read_data and read_snapshot operations. While this did seem to fix the problem of unanswered data requests, it did not improve the lag of the browser. Data updates became less frequent but took just as long to load into the browser, looking at the timing numbers. 
​	Given that it didn't work as much as I'd hoped, I will attempt to implement the other idea from yesterday that seemed promising - to only read every Nth data point when streaming at N times speed.
​	After researching a bit, I found that there are some good known algorithms for downsampling data (namely the Largest Triangle Three Bucket algorithm, which is relatively simple). However, none that I saw I had any idea about how to implement in Redis since most requires some sort of aggregate function (which redis doesn't have), which means I would have to manually read the necessary data and that completely defeats the point. So instead I just settled on my original idea of taking every Nth data point when playing back at N-times speed. 

​	That turned out to be a little challenging to implement, but after I did it and debugged it for the remainder of the day, it works really well! As a bonus, the Bokeh plot displays a larger chunk of data at a time because it is less dense. I am VERY happy with how this turned out, but there is one other thing: I am currently just down-sampling everything to a constant frequency, and not based on the actual sample rate of the original stream. This is not ideal, but it *might* just work for the time being. I will have to collect some real EEG data to find out. I'll start that tomorrow.

##### July 21st, 2021:

​	Alright so this morning I had the task of testing and debugging the frantic 2am rewrite that I did last night. Surprisingly, there were relatively few issues. The one that I did run into was actually not a result of that - it was something already on my list to fix. The problem was that the total_time wasn't being correctly retrieved for video streams. The problem was that I was trying to read the last redis timestamp ID in the stream, but to do so I was using the redis instance that automatically decodes bytes data. However because it technically also reads the video frame in that data point, it was throwing an error because it couldn't decode it. Using the redis connection instance with decode=False fixed that issue, but I then had to manually decode the timestamp from bytes afterward. 

​	Oooook this is driving me crazy. I finally figured out the thing that's been bugging me - all these problems seem to be happening with the video stream and not the bokeh streams. This didn't make any sense to be because it's the same methods that are being used to calculate the time information. HOWEVER, this discrepancy is because of the room system I implemented for the video streams. When a new connection asks to view a video stream, the server checks to see if a stream for that live video stream is already running, and just adds it to that room to get the data. BUT the key thing is that data is coming from the first Database connection object that started reading that video stream, not the one that just joined. The result is that any timing information the current Database object has will not be synced with the one streaming. The reason this doesn't happen for the bokeh streams is that each one streaming is a completely separate database object each requesting its own data, even if redundantly. I haven't bothered changing that because I know I will be rewriting it with the Bokeh server. 
​	I think the best thing to do right now to save time is to just make the video streams independent like the bokeh streams - this should fix the time display issues is live mode, even if it is less efficient and introduces the same data splitting issue (only in the same session) as the Bokeh streams). However, I'll make a note about this problem so I will remember when I come back to implement the Bokeh server (and presumably make a general-purpose method to stream data to rooms instead of individually).
​	I wrote a big long #TODO in video_stream_events.py explaining everything, so hopefully I will remember all this when the time comes.

​	Alright now FINALLY I began testing higher speed playback. I added a playback multiplier to the time value returned by PlaybackDatabase.time(), and set it to 2. The result was just as expected for the Bokeh plots (YAY), but not for the video. Because the playback speed of the video is determined by the browser, not how fast the data is received, it played back at normal speed even though more video data was coming in. This means I need to rewrite the video.html file to get the playback speed information when loaded.

​	So after rewriting the video.html file and some wrestling with the HTMLMediaElement, I finally got the video to playback at the speed determined by the Database object. Yay!
​	I noticed that the video was having trouble playing right with the most recently obtained frame data, so I added a button in the browser that sets the video time to the current duration. This works just fine in Firefox, but for some reason not in Microsoft Edge, where I get an error saying that the HTMLMediaElement has no property currentTime. This is strange especially since I know that this property is supported by Edge (https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/currentTime), and my Edge browser is definitely up to date. Weird.

​	Next I was going to try collecting 20 minutes of data and seeing if it will handle 10x playback speed, but evidently my wifi is intent on being terrible. I still don't understand these periodic slow-upload times. My ISP is a demon company run my demon people.

​	After waiting an hour and finally recording about 20 minutes of video and random data, I tested playback on 10x speed. The 20-minute data file would not play. However, when I tested 10x speed on a shorter data file I exported earlier, it worked as expected. Not sure what is going on. I tried manually querying the redis database and the response was not at all delayed, so I don't think it's a problem with latency on redis's part. Also the browser data polling requests weren't going slow, they were just getting nothing as if the database was returning nothing from the queries. Then I restarted the server and tried again, and it worked fine. It hasn't happened since.

​	Alright NOW I have an issue with high data volume - only with the EEG stream. There is a lot of data here and I can't tell if it's the browser, the server, or redis that can't handle it. When I try to view the EEG stream during playback, all data polling requests hang and don't go through. Note that this is different than the problem in the above paragraph, where the requests were *going through* but the redis queries weren't returning anything. Here, these requests aren't even getting a response, indicating the Flask method handling them is not completing.
​	After some testing, I found that the bottleneck appears to be in the database.read_data() method. When playing the file back at 10x speed, those operations take a solid half a second, whereas normally reads for the EEG data at normal speed take on the order of 0.05 seconds. That makes a lot of sense actually, now that I think about it.
​	I did some profiling, and as I suspected the database read operation is what is causing the bottleneck, although to be fair my manual conversion to a python dictionary and then to JSON format accounts for ~20% of the time. Still, I now have the task of figuring out what to do about it. My first thought is of course to slow down the polling, but that would slow it down in live mode too, increasing the lag between measurement and display. I also thought about using a buffer between the database and the Flask server, but I'm not sure I am ready to spend another few days on that. Another thought I had was to put a clause in the database to ignore read requests if a current request for that same stream is already being processed - this might prevent the server form overexerting the database. Yet another thought I had was to somehow only read every Nth data point, which would be great for cutting down the amount of data read dynamically, but I don't think there is a way for redis to do that without me implementing the necessary write commands manually. I am going to stop working for tonight and try to work on those last two ideas tomorrow.

##### July 20th, 2021:

​	Ok so just quickly before I work on playback, I added a small script reload_flask.sh that sends a HUP signal to Gunicorn which reloads the Flask app. This is just so I can push changes to the server without interrupting the databases or streams. I also fixed a minor bug where Analyzers would target the same stream twice unnecessarily.

​	I also had a great idea to solve the issue I've mentioned the last few days regarding different browser socketIO clients not being able to share global data from their session because they don't know their own session ID: Add a socket message handler that echoes the socket's session ID, so it knows what session that is. Also emit some info like what databse they are viewing - that will allow them to a join a room for that too. This way, all tabs of a single session can receive the same data, and all sessions viewing the same database can receive the same data. Not gonna do this now, but I think it will work great.

​	I had to solve a problem with the elapsed_time metric - It wasn't resetting when a live stream was stopped then started again. Was an easy fix - just had to make sure to reset the self.real_start_time property of the Database class.
​	I also added a stream_time div to the video stream since it doesn't have one yet, which made me realize I hadn't refactored the CDN script tags yet like Dr. Ghassemi suggested, so I went ahead and did that. Now all CDN script tags are included from templates/cdn.html as macros.

​	So again, before I dove into implementing faster playback I went to test the playback with these changes and for some reason the video stream would occasionally not show anything in playback mode. 
​	After some digging I found out it was because of the way I organized the video streaming thread back before I implemented playback. Each incoming socket connection joins a room for the given video stream to avoid reading the same data from the database redundantly. However, this meant that I wasn't checking the database every time a new stream joined. So, any time after I viewed the video stream after I had already started it, the database in the running stream thread would not have changed. I am debating whether it is worth the extra overhead to check the database for every frame sent, or have each socket connect using a different database connection and risk reading redundant data.
​	So I went ahead and just rewrote the whole thing. I now have it set to do the following: If an incoming socketIO connection is from a session with an associated Live database, it joins a room with ID equal to the stream ID that it requested. This means that a socket connection from ANY session requesting that stream will join the same room and view the same video. On the other hand, if an incoming socket connection has an associated database that is in playback mode, it joins a room with ID equal to the stream ID concatenated with the session ID. This makes that room unique to each session and stream, so different sessions viewing the same playback database will not conflict

​	Now I'm trying to fix another problem - the live database start_time is not global. Two sessions viewing the same live data will have different start times (and thus get different elapsed_time values) when viewing any given stream. This means that I have to store this value in the database itself to be read by every incoming connection that wants to read from it.
​	Aaaaand now I'm completely rewriting my Database classes to be two separate classes - LiveDatabase and PlaybackDatabase. The confusion of having if-statements everywhere to check which mode the database was in is just driving me crazy. This way the separate functionalities of the two database modes will be contained in entirely different classes - much easier to think about and work with.

##### July 19th, 2021:

​	Started today by trying to figure out why the video stream isn't working during playback. Luckily, this wasn't a huge issue - just some byte decoding problems that were easily fixed.

​	The next task is to make the playback run at 10x speed. At the moment, it should be theoretically possible to just increase the rate at which time values are output from my Database.time() method, since all playback takes it's position from that. The main concern I have is how that much data will be handled by the browser. The first step, though will be to record about an hour of data. I have never done that before, so finger's crossed.

​	Sooooo we have problems. I started recording data and set it run for 10 minutes. The problem was at the end when I stopped the streams, it never saved the file. I'm not sure if it was because it took too long or if a problem happened. I'm going to add more logs to that are and see if I can find out what happened.
​	Turned out that Flask just tried to save the database while it was already in the middle of saving automatically in the background - I just need to at something to catch this case and wait for the background save to finish before dumping the file. This also made me realize that a little display in the browser to indicate the time since last save would be super handy.

​	I hooked up the Cyton board that Dr. Ghassemi recently sent me, and just like before it had a very spotty connection. I then disconnected the electrode cap and reconnected it to the other Cyton board that I've been using since last summer - the stream worked just fine. I can only assume that this means there is something wrong with the new Cyton and dongle, which is really unfortunate because I was hoping to be able to stream two separate EEG signals at once as a form of limit-testing. Instead, I will just be using the Synthetic EEG data that Brainflow can generate.

​	I implemented the little last-database-save indicator in the control center. It works great. However now I have another issue - when a background save is in progress, some database read/writes will fail because the database is busy. I dove into trying to make sure errors are caught and handled properly, and one thing I realized is that having the buttons in the interface enabled/disabled based on the database state is not a good idea because, if an error happens in a place I didn't expect, the buttons might not be in the right states. Instead, I should have some non-interactable indicator that says whether the database is streaming or not. ~~I won't do that now, but it is something for the future~~ So I went ahead and did it now anyway. I couldn't stop myself. Now I have a nice little section in the control center that displays the current status of the database, the source of the database (live stream or a filename), and the time since the last save to disk. This information is updated once per second by polling the server with a SocketIO request. I would just have the server broadcast this info to a room for the session, but I can't get the session ID in the browser - that was the problem I had at the beginning of last week. 

​	I just spend waaaaay too much time refining the new "Database Status" section of the control center, but it was very worth it. It's something I've been wanting to do for a long time now - and it feels good to have real-time feedback on the state of the database. I added a bunch of extra error checking for exporting the database file to the 'saved' directory, just incase things go wrong - I don't want to lose hours of data collection when I finally get to it.

​	Ok, TOMORROW is when I start working on increased speed playback :)

##### July 18th, 2021:

​	Alright so I have a wrote a new method called Database.validate_redis_time() which takes in a redis timestamp returned by Database.time_to_redis() and a stream ID. It checks to make sure the timestamp is greater than the last one used for that stream, and if not it adds a redis-style sequence number to the timestamp. The benefit of this is that I can now completely control the Redis timestamp ID of a given data insertion, meaning that the playback of database files will be true to when the data was collected by the measurement device, not the database. The downside is that this adds some overhead to reading and writing to the database - thought I haven't *noticed* any performance issues. Also the redis timestamps generated are only accurate to the millisecond, which means that any timestamps taken by a measurement device which are more accurate will not be reflected in the redis timestamps. This only affects visual playback - of course the fully accurate timestamps are stored along side it for analysis. Again, I am hopeful that this will all go away when we move to a database designed specifically for time series data. Still, I would like to try and implement RedisTimeSeries before we go that far, because form what I can tell they circumvent this problem with the Redis Stream data type and make everything much nicer. Regardless, that will have to come at a future time.

​	Next I began investigating a problem that was causing the Fourier and Headplot playbacks to slowly fall behind the raw EEG data. They would start out in synch, but within the span of a minute they would be at least 5 seconds behind. Of course this would be disastrous on the scale of hours, so I need to fix it right away. After debugging for some time, I finally realized that this is happening because I am reading data chunks relative to the last redis timestamp ID read, but with a time difference calculated from the last absolute time read. This means that, if a query returns less data than the max_id requested, then the last_read ID will be behind the last_read absolute time - the consequence of which is that the time difference will appear to be correct while the time difference between ID's is slightly shorter. The reason the Fourier and Headplot data columns were lagging is because the redis timestamps are more spread out compared to the raw data, so the difference between an absolute time difference and a time difference between data points was more significant. To fix this problem, I needed to instead calculate time passing relative to the start of the stream, not the last data point read. Once I implemented this, the problem went away.

##### July 17th, 2021:

​	The first bug that I fixed was the last one I mentioned yesterday - where stopping the playback would jump the stream ahead by some number of seconds. I figured out that the problem was that while self.last_start_time is an absolute time, self.last_stop_time is relative (it's relative to the "time" with respect to the stream, which is currently paused and not moving forward). The mistake was that when the stop button was pressed, I was moving self.last_stop_time up to the current absolute time(), which is wrong since it must be relative to the current time *relative to the stream time*. To fix this, instead of setting self.last_stop_time to time() in the database stop() method, it is instead set to self.time(), which returns a relative time in playback mode. Also, I changed the variable names to reflect this distinction. self.last_start_time is now self.real_start_time, and self.last_stop_time is now self.relative_stop_time.

​	Next I began investigating a new problem I noticed where the playback of just the EEG filtered data would stop at the same time every time - at exactly 59 seconds. The problem appeared to be that the database was only returning the same one query result each time, so the last_read redis ID was the same every time, but the last_time timestamp was continuing to update, so the time_since was always what was expected. After some digging, it turns out this is because the behavior of XRANGE is slightly different than XREAD. XREAD takes in a redis timestamp ID and looks for all data points *greater* than it. XRANGE, however, taken in two data IDs and looks for data between them *inclusively*. I was assuming that it would be exclusive, like XREAD. The fix for this is to add the prefix "(" to the minimum range value for XRANGE, and it is treated as exclusive. However, the question still remains why this happened in the first place - I believe it it has to do with the fact that at exactly 59 seconds in that stream, the connection lagged a little and a chunk of data came in a bit late, causing Redis to time stamp it later. This meant that when the playback tried to read the next range of timestamps, it didn't return anything (except the included start of the range, which is now no longer included). I believe this indicates a larger flaw in the way that I am playing back data. Because Redis uses its own timestamps to store the data points, they are not at all related to the time at which the data was taken, so any lag in the network will be reflected in those timestamps. If I want the playback to be accurate to the time at which the data was taken, I should be reading from the actual time column of the database. This, however, requires that a time column be present in every stream. If I implement this, I will need to make sure that this structure is added under the hood, or at least throw an error if no time column is added to the stream. It is unfortunate that Redis does its own time stamping in this manner - just another indication that it is definitely not meant for streaming despite the STREAM data type.

​	I did some searching, and came accross RedisTimeSeries, a Redis module that supposedly implements a much nicer time series data type for Redis. I was really interested in using this, so I started the process of installing the necessary dependencies, and started testing it out in separate environment on the AWS. However, when reading through the documentation of this interface, I noticed that part of the way they were implementing it was by manually setting the Redis timestamp ID.... evidently there was a way to do this that I just didn't know about. As soon as I found this out, I immediately went to try this out instead - it would save me from having to do this right now (though I still now want to try RedisTimeSeries before Influx). After implementing this, I ran into all sorts of issues with the redis time stamp format - I'm going to continue working on this tomorrow.



##### July 16th, 2021:

​	I've written the necessary methods to retrieve time information from the database - one to get the currently elapsed time of a stream (both in live mode and playback mode), and one to get the total time in a playback stream. 
​	In the process of adding this to bokeh_plot.html, I also refactored it a bit - I moved the Bokeh javascript to a separate stream.js file, and added SocketIO and jQuery compatability. I noticed that I was still using an XMLRequest to retrieve the Bokeh plot, so that's something I'll probably change in the future. It works fine for now so I didn't touch it - I'd change it just for the sake of consistency.

​	When testing the get_elapsed_time() method, I kept getting a strange error when trying to parts data form my get_group() method. It turned out to be a problem parsing data from redis that I have just inadvertently overlooked when using it in the past. Some time ago I added a 'name' key to the "group:" data columns to identify the group name. However, stream names and their corresponding IDs are also in the same data column. When using get_group(), it actually returns that 'name' key and an empty dictionary as if it were a stream and corresponding info. This didn't cause any problems before because the index page just ignored the empty dictionary. It is now fixed - get_group() just ignores any keys it finds that doesn't have a corresponding "info:" data column.

​	Another problem I ran into was that any stream with a prefixed ID in the data column wasn't being read with get_elapsed_time() or get_total_time(). The problem is because those streams are not returned by get_group(), which returns all 'info:' data columns associated with the given group name, and prefixed streams all have one associated info column. To fix this, what I needed to do was write a new method (get_streams()) which, given a group name, searches for any 'stream:' column that contains an ID specified in the group column. It then returns a dictionary of all the found streams and their corresponding names with the prefix.
​	Doing this made me realize that I sorely need some standardization in the way that I name the 'stream:' columns, because if someone writes data to a column with a different name structure I won't be able to find it. The best way to do this would be to hide their structure from the user. There should be a given method to "name" a stream, but under the hood always add the stream ID to it. This would hide the stream ID portion from the user and allow me to dictate a specific naming structure to the columns which will be guaranteed. The trouble then is how to retrieve that full ID in the create_layout() method. I think that whole thing should be redone - maybe as a class method that must be written? Anyway, for the purposes of this get_streams() method, the structure is assumed to be:  "stream:prefix:full_stream_id"

​	Continuing the debugging, I found that stopping the playback of a stream jumps it forward by about 5-10 seconds. There is also an issue where sometimes a playback stream will just stream the same chunk of data over and over. Needless to say, there are a lot of issues right now. Also my internet is bugging out again so I think I'll end for tonight and try again tomorrow.

##### July 15th, 2021:

​	I first needed to implement a way to keep track of the current playback time when a database connection is in playback mode. I did this by defining start_time and stop_time variables, and using a custom time() method to only return the time as if the start() and stop() methods stopped time. This means that my read_data() method can just use self.time() for both live mode and playback mode.

​	I then started testing normal live streaming in the new setup. I found a bug where entering a stream, leaving that streaming, entering a different stream, then entering the first stream again breaks the first stream. When it breaks, all data polling requests return no data. After an hour or so of searching, I finally found the error in my method that converts from unix time to redis timestamps. It's a factor of 1000 not 100. Ugh.
​	The good new is, though, that the new structure has solved the data-splitting problem that I've been pushing aside forever. Of course this is only true when viewing data on different sessions, but nevertheless it certainly feels good to have a solution. Dr. Ghassemi will definitely be happy about this.

​	Another thing I noticed when testing this is that the Internet Explorer default HTML5 video tag does not automatically have an interactable slider at the bottom of the video like Firefox does. This means that there isn't a way to jump to the front of the stream in Internet Explorer. I think the best way to cover this case is just to add a button for that purpose - I'll put that on the backburner for now (made a note).

​	After debugging the playback feature, I finally got it to work for the first time - I was able to play back data from a loaded database. There were a few problems I noticed though: First, the Test Group data column wasn't updating live. Each time I opened that page it would display a new chunk of data, but it wouldn't update live. I'm not sure why because other data streams were updating, including the Random Analyzer stream. I also noticed that when going back and forth between streams, there appeared to be a roughly 2-second time jump each time I switched from one to the other. Again this is really weird because each stream's last read data point shouldn't be affected by another.
​	I also found myself wishing I had a way to tell how long a stream is and where the playback currently is. I want to implement this next along with debugging the above problems. The tricky part about this is that a playback can be paused, so it isn't enough to just have the browser keep track of time - the browser would also have to know exactly which data point it currently has, and update in real time. Technically the browser does have this information, but I don't easily have access to it because the data polling and plotting is all handled by Bokeh. The best thing I can think of is to make a button that, when pressed, asks the server where it is in the playback. Of course this would not update live unless I made it automatically do this every second. Getting the total time of a stream should be simple, though - I just have to query the first and last data points and subtract their timestamps.

##### July 14th, 2021:

​	I've encountered a very strange problem - the updates to the session variable are only persisting in some circumstances. I noticed this when my start and stop buttons were not staying in the same state after refreshing the page. The start and stop buttons should retain their state as long as the session is active, but they don't and I can't figure out what the problem is. They change state immediately when pressed, which means that the session is at least maintained for the duration of the request, but as soon as that is over the session reverts to a previous state. The weirdest thing is that if I add a dummy session variable (like session['testing'] = True) to the start/stop socket handlers, this behavior disappears and the session persists as expected. However if I add the same dummy value to the session earlier, like in the socket connect handler, that doesn't work. The only time the behavior seems to go away is when I initialize an unused dummy session variable.

​	Ok so I did some searching, and apparently Flask-Sesison uses a CallbackDict to track changes to the session variable, which means that modifying a value in-place (like I was doing with the dictionary of button values) does not trigger an update. The dummy variable was being set, which did trigger an update. The official way to do this is to manually set session.modified=True. This is stupid.

​	I've now fully implemented the "load" function. When a file is selected and loaded in the browser, a new redis server is started which targets that file. Also, when the user switches back to the live database (with the "live" button), that redis server instance is shut down but not saved back to disk. Meanwhile, the live redis server instance was not affected. This is all accomplished by keeping an index of redis connections within the flask session, which simply connects to different databases on different ports. As of right now I have set a hard limit on the number of redis server instances allowed to be active at once, and the ports on which they are active: 7000, 7001, and 7002. 

​	However I did come across a potential issue - Redis must load the entire database into memory in order to allow reads. Because of this I added a loop to wait for the database to be ready and send intermittent signals to the browser logs. However since the databases I'm working with right now for testing are not large, I have not tested this functionality. I figure once I start doing more heavy duty testing I will deal with it then. I also added another button to the control center labeled "abort". This forcably kills whatever redis instance is currently associated with the session. I figured it might be a good precaution to take since I don't really know how Redis will handle the amount of data I intent to collect with it. 

​	I finally was able to test the actual playback functionality, and as expected nothing worked. Time to debug that. Regular live streaming works, which is fantastic, so It's probably an issue with my read_data in playback mode. I will start with that tomorrow.

##### July 13th, 2021:

​	I started today by creating a "dummy" database class so I could test the server sessions first. While testing, I came across a bug with something I wrote awhile ago - the connection timeout that I implemented for the Redis connection. I wrote a timeout so that, in the event that the connection got disrupted, it would attempt to perform the database operation again. However apparently this has never been triggered until now, and only now have I realized that Redis-py already implements its own connection timeouts. I didn't know about this, so the default option was set, which is just to never timeout. I found this out because my redis connection was hanging indefinitely. I scrapped my entire connection timeout loop, and instead defined the socket_timeout and socket_connect_timeout parameters for the Redis.ConnectionPool objects. Now timeouts work just as expected.

​	Also, as I was going through this old code, I found that I was trying to manually disconnect the Redis connections, which was causing some problems when keeping track of what connections were active and which were. I'm not sure why I did this because the Redis connection pools handle that already. Maybe this was before I switched to a connection pool? Not sure - but regardless I removed those pieces, so now I don't have to deal with it at all.

​	I ran into another issue. When navigating to the index page in two separate browser windows, the server creates a new session as expected, but when I do anything on the page that should edit the session (e.g. clicking a button), that change is emitted to both browsers. I do not think the issue is with the sessions because both still access their own unique session variables - the problem is that the SocketIO emit messages are received by both windows. Now that I think about it, though, it's almost certainly because I'm using a single namespace identifier for the browser, when instead each session should get its own. I have a suspicion that I will need to use SocketIO rooms to solve this.

​	Alright so I have run into a problem. The functionality I wanted was to have each session keep every instance of it up-to-date. This would mean that if I have multiple tabs open in the same session that each tab contains the exact same information. This would prevent things like pressing a disabled button in a different tab. However, this requires that I have the socketIO instance in each browser join a specific room, that room being the one associated with the session. However, Flask uses an HTTPOnly cookie to store the session ID, so I have no way of getting the session ID in the browser. I can easily get the session ID on the server side of course, but I cannot find away to do it on the clients. 
​	I could avoid this problem by instead having the server communicate only with individual browser socketIO instances, but that would mean that two tabs in the same session might have conflicting information. Now obviously there is no reason to have two of the same tab open, but that doesn't mean it won't happen of course. I think that is what I will try to do for now, and I'll ask Dr. Ghassemi his opinion next meeting.

​	After implementing what I outline above, I found another issue. If two different sessions are viewing the same live database, neither will have any idea if the actual streams have been started. This, the start and stop buttons will be out of sync and not update. I believe that I can fix this by emitting the change to all browsers, but that introduces inconsistent behavior that the user might not expect. Unsure how to resolve this - will ask Dr. Ghassemi on Monday as well.

##### July 12th, 2021: 

​	I talked to Dr. Ghassemi today, and he had a great suggestion for solving this organization issue that I've been having. The plan is that, when playing back an old file, the app will start up a new parallel redis instance that is only accessible on the local network of the server. That way I won't have any issues with Pi's connecting and setting the database in read-only mode like I was trying to earlier. Then all I have to do is tell the Flask app to create a new Database class object that points to that redis instance to read data from in playback mode.

​	This structure, however, requires that I implement server-side sessions because the Flask app will need to maintain a connection to both databases and serve their contents according to which session requests it. Therefore I have install Flask-Sessions (which is server side, as opposed to the native sessions which is client side) and configured it to use redis as a session store. This seemed the most practical since I already have redis set up so it was simple to implement. Every time the program is run, now two redis instances are started - one for streaming and one for Flask-Sessions. My goal will be to make it so that each time a playback is requested, a new redis instance will be started to serve that data. Note that it was important to configure Flask-SocketIO as well to play nicely with Flask-Sessions because it is handled through websockets not HTTP. I got this from a great article by Miguel himself (https://blog.miguelgrinberg.com/post/flask-socketio-and-the-user-session).

​	There is still one part I'm not yet sure about - where to store my Database objects. Flask-Session is using redis as a store, but Redis doesn't handle python objects - so where do I keep the Database classes that are associated with each session? Should I just keep a global dictionary? But wouldn't that defeat the purpose of having Redis as a session store in the first place? 

​	After some thinking I've decided to write a new DatabaseController class that will keep track of all the various Database classes with an internal dictionary that indexes these Databases by the session ID given when they were created. I believe this way I can even allow multiple viewings of the same database file or live database file if I want. I'll probably be working on this for the next few days.

##### July 9th, 2021:

​	I have not made much progress today. I am seriously considering rolling back the repository to a point before I started to restructure the database/streamer interaction, because a lot of functionality is broken and I'm not sure how to fix it. 

​	One thing that I managed to accomplish instead was some interface management. I wrote methods that enable and disable various buttons in the browser. The Flask server can then control when each button is enabled at any given point in time, but I am finding it difficult to make sure that it is robust enough to be useful. I had a problem of these button settings being completely wiped if the browser was reloaded, which made me think I had to store them in the session variable. I tried this, but it didn't seem to completely solve the issue and I realized that it wouldn't even persist if someone were to navigate to the site from a different browser. So instead I've attached a simple list of button states to the Flask app itself, which is then sent to the browser during the refresh() socketio call along with the list of pages and files. This feels kind of weird to me but it was the only way I could think to do it. Of course normally I would use the database, but now the database might even be shut down when this happens so there is no guarantee that information will be available much less persistent across database instances. 

​	One thing I am considering is the use of redis's multiple databases. It allows one to create separate instance of redis and read/write to them separately. My idea would be to keep one database reserved for Flask, one database for the current stream session, and then load old files into another one. I have no idea if this is the intent for that feature, but I could certainly give it a try if I eventually give up on my current approach. I think I will wait to consult with Dr. Ghassemi on Monday before making that decision. Until then I will continue to try and work out the bugs.

##### July 8th, 2021:

​	So I got sidetracked and rewrote the my database read_data() method. I realized that the way I was calculating the max_time value was wrong, but that edge case just hadn't come up during streaming so I didn't notice. I have now tested it and it works correctly. This just allows the Flask app to put a limit on the maximum amount of data to read from the database, in case the browser disconnected for some time, and then later requested all the data it missed. Instead of requesting all that data from redis, it only requests up to a given number of seconds. 

​	I also noticed that the way I was time stamping the data was not going to work. I have been time stamping relative to the start of each individual stream, which made the x axis of the plots readable. This was, of course, a cheap fix and I have now decided to do it properly. Time stamps are now in UNIX time, which should be consistent across all streams. In order to get human readable times displayed in the browser, I found the Bokeh has a really handy formatter object that will automatically convert time stamps using strftime() format options, which even changes at different time scales. I love it. I now have a custom DatetimeTickFormatter in the new utils.py file in app/bokeh_layouts that can be used by any create_layout method.

​	I also cleaned up the main browser_events.py file. I think I'm procrastinating. I made a decorator to catch errors and send them to the browser, so it's not entirely useless.

​	I tried to rewrite the way that the database and the Streamers interact to help implementing Playback mode, and everything broke. Like so many things. Things that aren't even related, like now the Picam object hangs when I try to instantiate it. I can't do any more tonight I'm going to try again tomorrow.



##### July 6th, 2021:

​	I decided to add the confirmation popups today first to make renaming files easier in the future. Turns out jQuery saves the day again with the IU package. This took me a while to figure out, but I now have a basic confirmation dialog for the delete command, and a dialog to rename files with basic file name validation. I started out with the usual form submission with HTTP requests, but it was sort of cumbersome so I instead switched to using the socketIO structure that was already present in the script. This was way easier and allowed me to quickly implement error messages on the server side, which now appear in bold red separately from the normal log messages, which is neat. These are used when invalid file names are given and such. I have the confirm_dialog div in index.html so it can be used for other actions that might need confirmation in the future, and theoretically it can be used multiple times for various dialogs, though I haven't tested it. Worst case I just made a new div for each dialog I want on that page.

​	I also bullet point #2 from yesterday, which involves replacing the current database file (located at data/dump.rdb) with the specified database file chosen in the browser. The server then starts Redis, which automatically loads the contents of that file, displaying it as if it were the current live database. 

​	The tricky part is figuring out how to read from the database from the beginning. I first tried implementing a second redis database instance that would stream data to the main database, mimicking the data streams that its built for. After writing that, I realized that I would be performing the exact same operations if I were just reading from that secondary database instead, so I scrapped it and started over. Now I am in the process of trying to rewrite the original database read operations to accommodate.

​	The main issue I am having now is with synchronization. The streams coming into the server are not synchronized, and therefore have their own time axis associated with them. When reading through the data from each stream, I need to have a synchronized time scale in order to read them all at the same time. I am trying different methods to see what works.

##### July 5th, 2021:

After talking with Dr. Ghassemi today, we decided I needed to implement a feature which would allow the user to review all data collected. This requires a number of things:

- ability to select old database files in the browser
- ability to load old database files into the current database
- ability to stream the current database file from the beginning at an increased speed
  - this will require that a sample rate be assigned even to streams without one, like the sense hat stream. Otherwise the server will have no idea how fast to stream it back.
  - not yet sure how to deal with widgets here. should they just be completely disabled?

Today I tackled the first of those. I added a div in the browser that displays a list of all the redis dump files in the data/redis_dumps directory. I wrote javascript which allows the user to select a file from that list, and then execute a command which is sent to the server along with the selected file name. I started out not having any idea how to do this, but so far I have learned that jQuery is magical.

I added 3 commands that are sent from the browser: Load, Rename, and Delete. The expected functionality is that Load will load in the selected database file, ready for playback. Rename should just create a popup with an input and rename the file to that name. Delete should delete the selected file. I have currently only fully implemented the Delete command, as it was relatively simple. Tomorrow I will begin working on the Load command. The Rename command will be tricky because it requires a popup with a submission form, and I do not yet know how to do that. I also would like to add a similar function to the Save button.

##### July 3rd, 2021:

Today I'm working on getting the new EEG electrode cap streaming that Dr. Ghassemi sent over. I've done all the setup and things appeared to be working as intended, however occasionally the board stops transmitting and I can't figure out why. I've tried it on the OpenBCI GUI on my Windows 10 PC, but I see the same results. I've made a [post](https://openbci.com/forum/index.php?p=/discussion/3045/cyton-daisy-fails-to-start-session-other-cyton-daisy-streams-inconsistently#latest) on the OpenBCI forums to look for assistance. For a while I also thought that the first Cyton board stopped working, but it just turns out that I had the switch on the dongle on the wrong setting. Sigh.

In the meantime, I was able to add another feature that I've wanted to for awhile - the ability to keep the widget configuration in the Bokeh plot even after closing the window. The default positions of the bokeh widgets were normally just set in the create_layout() method, so every time the page was loaded it would load the same widget configuration. The problem with this is that the filter and fourier settings remain how they are regardless because those settings are stored in the Analyzer class. The result is that after changing the widgets and closing the window, the next time the window is opened the widgets would be back to default while the filtering settings remain changed. This is annoying, as one can imagine. To fix this, I added another key in each stream column in the database that holds all the widget information, stored as a JSON string. I chose a JSON string instead of storing each widget value separately because it requires less type conversion. The widget value is first set in the browser in JavaScript, and those values are used in Python. The Python json package does the conversion for that automatically, but if I were to do the intermediate step of storing them in Redis it would just have made it more complicated. The downside to this is that it added yet another step to writing the create_layout method that a potential user would have to know. Again, I'm putting off making this more compartmentalized for when I work on the Bokeh server, because I would rather not spend a lot of time doing something I might have to scrap later anyway and won't even be all that useful.

##### June 30th, 2021:

I finally solved the bokeh problem. Since have different data streams being plotted to the same Bokeh plot, each stream had the possibility of having different column lengths. Normally this wouldn't be an issue because it can be solved by making sure that each data stream is paired with it's own time stamps, and that only this pair has consistently the same length. This worked up until I used the AjaxDataSource in "append" mode, which means that data points are rolled over after reaching some maximum limit. This is necessary for the scrolling graphs that I want for the browser display. The problem is that as soon as the number of data points reached that limit, I would get a mysterious "size mismatch" JS error in the browser and the graph would freeze. 

I asked about this on the Bokeh Discourse site (https://discourse.bokeh.org/t/size-mismatch-in-ajaxdatasource-in-append-mode/8082), and they informed me that my workaround wasn't enough, and the ColumnDataSource object always assume columns of equal length *even* in situations where some pairs are independent of one another. They told me that the only solution to streaming completely independednt data streams to a single plot is to use seprately defined AjaxDataSources. So that's what I did. It adds a lot of overhead and 4 times the number of requests sent to the server, but it works. That's all I need for now, but this is yet another thing that I hope will go away when I write up the independent Bokeh server in the future.

This also means that the modifications I made to Database.read_data() and Database.write_data() to handle data columns of unequal length aren't necessary, but I will leave them in just because they are technically now able to handle more general cases. Again, this will all get scrapped anyway when we move to a different database.



##### June 28th, 2021:

Today I'm just focusing on debugging the system I decided on yesterday. I had some trouble making each Analyzer able to access the ID of its targeted streams. As of yet I have not gotten rid of IDs just in case I need them in the future.

I also started having a weird issue where the server takes an unusually long amount of time to connect to the redis database. It's always been near-instant, but today it takes a solid 5-15 seconds, and sometimes it just never does. I have no idea why. I think it was just a temporary network issue because it went away after a few hours. Still annoying though.

I had some problems with the Test Data Analyzer, which is what I'm using to find bugs before I try to make changes to something like the EEG stream. I found out that my Database.read_data() method didn't cover the edge case where the data being read has columns of different sizes. Up until now, all the data being put into the database has had consistent lengths within each write operation. This ties into one of the most annoying things about redis, and that's the fact that data points are written and read individually each with their own data column name. This means that each data point is read in as a dictionary, and I have to manually collect these points into a dictionary of lists. When these lists don't have the same length, my algorithm breaks because I was checking the keys at the beginning of the loop to make it more efficient rather than checking every time I read in a data point. Now that the Test Analyzer has revealed that writing multiple streams to the same group creates this problem, I had to rewrite this code to slightly less efficient, which I am not happy about.

Even after fixing that, I am still getting an error in the Bokeh plot. Evidently it doesn't like two completely separate data columns having different sizes. sigh.

##### June 25th, 2021:

For the past few days I've been experimenting with InfluxDB to see whether I can feasibly replace Redis with it relatively quickly. InfluxDB is a database designed specifically for time-series data and IoT streaming, so I figured it would be perfect for the job. Redis isn't the best fit because it focuses primarily on being a fast key-value store rather than a real-time streaming centered data store. I was able to download InfluxDB onto the AWS instance and play around with its REPL interface to get a feel for it. I learned that Influx has its own unique query language called FLUX which can perform read operations similar to grouping and indexing methods used by SQL queries, but in an easier to read format designed for reading time-series data. This would be fantastic, except that I could not get data writes to go through with the provided python library. I'm not sure what the issue was as I couldn't really debug it - my read queries were just returning no data. If I spent enough time on this I might be able to get it working but I discussed it with Dr. Ghassemi and he suggested that I drop it for now. Redis is working fine for the time being and will server its purpose to get the data we need for the MSGC grant. Once we have more time to improve the project I may return to this.

I then began to tackle another feature that I need in order to perform the data collection for the next stage of the project - the ability for a single Analyzer to target and read from multiple Streamers. At the moment, an analyzer can only target a single data stream because all the data analyzers I have currently only need to do that - it wasn't until last week that I realized I would need to be able to target multiple streams for when we want to do concurrent analysis on all data sources for machine learning.

My first idea for this was to just add some logic to the Analyzer to be given a list of stream names and groups to target, and perform the steps necessary to start the analyzer if at least one of the streams is present in the database. When I wrote this up, the end result was really messy so I wanted to find a better way to structure this that makes more intuitive sense to someone trying to build the same thing using my library. The main issue is that the Analyzer class definition needs to be passed its own name and group, as well as a list if all targeted names and group, which is just not a good way of clearly stating what is going on. I decided that I would try to implement a decorator-style method of doing this. I've wanted to do something similar to Flask and socketIO style handler definitions, where a method is defined and decorated to indicate the conditions under which that method is called. The idea is that one could simply define the Analyzer class with its own name and group, then define loop methods to read and write data. Those loop methods could then be decorated with a function to define which streams it is targeting, one decorator per target. I believe this would look a lot nicer and be much more intuitive to work with, but when I started building the necessary structure I ran into some problems. First, there really wasn't a clean way to access class members like self.database that is necessary to have inside the main loop. Second, user-defined object like the Picam.picamera don't have a nice place to be stored other than just in the global scope, which obviously just isn't a great idea. The way that Flask appears to have solved this problem is by creating the object current_app that can be imported and gives the context of the current Flask app. I can't quite do this because I have multiple Analyzers and Streamers to worry about all running on different processes rather than just one object, so that isn't an option for me. I thought about just creating a global dictionary that contains them all indexed by name and group, then then I was just getting further away from my original goal of simplicity.

Finally, I settled on a sort of compromise - I will instantiate the Analyzer/Streamer classes and pass them into the Client object like I did before, but this time add methods to the Analyzer class that allows the user to define what stream is targeted. This method can be called any number of times to target as many streams as necessary for that analyzer. I honestly probably should have just thought of this right from the beginning to avoid creating a structure that I ended up scrapping anyway.

After building this new structure and implementing logic to target multiple streams, I made a fantastic discovery - this actually solves (somewhat) the annoying issue I've been having recently regarding the Analyzer classes updating their info dictionary. The problem was that analyzers that read data output from another analyzer (as is the case with the Fourier analyzer and the Filter analyzer) had trouble keeping their info dict up-to-date with the raw data stream's info because they had to get it from their target instead. I'm not really sure what the issue was but sometimes the top layer analyzer wouldn't get the info in time and fail to receive the necessary data like the sample rate and data column names. Now, when the analyzers are able to read from multiple streams they will be able to pull that info directly from the raw data stream instead of getting it from their target. This eliminates the need for to copy the info dict in the "check_database" method. This is great, but now the only issue is that it introduces a little more complexity when writing the loop method for the analyzer as you have to know the group and name of you target stream to get its ID. At this point I'm just considering getting rid of IDs altogether because the group+name combo is intended to be globally unique anyway, and IDs aren't at all human readable. I'm still working on debugging this new design, but hopefully this will make things much nicer.

##### June 21st, 2021:

I mentioned that I was having trouble seeing any peaks in the ECG frequency domain that might correspond to heart rate - however after messing with my configuration settings for a while, I think I found the  problem. I believe the time window over which I performed the FFT was just too small - I've had it set for 2 seconds (which is what I used for the EEG). However because heart rate is such a low-frequency measure (comparatively), and because the various repeating waveforms are all around the same frequency, I guess it was just hard to detect with such a low resolution. After increasing the time window to 10 seconds (thus increasing the resolution of the frequency domain), there was a clear peak between 1-2 Hz, which is what I expected to see in the first place. 

Today I've also run into a problem out-of-the-blue. For some reason, all the streams appear to have a significant amount of lag. This is causing the browser to have a hard time receiving the data because it's getting massive chunks at once rather than in small increments as intended. I'm not sure where this is coming from - I haven't made any changes to the way that the streams are read or transmitted, so the only thing I can think of is that my network is really slow today. Regardless of the cause, I added an option in my database.read_data() method to but a maximum limit on the amount of data read. This didn't fix the problem of course, but it at least helped lessen the load of the big chunks coming. It also doesn't work well with the video stream because there isn't a way to guarantee that a key frame is sent right at the beginning of each chunk, so it has to read all the data sent and can't skip anything.

I did some testing, and it indeed appears that the send operation for the EEG and ECG data is taking an unusually long time, which only increases as more data is collected.  

##### June 20th, 2021:

On Friday, I had said that I wanted to get  the EMG sensor working for our data collection, but it turns out that in order to use the pre-made sensor that we got in the kit I would need a soldering iron, which I do not have. So instead, I decided to see if I could just do it with the sticky electrode pads and the extra wires.  However, I figured while I was at it I might as well try to replicate the ECG setup from that paper that Dr. Ghassemi sent me. So that's what I did. I now have a working 3-lead ECG stream working with appropriate filtering and whatnot. I'm more excited about this than I probably should be. I threw  in an FFT stream for it just to see what would happen, but it doesn't look like anything exciting. I thought I might get a little spike at the frequency of my heart rate but I didn't anything of the sort. Oh well. I'm not certain I have the chest electrodes in the right spots, but at least I see a recognizable ECG waveform.

I also looker more deeply into InfluxDB, and as predicted it is much  more complicated than redis. It took me 20 minutes just to realize that  there are 5 different python libraries all for different versions of the software: [influxdb](https://pypi.org/project/influxdb/), [influx-client](https://pypi.org/project/influx-client/), [influxdb-python](https://github.com/influxdata/influxdb-python), [influxdb-client](https://pypi.org/project/influxdb-client/), [influxdb-client-python](https://github.com/influxdata/influxdb-client-python). As you can imagine this naming convention is driving me insane. Anyway I installed InfluxDB on my own PC first juts to mess around with it - I haven't yet tried to put it on the AWS yet. I downloaded the windows  version of InfluxDB and it came with their own GUI, though obviously I'm going to need to use a CLI tool for the server. They certainly aren't  making it easy for me, and I'm a tad bit overwhelmed. Their First thing I learned, though, is that InfluxDB has its own query language,  syntactically similar to JavaScript. Their data structure are more  complex than Redis for sure - I had to sift through quite a bit of  documentation to figure our their terms "Bucket" and "Organization"  which at first glance are not at all intuitively defined. 

##### June 16th 2021:

Today I wanted to get the pulse sensor plot working, and I decided to first see if I could set up the OpenBCI board up for the second Pi, not just the first. I immediately ran into a problem. The Pi wasn't detecting the dongle even though the blue light was on. It took some digging, but it ended up being a problem with file permissions and for some reason the ttyUSB0 directory defaulted to 660 every time I plugged in the dongle. I don't recall ever having this problem setting it up with the other Raspi, so I have no idea what was causing it. I tried to write a short script to detect this issue and fix it automatically, but the problem is a little deeper because the file that you are supposed to target for things like this (/sys/bus/usb-serial/devices/ttyUSB0) is actually a symlink, and *that* directory's permissions are fine. The problem is with the directory it dynamically points to. In the case of this Pi, it's this beast: /sys/devices/platform/scb/fd500000.pcie/pci0000:00/0000:00:00.0/0000:01:00.0/usb1/1-1/1-1.1/1-1.1:1.0/ttyUSB0
However I don't think there is a guarantee that this is always the case, or that I can find which file it points to every time. For now, I just manually changed the permissions on this file, and it seems to have stayed that way even after reboot. I've made a note in the relevant script about this.

Once the board was set up, I wrote the CytonStreamer class and the cyton_pulse_streamer.py bokeh setup file, creating two separate AjaxDataSource objects to poll for the raw data and the calculated heart rate. The CytonStreamer streams all channels from the cyton board, including the data from the analog channels, to which the pulse sensor outputs its data. I'm ashamed that I fell for the same trap I did last time when working with the analog channels: I forgot that the board has to explicitly be set to analog mode before every session, so I kept looking for errors in my database read/write methods to figure out what the problem was. Once I got that working, I wrote the CytonAnalyzer class - then it was just a matter of implementing the rudimentary heart rate calculating algorithm (just a simple moving average with peak detection on the pulse sensor). The only major issue I ran into is that the heart rate doesn't naturally have a timestamp associated with it, so I used the timestamp from the last data point from the pulse sensor raw data stream. The problem with this is that the data points aren't very evenly spaced, since the analyzer is just run on a loop and not regulated on an exact schedule. This isn't a huge problem from a data collection standpoint, but I wonder if that will affect any ML algorithms we might try to use. In any case, I would want to have a much more reliable heart rate algorithm first.

##### June 13th, 2021:

I wrote the run_pi.py file for each Raspberry Pi and added those streams to the server_stream_config.py file, and everything works as expected. Each stream is separately viewable in the browser, and I could even combine streams from the two Pis into one page just as I intended. I'm very glad that there weren't any major unexpected problems. I did move the run_pi.py and run_server.pi out of the lib/ directory because I realized that every time I wrote that directory to the devices it would overwrite the device-specific code. I also added them to the git-ignore so they wouldn't be overwritten that way either.

Also I noticed that the CPU usage on the AWS is really high when running the app. I don't think redis is causing it, so it is likely in my code for analyzing the EEG data as it is being read from the database.

Turns out it was exactly that - specifically the FFT. Since the previous version of the app I increased the rate at which FFTs are performed from once every two seconds to multiple times every second. I figured more data in general is better, but I guess the AWS instance couldn't handle that much. I've dialed it back a bit to about twice per second, but upon further investigation it looks like the main cause of delay is writing the FFT data to the database, especially at high time windows, where the write operation takes 10x longer than the actual FFT. I might consider decreasing the rate of writes if this becomes a larger problem.

Also apparently the EEG filtering algorithm has been broken for awhile but I didn't notice because the widget data communication was also broken and thus never triggered the data filtering. Once I fixed the widgets I found that the modification to the database read_data() methods which I made yesterday weren't compatible with the numpy arrays that the Filtering algorithm outputs. That is now fixed using this disgusting one-liner: if hasattr(type(list(data.values())[0]), '\__iter__').

##### June 12th, 2021:

​	Figured it out! Turns out I just wasn't reading the full binary image data from the redis database. After I fixed that, the video stream worked! Best of all, the latency appears to be just as fast as it was before - less than a half second delay in the video. A couple things to note, though: I had to add a line in the JS that plays the video at 110% speed so that it is always caught up to the live stream. Otherwise, when the video is played it tried to skip to the live timestamp but never quite gets all the way there. The increased speed ensures it always catches up even if there is a momentary delay between the Pi and the server. Also I had a conditional statement in the webpage that only processed video frames if the tab was selected because I figured that would just make it generally more efficient when viewing multiple things in separate tabs. However, due to how H264 encoding works, when you switch back to that tab the image gets really garbled for a second because the stream may not have received any keyframes before any drastic changes in the video. I disabled that conditional for now, and I don't think it will be a problem because viewing multiple streams at once isn't really the intended purpose here.

​		I then went ahead and made a change that I've been putting off for awhile. I removed the reliance of the setup scripts on JSON config files. This is because I want to make the python interface more like an imported package rather than an application, so all of the config options (like server IP, port, database port + password, etc..) are passed into the Client as parameters in the python run files on both the server and Pi. The setup scripts no longer run any python configuration files as a result. The VCP detection thread I had previously running in the raspberry pi setup script is now a standalone helper tool. The intended use is to simply run it and make note of the dev ports that any given device uses, and hardcode those values in the python file. It was originally used to automatically define them, but I think that method was far too complicated, and this is much more straightforward.

​		I then also booted up a second Raspberry Pi to see if everything works as intended with two devices (in theory it should). After running the setup script, the Pi was able to function exactly the same as the first one. However when I tried both, I realized they had the exact same stream names initialized, which of course was the case because I just did a simple git pull. But because of the new system that I made to initialize the streams, I will have to write a unique run.py file for each Pi to give each stream unique names. Even though this is a little more hassle than I originally wanted, I think it is for the better because it offers more flexibility. I can now make a completely different set of streams activate from different raspberry Pis. I will write this file tomorrow and go from there.

##### June 11th, 2021

So I've been trying to get the video stream working for the last few days, and so far I've built the following structure:
		The Flask server has socketIO handlers for a "/video_stream" namespace, and the video stream html page has JS socketIO clients ready to connect. When a browser client connects, that client is added to a socketIO room with the ID of the video stream's ID. Any other browsers wanting to see that video stream will be sent to the same room. This is to avoid reading the same video data from the database multiple times. (Note here that this is not yet implemented in the Bokeh plots - each separate viewing of the data requires separate reads, but I will leave that to be solved when I move the bokeh plots to a separate dedicated server). Once a video stream room is created, a new thread is started which reads from the database and emits the video frame data to the /video_stream namespace to the room with the stream's ID.
		In theory, this should all be fine - however I am having trouble given the way in which the H264 stream is structured - there aren't individual "frames" of data, rather the H264 encoding sends information about what changes from frame to frame, with full key frames interspersed. However the streamer on the Raspi still treats it like a regular time series data stream, with each section of data being timestamped and put in a separate time slot in the database. I am not certain whether this is the cause of my issues (because in theory that should not be a problem), so I am continuing to investigate. Another possibility is that there is a bug somewhere else that is causing the stream to not work. I also had to add additional functionality to my database class, forcing it to not decode the data sent by the Picam. Normally python-redis decodes the data in utf-8 (because it's read in as raw byes), but obviously the H264 video encoding should not be decoded. 

##### June 6th, 2021:

So I rewrote the whole structure of identifying and keeping track of streams. Here's how it works:

​		Each stream has three defining properties. The ID, name, and group. The ID is a globally unique string generated and used to look up each stream in the database. The group is a globally unique user-defined string to identify a collection of streams, which corresponds to a single webpage on the server. The name is a locally unique string identifying a stream within each group. Like before, each stream gets two 'spots' in the database - an info key-val store and a data stream. The info hash stores various information like the ID, name, group, name of the hosting client, and anything else that may be necessary to know. The stream is the actual data stream. The info hashes are stored in the database with the key "info:streamID" and the streams are stored with the key "stream:streamID", where "streamID" is the ID of the stream. Now in addition, each group of streams also gets it's own spot in the database, identified by the key "group:groupName" where "groupName" is the unique name of the group. This is a key-val store that simply relates the name of this group's streams to their ID. This allows the browser to easily identify the groups in the database and create links to each stream page. When the stream page sends a request for a bokeh layout, all it has to know is that group name, and the server can get all the information about each stream within that page on it's own. The important result of this (which was not possible before), is that now streams from different streamers (no matter what device it's from) can be displayed on the same page. With this structure change I was finally able to make the EEG stream fully functional! Yay! I am very happy with this new setup - it is much more generalized than before, and I believe gives much more room for flexibility in the future.

Another important change that I made as a consequence is to fix the method of choosing which streamers to initialize. Before, you had to write a derived Streamer class in a specific file that defined the main execution loop of that Streamer. When the Pi client ran, it would look in that file and create an instance of each streamer it saw, and you could optionally disable certain streamers in the config file. This method is really just a relic from how I initially set this up for structural reasons that no longer apply. For this new structure, I needed a different method of initialization so I figured now would be a good time to change it. Now, the classes are defined in the same file, but instead of searching that file for class instances to create, the user manually creates as many instances of each streamer as desired and passes those instances into the Client class before running it. I am honestly not sure why I didn't do it this way initially, but I guess it's better late than never. 

The same method is done on the server side for the analyzer classes, however notably this means that the user must make an analyzer instance for each stream that needs one. Before this, I had tried to make is so that a new analyzer instance would be dynamically created if multiple instances of the same stream type were detected, but that created many more problems than it solved, so I decided to keep it simple, even if it requires slightly more organization on the part of the user. 

Additionally, the analyzer classes are no longer run only when it's target stream is detected. All analyzer classes are instead run right off the bat, and simply look at the database to see if their target stream is there yet. For the moment, each analyzer polls the database once per second to check to see whether their target stream has connected. Obviously I want to avoid polling as much as possible, so at some point I will construct some socketIO messaging triggers that would let the analyzers know when a new stream has connected so they don't have to constantly poll.

One thing that I was reluctant to do for this new setup is that I had to create yet another config file. This file relates the different streaming groups to the html file that the server should use when displaying them, and likewise for the bokeh layout file. I could not figure out a way to do this that didn't involve the user defining those values when initializing the streamers. I didn't want to do that because I'd rather have it all in one place, even if it means a separate file needs modifying in order to add a new stream.

Another consequence of this system is that, when writing the bokeh layout, the user needs to know a bit about the structure of the streams within the database, which is not ideal. This is because the item passed into the create_layout() method is a dictionary of dictionaries representing the structure of the group within the database. The outer dictionary has keys which are the names of the streams in that group, and the values are dictionaries of the info hash from the database for each stream. So, for example, if the user wants to define an AjaxDataSource which polls the ID of a particular stream, the would need to get that by indexing first the name of the stream then indexing the key "id" in that stream. Not a *huge* inconvenience I know, but I wasn't happy with it nonetheless. One thing to note is that it's possible this whole bokeh layout nonsense will be completely rewritten when it comes time to move all this stuff over to a separate Bokeh server. I'm not doing that anytime soon, but it should happen eventually to take advantage of the efficiency of websockets.

Anyway, it's been a wild couple of days. Next task is the video stream. Fingers crossed that my new structure makes this as painless as I hope it will.

##### June 4th, 2021:

Picking up from last night, I created a new data stream format for data types that are meant to be read in "chunks" as opposed to data that can be read consecutively. This is for the fourier stream (and headplot data stream), and I've written a couple new methods in the database class to account for it. The next task is to get the widgets working. At the moment, the analyzer socketio is not receiving the widget data and I'm not sure why.

After some debugging, I *think* the issue with the socketIO message is that something goes wrong when the namespace has hyphens in it. This is not intended behavior, and I found a github issue submitted a couple years ago that claimed to have fixed it, but for the JS client rather than the Flask-socketIO client. I have submitted another issue following up on it.

So after 4 HOURS of debugging I have found the issue. The issue is that I am stupid. Turns out that somewhere else I had registered the socketio.ClientNamespace class twice to the same socket, causing one class to eat the messages and the other to not get anything. After I fixed this, all the widget info is being sent to the proper ClientNamespace. The fourier transform and the interactable plot is now working great.

One more problem, though: The Filtered EEG signal is not the one being shown in the EEGStream. This is because I forgot to actually stream the filtered signal to a separate column in the database. I first fixed this by doing so in the main execution loop of the EEGAnalyzer, but that turned out to be a really bad idea because it is reading a fixed length of EEG data for the fourier transform, rather than only new incoming data. The solution to this is either to read from the EEG stream twice - once for the filtering and once for the fourier, or run a separate Analyzer that does the filtering by itself. I am inclined to try the latter option just to see if my organizational method actually worked - I haven't actually tested running multiple analyzers for the same stream yet. It might be overkill, but I'll take the opportunity to test my setup.

While doing this I found numerous errors, of course, so I'm working on fixing them. Most notably, though, the decision I made yesterday about whether to wipe the database between sessions has gotten more complicated. I still want to keep wiping the database each time (because otherwise streams will have to be manually removed), but now the problem is that when the database is restarted while the analyzers are running, they can't look for the info column from the stream they are meant to read from. A temporary fix would just be to ignore this and keep the information they got previously, and only update when the info becomes available. I'm not yet sure if this will have any consequences down the road.

And now another problem - the method I'm using to identify analyzer streams in the database is to take the raw data stream ID and prepend it with the analyzer name, so like "EEGFilterStream:12345". I do this rather than give it it's own ID because the bokeh polling needs to know where to get the data from, but it only knows the ID of the raw data stream. This way, all you have to do it tell it the name of the analyzer streams and it can find those streams with the one ID it knows. One minor problem with this, as I have just discovered, is that in the case of nested Analyzer streams (as I have just created with the EEGFilterStream and the EEGFourierStream), the higher level streams now had a doubly-prepended ID, so in this case "EEGFourierStream:EEGFilterStream:12345". Now this is an easy fix - all I have to do is make sure that each analyzer takes the ID from the original data stream, not the stream it's directly reading from. The real problem, however, is that I cannot add any streams to a page which is not related to any other stream on that page. So if I want to stream an interpolated EEG signal to the same page as all the EEG's that stream would have to somehow know the stream ID of the original EEG signals, which may not even exist. This indicated that I need to (yet again) redo the structure that I am using to identify different streamers and analyzers. I will begin with that tomorrow.

##### June 3rd, 2021:

So all the ideas I had yesterday were garbage. The Streamers have no way of knowing when the database disconnects, except if it tries to perform a read/write and the connection is refused, and I don't intend to implement any kind of polling. Instead I think I need to solve the problem on the server side. 

New option 1: 
	I will continue wiping the whole database as usual, but when the database starts up again, immediately send a socketio message to request that all streamers send their info package again. At the moment I don't see a downside but I'm sure something will come up. I'm gonna try this next

New option 2, incase option 1 doesn't work: 
	Instead of wiping the database each time it is shutdown for a new streaming session, I will instead only wipe the streams, keeping the previous stream info. This way the database can be restarted without any communication to the streamers and retain everything it needs. Of course, the dangerous part about this would be that the database may then fill up with info columns, which would have to be manually cleared.

I have now implemented option 1, and I think it's working well. I had to create two socket methods, "init" and "update". The init method is called when the server starts the database, and requests that the streamers respond with an init message. This lets the analyzers look at all the streams connecting and decide if new analyzers are necessary. The update method is sent by the streamers to let the server know that the info column in the database has updated, and the server also refreshes the links in the browser. The init method also calls the update method, but the reason they are distinct is because I don't want the analyzers looking at all the streams every time one of them updates - that would just be wasteful. 

Now that that is solved, I need to make sure that the Analyzer received the notification that particular bokeh buttons have been pressed on the stream. The main problem here is that the browser stream doesn't have access to the Analyzer's stream ID. The only thing that the browser knows is the ID of the stream sending the raw data. I think the way to solve this is make the analyzer stream's ID based on the raw data stream ID in a predictable way - like prepending it with some name so like "fourier:<id>". This would allow the bokeh layouts to specify which stream to send the button updates, but only knowing the raw stream ID. 

After implementing this, I discovered a major issue with how I am handling the fourier data. It isn't a time series data set, but redis treats it like one, so all the fourier data is being appended one after the other just like any other time series data set. I am debating whether to create a separate database stream types specifically designed for this, or to read it out of the database in a way that accurately picks out the data I want. I am not yet sure what the best course of action is. On one hand, creating a separate stream type in the database is more efficient but will also require yet another thing to change when a new database is used, and also allows for the possibility of other data types needing their own stream schema as well. On the other hand, reading the data out differently is less efficient but would be the same regardless of the database being used. In the long run, the former option is best, given that switching out databases is not the primary goal of this project. I'll work on that tomorrow.

##### June 2nd, 2021:

Today I FINALLY restructured the horrible way that I was running the startup scripts. I had been having trouble with python imports back in the original app design, so my solution was to have python files in the top level directory that just import the scripts I want to run. That way, any imports are always relative to top level. However apparently there is already a way to do this, but the python documentation on imports never bothered to make it clear. All I had to do was use the -m flag to make python treat the scripts like packages, and all my import problems went away. I feel so stupid but so smart at the same time. 

I've since been working on an issue that happens when a Streamer class disconnects from the server socketio. The server and all processes on it need the info column data in the redis database, and that data is normally sent before the Streamer starts it's main execution loop. However, when the redis database is restarted and the Streamer disconnects, the Streamer get's stuck waiting for it's execution loop to start up again rather than break out, meaning that it won't send the info column again afterward. I fixed this problem by having the Streamer also send this info when the start method is triggered, and that solved the issue until another problem came up today. The Analyzer class needs this info when it's own start method is triggered, but since both the Streamer and the Analyzer start at the same time there is no guarantee that it will be available. I have a couple options here - either put the analyzer on a separate namespace so that it doesn't start with the rest of the streamers (and only call it after the streamers have started), or have the analyzer wait until it sees the info column in the redis database before starting. Keeping in mind that I should make this code dependent on redis as little as possible to make any future changes easier, I think the former is the way to go. The problem there is that then I will have to make a separate emit for every other message sent to the streamers. Ideally, of course, the streamer would sent the preliminary info to the database immediately when the database is wiped. The problem with this, however, is that when the database disconnects the streamer doesn't exit the main execution loop, it it first sent a "stop" signal which just makes it halt and wait to start up again. If I want it to send the info again, it would have to somehow know that the database was wiped. I'm not yet sure how to do this, but I think my best shot would be to handle the database connection on a separate thread from the main execution loop. That is what I am going to try to implement next.

##### May 31st, 2021:

I have been struggling with an issue where data streamed to the browser appears to overlap. I had previously thought that this was due to the way in which Redis was keeping track of the previous read position, but it turns out I was just doing it very wrong. I am still keeping track of the read position manually (now part of my database class), but it turns out that is actually how you are intended to do it. Redis itself does not keep track of anything. What I previously thought was a method to keep track of the last read position was in fact not intended for that purpose at all. With this fix, the data *appears* to be not overlapping that I can see, even with multiple streams reading from the database at a time.

I then set out to get the EEG stream up (again), and so far I have the raw eeg data streaming to the browser. The issue now is getting the widget values from the browser to interact with the analyzer client running on a separate process. I first decided to do this by forwarding the widget updates through the Flask socketIO to a socketIO client on the analyzer, but this ended up being really messy, and adds an additional step to creating an analyzer class that has any interaction. Another possibility would be to use the Redis info hash, as the analyzer already has access to it. I'm fairly certain this would be slightly less efficient, but it would eliminate reliance on callbacks to update the information. However this also does not solve the problem of the extra step when creating an interactive page.

##### May 30th, 2021:

Started today by working on a method of streaming two different data columns in the database to the same browser page. This involved changing the database schema a bit, so now analyzer streams are formatted like "stream:some_name:uuid_of_original_stream". Note that the uuid is not the analyzer stream id, but rather the id of the raw data stream. This is because otherwise there would be no way for the create_layout function to find it, because all it knows is the id of the raw stream. This way a single bokeh layout can request data from multiple streams. However this method is not perfect as it still would not allow one page to request streams from unrelated data sources, like a sense stream and video stream at once. I think I would rather reformat everything so this is possible.

##### May 28th, 2021:

I started today by trying to get the EEGStreamer running, but I soon realized that I don't currently have a way to have the Pi send more information about the stream, like the sampling frequency or list of channel names. I solved this temporarily by sending this info along with the dictionary written to the database initially. 

More importantly, however, I also realized that I will need a way for the analyzers to differentiate between multiple instances of the same stream even on multiple raspis. The problem is that I am currently identifying streams by the class name given on the PI side, but this definitely won't work with multiple instances of the same stream. I decided to use the UUID package to generate unique IDs for each stream, and that is what will identify different streams on both the browser and in the database. In order to make this work with the Analyzer classes, I had to rewrite the Analyzer Client on the server to have socketIO instance that is notified whenever a new stream is connected to the server. The Analyzer Client receives the info that the Pi sent, including the unique ID and name. If the streamer name matches one of the Analyzers' designated targets, the Analyzer Client creates an instance of that Analyzer and runs it on a new process. This ensures that even multiple instances of the same stream type will have it's own Analyzer instance. 

I also had to redo some of the abstracted database internals, as I was having too much trouble handling connection errors. I decided to implement a small failsafe in the database read/write methods themselves: if a ConnectionError is encountered while reading or writing, the database automatically tries to establish a new connection. Only if that fails does it let the Error propagate through to the outer scope. This still leaves me with a bunch of try/catch clauses in the flask app which I am not happy about, but I'm not sure how else to go about it.

Unfortunately, I was still not able to start on the EEG stream because of all this, but I will try again tomorrow.

##### May 27th, 2021:

(really this was from last night but I was too tired to write up a log)

I have gotten the SenseHat stream functional - it was relatively similar to the random data stream that I have been using for testing.

The tricky part is the EEG stream because it requires processing on the server - fourier analysis and signal filtering. To prepare for this, I decided that I would attempt to emulate an external algorithm manipulating the data in the database. To do this I re-structure my Node classes to allow a Streamer to run on the server instead of just a Pi. This streamer acts just like the streamer on a Pi, except that it reads data from the database, modifies it in some way, the streams it back to the database. As a result, in the browser we see two streams for that data - the raw stream and the modified stream. Of course seeing the raw stream is not necessary, so for the EEG stream only the modified data will be viewable. For testing purposes though, I have both the raw and modified data for my random testing data streamer.

Building this also let me discover some other problems with Redis: it doesn't work well with requests from bokeh. This is because Redis' built-in method for keeping track of what data has been already read from a stream assumes that data will be delivered in the order that it returns it, which is not the case for when the data is being sent via a response to an HTTP request by Bokeh. I was able to get around this issue by instead manually keeping track of the last data point read using the session variable, and updating it with respect to the bokeh plot rather than redis. Even so, this still does not completely solve a similar issue that I mentioned awhile ago where too-fast requesting of data leads to data overlapping in the browser. This is not something I want to try to solve yet as it can be avoided by lowering the polling frequency of the AjaxDataRequest, and it doesn't affect the integrity of the data at all. Hopefully this is something that will be solved by a more specialized time-series oriented database.

(actually from today)

I was planning on getting the EEG stream working today but I decided that I should abstract the connection to the redis database sooner rather than later, because the longer I go without doing it the harder it will be in the long run. The EEG stream will have to wait for tomorrow.

I created a Database class that can be accessed by the Streamers and the Flask app, and it implements the necessary read and writes that I've been using with redis. I also implemented a custom exception that can be handled outside of the function call without relying on any of the redis-specific exceptions. Hopefully this will make any transition easier in the future when I decide to switch to a different database. It's not perfect, but I tackled a lot of issues that I would have had to deal with later anyway. My only concern is that I have no idea what sort of information any other database system will need, or even if it will be capable of both a time series data stream and a key-value store like redis. I suspect that it's possible I may even continue using redis as a key-value store if the other database isn't built for it. In that case, swapping out the redis stream functionality alone should be fine. Ill just write another class to deal with the other database and swap out those method calls in the flask server.

I was also able to make the browser's list of connected streams live. By that I mean that when you do hit 'start' or 'stop', the list of currently connected streams updates live. I kept the 'refresh' button in there just in case for not, because the streams don't always terminate fast enough. 

##### May 22nd, 2021:

One problem that I discovered with my redis-streaming setup from earlier in the week is that I have no way to communicate with the Raspberry Pis once the stream has started. This is because I am no longer using my socket-based application to send commands like when to start and stop. This is a problem because I want minimal interaction with the Pis themself - everything should be able to be done in a web browser connected to the server.

It was at this point that I started looking into the Flask-socketIO package, which is a flask extension of the  python-socketIO package, which is in turn a python extension of the socketIO library. This package would allow me to essentially replace my entire socket-based messaging system with an external library. Ideally, I would be able to use this to send commands back to the raspberry Pis to notify when they should start and  stop the data-read loops. As I was researching this, I also discovered that the socketIO library can be used as an implementation of a websocket protocol for webpages - this is also really fortunate because I will need a way to deal with websockets for the video stream.

Getting flask-socketIO and python-socketIO functioning was rather difficult, as I had to integrate it into my current stack of Gunicorn + Redis. I ended up choosing Eventlet for an asynchronous worker (at the recommendation of the Flask-socketIO documentation), and was able to get the gunicorn application running with it. However, one problem is that Redis is single-threaded, so it does not benefit from Eventlet workers. Eventually I would like to move away from redis to something more robust like InfluxDB, but I am sticking with redis for now due to it's simplicity.

Also it is important to note that I had to use a previous version of Eventlet (0.30.2 as opposed to the current 0.31.0) due to a bug in the current patch that breaks with gunicorn.

After finally getting this stack to play nicely with each other, I was able to implement a command system on the website. The Flask server has a defined set of events that can respond to socketIO messages from the Streamer classes on the Raspberry Pis and a connected web browser. I made some simple buttons on the browser that send commands to the server which then relays the messages to the Pis, giving me a method of communication from the browser to the Pis. 

Finally, now that my socket-based data transfer layer is no longer being used, I cleaned out everything related to that library (which was painful yet cathartic because I spent so much time on it over the last year). All that remains is the threading/process structure that runs on the Pi's and runs the streamer classes on separate processes. Eventually I would also like to replace this with another library, but it is not causing any problems at the moment so I will leave it for now.

##### May 19th, 2021:

Today I was able to make a functional test stream of data from a Pi to the server without using my data transfer application as an in-between. I did this by connecting directly to the redis database from the Pi. I had no idea whether this would be faster or slower, because while this bypasses my data transfer code, it also might introduce network latency into the redis operations (but I can't be sure, I don't know how redis handles it). The two test streams (one using my data-transfer app and the other connecting directly to redis) appear identical as of right now, so I am going to continue using them both until I can tell which is better.

I also worked on adding buttons to the website that control whether or not the streams are active. These buttons start and stop the redis database server and also control where the redis database dump file is moved to afterward. This effectively means that starting and stopping the database separates the streaming sessions into separate dump files.

##### May 18th, 2021:

Today I was able to get a synthetic data stream functional on the new Flask+Redis system. There were some unfortunate problems with Redis that I did not predict. Everything read from the database must first be converted into a python dictionary by looping over each data entry and adding it to a list while also converting it to a float, which is extremely inefficient. There does not seem to be a way to natively do this in redis-py. I do not yet know if this will be a major issue as haven't yet tried multiple high-capacity streams at once. Luckily, this should not be as a much of a problem with the video stream as each frame should just remain as bytes until it reaches the browser where it is displayed - no data type conversion will be necessary at any stage.

Also I have noticed that due the the asynchronous access that Flask has to the redis database, requests are not guaranteed to send data to the browser in the right order. I could fix this by serializing access to the database, but I am afraid that will cause problems in the future, so I'd rather it be a last resort. This only seems to happen with AjaxDataSources with high polling rates, so I would like to try and pull the same trick that I did awhile ago with the EEG stream, where the browser receives large chunk of data and scrolls the plot to give the illusion of continuity. This will allow me to reduce the polling rate of the AjaxDataSource.


##### May 17th, 2021:

Over the last semester I have been working to rewrite the entire application to use high-level infrastructure. I have chosen to use Flask as the front-end to replace the browser oriented functionality, and Redis as the middleware database to store the sensor streams.

As of today, I have rewritten the application to use Flask and Redis, however the sensor streams are not yet fully functional. I am currently using my nodal structure and sockets to stream data from the Raspberri Pis to the server, but instead of storing the data in my circular buffers, the data gets dumped into the redis database. Eventually I would like to completely remove my nodal structure in favor of streaming the data directly from the Pis to the redis database on the server, and I will begin work on that once I have the current setup functional.

I am starting a new branch on the GitLab dedicated to this rework, called development_branch, as I hope this will become the main focus of all further development. 

My main focus right now is to create a reasonable structure within the redis database in which to store the sensor streams. My current idea is to have each sensor stream in a separate "STREAM" data type in redis, denoted with the key "stream:StreamName". There will then be a small table associated with each stream denoted by "info:StreamName" that will hold any information that the Flask application needs access to, such as what type of data the stream holds and whether it is currently active. I also need to create a method to separate out streaming sessions by storing a session to disk when it is stopped and starting an empty database for each new session.



##### January 15th, 2021:

Today I fixed an issue with the EEG x-axis starting at the wrong location when first entering the EEG stream page. Additionally, the y-axis range now automatically adjusts to match the range of each EEG channel individually (using the handy figure.y_range.only_visible attribute). To do this, however, I had to remove the panning/zooming tools as they disabled the auto-scaling.

I also fixed an issue with the color mapping of the head plots. It turns out that adding a non-default tick formatter to the colorbar breaks the setter function for the low and high color mapper values (I used a custom formatter to increase clarity of the tick marks). According to the bokeh documentation, this is intended behavior for some reason. To work around this, a new ColorMapper object needs to replace the old one each time, which is a little annoying. 

I also experimented with bringing back the spectrogram, this time utilizing the new radio buttons I added yesterday to select an individual channel. However this means that the full spectrogram data for each channel needs to be sent to the browser, and each image rendered individually. This significantly decreased performance so I decided to leave it out. Incidentally, I did figure out why occasional blank spectrogram slices were appearing - it's because I'm using my DataBuffer class, which only stores one time instance. When switching to the RingBuffer, the blank lines were gone but my RingBuffer class is not designed to handle 2D arrays. So, if I were to implement the spectrogram figures in the future (which I doubt), I would need to subclass the RungBuffer class and reconfigure how data is written into it's internal buffer.

Tomorrow I will look into using msgpack to send data to the browser to decrease the payload size. This will require the use of an adapter function for the AjaxDataSource to understand it, and I'm not sure how much of a performance decrease that will entail. I expect that it will be a net overall positive.

I also have recurring unsolved issue where the server and pi processes fail to terminate when issuing a keyboard interrupt. I am unsure of the cause and would like to do some digging.

##### January 14th, 2021:

I have solved the issue discussed yesterday. The simplest solution I found was to instruct the server to reject any data requests received before it has finished sending the previous one. This prevents any data being sent out of order. 

I was also able to add a "smoothing" feature to the EEG stream. Data chunks are still sent at the same interval (once per second), but the plot now incrementally slides the x-axis range to give the appearance of continuity. This effect is a little choppy at the moment due to the unpredictability of how long it will take for the next data chunk to load in. In the process of adding this feature, I also removed the clutter of numerous figures for each EEG channel in favor of a single figure with toggleable buttons to select which channel to view. I believe this increases performance since only one line plot must be rendered at any given moment.

I also updated the x-axis time tick marks to represent time since the start of the stream instead of raw epoch time.

##### January 13th, 2021:

I have finally found the source of the EEG stream issue (data was occasionally coming in out of order, causing the line graph to jump back and forth across the time axis). I thought it was a problem with the threading control on my RingBuffer, but no amount of locking and queueing fixed it. Eventually I took a look at the requests coming in to the page, and it turns out that sometimes a data packet will take just long enough to send that the AjaxDataSource from boken sends another request, and gets a response before it's finished. The result is that the packet directly after gets loaded in first, and the one that took too long get loaded in after, causing the time jumping. 

The solution to this should be to make sure that the data being sent is no more than necessary. I believe the reason it was taking so long is that the entire contents of the RingBuffer were being sent, when the only data needed is that which can be displayed at once in the EEG figure. This, however, does not account for the possibility of a request taking too long for some other reason. In that case, the only solution would be to slow down the AjaxDataSource. I am not sure whether it is possible to change the polling speed after initialization, but I will give it a try.

##### January 7/8/9, 2021:

Over the last few days I have managed to implement H.264 compression on the video streams. I found an open source JS script that decodes H.264 frames in-browser. Using this in-browser decoder requires the use of a WebSocket protocol, so I implemented rudimentary WebSocket handling on my server (this is what took most of my time). No extra WebSocket protocols or extensions are supported as of yet, but full encoding/decoding with fragmentation was necessary to support the H.264 stream. On the bright-side, I have now memorized the entire byte structure of a WebSocket frame.

With this compression, video stream sizes have been decreased by roughly 80% (yay!!), and there is no observable decrease in performance when running two video streams simultaneously. 

##### January 6th, 2021:

It turns out that the stream format I'm using (mjpeg) is already mostly compressed. I tried using jpeg compression to reduce the quality and it actually *increased* the image sizes. I found that the Picam is capable of streaming the images in h264. which is much more compressed. If I could stream this format, I predict that the network lag issues would be completely solved. The trouble is that h264 is not supported natively in browser windows like the mjpeg stream is, so I have two options: 

- Decode the h264 stream into mjpeg images on the server, then stream to a multipart/x-mixed-replace stream as I have been doing.
- Use some external pluggin in the browser to be able to decode h264.

As of right now, I am not sure which is best. I am currently researching what sort of programs I would need to decode h264. I could use ffmpeg, however that program is significantly larger than the entirety of my application, so it seems rediculous to use it. I would prefer a direct, lightweight solution. I have found a couple github repos that claim to do the trick, so I will have to try some of them out first.

##### January 4/5, 2021:

I have been digging into the source of the video stream issues with multiple pi's at once.

I was initially convinced that there was an issue with my DataBuffer class, which is used to store each image frame after it has been received and before it is sent to a browser window. I had designed the  class to be thread-safe, so it requires careful locking to make sure no race conditions occur. I thought this was slowing  everything down, so I made some optimizations with how the data is  handled on the server. This did not help.

After more digging, I was able to pinpoint the exact line of code that  is causing the problems - it is the socket.send() operation on each  Raspnerry Pi. This method is from the python socket module, and it is  what sends the data through the created socket for the stream. I did a bunch of testing, and adding another Pi to the  stream consistently slows down this operation on the other Pis, though in sort of a strange way: Rather than slowing down all socket.send()  operations, the effect appears to be cumulative. For example, it might send 5 consecutive packages all at 0.3ms, then one  that takes 500ms. To me, this seems to indicate that only a certain  amount of data can be sent through my network from the Pis in a given  amount of time. I also made sure that this was the issue by running my program with one Pi on the 5GHz band of my network,  and one on the 2.4GHz band. In this setup, the two Pis no longer affected each other, confirming that the server is not the bottleneck.

I think this is also consistent with the drop in performance I noticed  since moving back home from MSU. On campus, while individual devices are given limited upload bandwidth, the network as a whole must have a  massive total bandwidth to accommodate the campus population. As for my home network, the total upload bandwidth is going  to be significantly affected by each additional uploading device. 

I am going to try to apply some image compression as Dr. Ghassemi recommended at our last meeting The hard part will be detecting when the compression is  necessary. My current plan is to have the server intermittently report  how many images it received in the last few seconds, and let the Pi decide how much to compress the next number of images.

##### January 2nd, 2021:

I've done some quality testing with a second raspberry pi at the same time, and most of the functionality appears stable, apart from increased lag on both VideoStreamers. This is further evidence that I need to re-write my DataBuffer class to be a little more robust to high volume traffic on slower connections.

Additionally, with the new Streamer class structure, I need to do checks at the beginning of each request method to make sure it does not get called twice (most importantly for the START method). This would be much easier using function decorators, but I will leave that to another time - perhaps after we migrate to Flask.

I have also looked into an issue that cropped up yesterday where the pi-side application would not start on boot as it should. The cause was the automated git pull. Evidently, on boot the pi is not able to access github for some unknown reason. I added a delay of about seconds to ensure that the pi is fully booted, but it did not seem to help. A simple workaround may be to just skip interacting with git when the pi first boots, and instead an an option to manually update later. 

##### January 1st, 2021:

I figured it out! The problem was in the raspi Streamer class, not the server Handler class. The while loop streaming the data would stop when reading from the stream (i.e. the picam and the OpenBCI dongle), but not exit the loop because the read is a blocking operation. Then when the stream started again, the same while loop would continue while a second while loop started in parallel. This created two identical streams, which bogged down the receiving data buffer on the server. To fix this, I rewrote the Handler classes to have separate START() and loop() methods. The START() and STOP() methods toggle the streaming condition (actually a multiprocessing Event() object), and the loop() method is automatically looped in the background, ensuring that only one instance of the stream exists at a time.

While this does solve the issue that I've been hunting for the last two days, it does bring up concerns about my DataBuffer class. Theoretically, it should not have performed the way it did when the amount of information doubled. I am noting in my TODO list that I want to implement a more efficient buffer - possibly one that drops data if it is receiving too much so it doesn't lag behind. Ideally, I would want this to be the equivalent of reducing the framerate of the video stream so it can keep up with current data.

##### December 31st, 2020:

I looked directly at the data saved to disk from the EEG stream, and it looks fine. The issue I am observing appears to only affect the data when displayed by Bokeh - still unsure why. 

##### December 30th, 2020:

The remote start/stop is still giving me trouble. I've fixed the issues with Bokeh, however some others have cropped up. 

Specifically, the VideoStreamer crashes after a starting up a second time. I cannot pinpoint the problem, but the error appears to be occurring in the data buffer class that I'm using the store the images before they are sent to the browser. Either the buffer itself is encountering an edge case that I did not think of, or there is a deeper threading issue going on.

There also appears to be a problem with the EEGStreamer, which also happens after starting up the second time. It looks like the data it receives from the server is out of order and missing chunks. This could very well be the same problem the video streamer is having, but I cannot be certain. 

##### December 29th, 2020:

This week I'm working on tackling some small things that should make the development process a little smoother:

- Automatically do a git pull on each application start
- Add the ability to remotely start/stop the streams from the browser window
- move the application to its own environment
- set a group process id of the application that can be targeted to kill all processes associated with the app
- send an email if something crashes

The automatic git pull wasn't difficult, although it assumes that the necessary credentials have already been stored, which may not always be the case.

Remotely starting/stopping the streams is easy to implement in theory, however I am having a lot of trouble getting Bokeh to work when the stream gets interrupted for some reason. I also had to rewrite how the OpenBCI board and Picam are initialized.

Moving the application to its own environment is also proving to be difficult because I am getting permission errors where there should not be any - I have not yet found the source of this issue.

##### December 18th, 2020:

I have finally discovered why the python logging module doesn't support multiprocessing. Evidently using fork() to create new processes has a chance to deadlock the program if it inherits an already-aquired lock from a thread. I discovered this after I created this exact scenario in my own implementation. I have since been able to work around it (I hope) and have fixed the deadlocking issues with the LogStreamer class.

A small detail is that the LogStreamer sends the raspi log file ever 60 seconds, but if the pi crashes in-between, the remaining log file isn't sent until the pi is run again. In order to fix this I will need to force the LogStreamer to send the log before the process shuts down, which may be difficult as the cleanup() funtion is at a lower level than the Streamer class operates. 

##### December 6th, 2020:

I have pushed forward with the "streamed logging" that I proposed to Dr. Ghassemi last week. The idea is that the raspberry pi periodically sends it's logs to the server, rather than store them on the pi itself. This reduces further the chance that ssh'ing into the pi would be necessary for the user. I have written a sort of 'proof of concept', but it is far from perfect. A number of issue cropped up, mostly having to do with serializing access to the log files in the multiprocessing module. It seems that python's logger module does not support this, so I had to work with my own logging mechanism. 

##### November 30th, 2020:

I want to set up a python virtual environment so as to keep this application away from whatever else might be going on in the system, however I've ran into some off import issues that I have been unable to resolve.

##### November 22th, 2020:

The raspberry Pi now checks for the server to be available every 5 seconds (by default) by creating a throw-away socket and trying to connect. If the connection fails, then it waits and tries again. By throw-away socket I mean that it's not used for anything other than checking the server. I would like a better way to do this, but it was a simple solution and I don't see any immediate issues with it.

I have also modified the Pi client code such that it reverts to that 'searching' state when the server disconnects. This means that the server can restart completely and all the Pis will re-connect automatically. My plan is that the server will also have the option of sending a specific signal to the Pis to shut them down completely. However in order to start the pis after that, they will need to be turned back on manually.

##### November 19th, 2020:

Finally got those setup scripts working smoothly. All initial setup configuration is obtained from the user when they run pi_setup.sh, and every reboot afterward runs the appropriate python file to run the client.

Next step is to rewrite the Pi client to continually try to connect to the server when run. I'm thinking every 5 seconds is reasonable. Maybe that could be an extra config option?

##### November 18th, 2020:

Working on writing a python script to prompt the user for all necessary config info.

##### November 17th, 2020:

I need to rewrite my setup scripts to also include configuration for the pi. I started working on a bash script that does this, given the assumption that python might not be installed yet. I'm really struggling with string formatting in bash, and I'm wondering if it might just be easier to install python, then do all of this configuration in python. The only issue with that is I was hoping to make the user configuration the first thing that pops up, rather than "Installing Python." I tried for awhile to have a nice user interface with animated loading icons for the installations, but I gave up after hours of trying and failing to understand why any sane human being would create such a horrible language.

##### November 15th, 2020:

After the last entry, I needed to figure out a way for the raspberry pi to keep a static IP address, because this would need to be solved in order for our solution to work. I struggled to do this because the solutions I found online did not seem to work. I was able to achieve a static IP address, but only temporarily as there was always a chance that the IP was already taken. After I figured out why this was happening, I realized that there was no way to guarantee a perfectly static private IP without configuring the router settings, which the Pi does not have access to. There is a chance that some networks have an API for their gateway settings, but again there is no guarantee. 

This lack of consistency and general complicatedness led me to try to think of a better way to solve the problem. The main issue we are trying to solve is to avoid the need for the user to ssh into the raspberry pi in order to run the program. About part way through last week I had a realization - why not just have the program run on boot? That way *nothing* needs to SSH into the Pi, not even the server. The pi could just be turned on, the automatically start searching for the remote server socket. Once it's found the PI can connect to the server without any interaction from the user. Once the program ends, I just need a way to automatically shut off the Pi. This solves all the problems we have been having - both for user interaction and network complexity.

So for the last week I've been experimenting with different ways to start a script on boot, and so far the best and "cleanest" one, in my opinion, is to edit the Pi's root user crontab file to include a @reboot line that calls the startup script. I've cobbled together a bash line that does just this from many various  resources online:

(sudo crontab -l ; echo "@reboot sh ${startup_path}") 2>/dev/null | sort | uniq | sudo crontab -

The reason this is so roundabout is because apparently it's bad practice to directly edit cron files, so it's best to feed the line you want to  append into the crontab command instead. However, doing this normally  would just overwrite the cron file, so in order to preserver something  that may already be there, we need to copy everything in the crontab  with "sudo crontab -l", then echo out the new line to append on a new  line. The 2>/dev/null is to get rid of some messages that output if  the crontab file doesn't exist. The sort and unique commands prevent  duplicates incase the user runs the setup file multiple times. 

Now, the next step is to add a method to trigger a shutdown of all Pi's connected to the server from user input. I am currently thinking that a simple button on the website UI will work just fine. The server will then send a special HTTP request to the Pi's, which will trigger a shutdown. This way, the user will never have to ssh into any of the Pi's beyond initial setup.

##### November 2nd, 2020:

I talked with Dr. Ghassemi at length today about resolving the issues discussed last time. We decided that the best way to solve the issue of depositing the public key onto the pi would be the following: When adding a Pi to the network, instruct the user to change the user password to some string known by the server. The server can then ssh into the pi with that password and deposit the public key. Then the user is free to change the password back to what it was if they so choose. This solves the issue of making the user keep track of ip addresses while also circumventing the security problems - this known password could be generated randomly.

##### October 28th, 2020:

Today I spent my time trying to figure out how to write a bash script on the server that automatically SSH's into a raspberry Pi, executes some commands, then exits. The script itself is simple - just a call to ssh with following commands for it to run. However, there are a number of issues that this doesn't account for.

First, this script assumes that all the port forwarding for the pi has been set up. I have no idea how to do this automatically, as the only experience I have in port forwarding is through the user interface of my home network. This is also tied to the fact that there is no guarantee that the Pi will have a static IP address. I have tried to solve this but the solutions I have found online were both complicated and ineffective. 

Second, this also assumes that the raspberry pi already has the public authentication key on it. I tried to write a script that automatically sends the public key to the pi, but this script needs to prompt the user for the Pi's user password, which is not ideal. I have a few options for this:

- Prompt the user for the password anyway. The problem is that this will happen for each connected Pi, and it might be difficult to recognize which Pi it is by the IP alone. Essentially the user would needs to know which password goes with which ip address (again assuming the ip address is static, which I haven't figured out how to fix as mentioned above).
- Assume the raspberry pi has the default user password. This would work automatically for newly-bought and un-configured Pis, but in order to re-purpose a raspberry pi, one would need to reset the password to default, then change it back again afterward.
- Use a default public/private key pair stored on the git repo. This way each pi receives the appropriate public key when the repo is cloned, and the problem is eliminated. The issue is that this creates an obvious security vulnerability.

Lastly, there is the issue of logging on the Pi. RIght now, log messages go straight to the terminal. I want everything to go to a log file instead, but that log file will be on it's respective Raspberry Pi. In order to notify the user that something went wrong, a message will need to be sent to the server and logged there instead, but the ssh session to start the pi has already been terminated, so it must then be through the socket connection itself. But if that is the case, then if the socket connection breaks, and error won't be able to be sent. I'm not sure how to fix this, other than to warn the user that log files on the pi have to be manually located.

##### October 26th, 2020:

Dr. Ghassemi and I talked today, and we discussed getting the application startup and configuration to a minimal amount of user interaction. One exciting possibility is having a single script on the data hub server that SSHs into all the Pis and runs the program automatically. Another thing I want to work on is running automatic git pull updates whenever the program is started.

In preparation for this, I completely overhauled the directory structure of the application. This is because I want to make absolutely clear what config files go with what scripts, and that the server and pi code are completely separate.

##### October 25th, 2020:

I wrote a class called MovingAverage that I now use to keep track of the heart rate from the pulse sensor. I figured it could be useful in the future. I also am now compensating for the plateaus in the pulse sensor differently, using the distance and prominence arguments of scipy.signal.find_peaks() instead of the height threshold. It seems to be working much better, as it is no longer double-counting each side of the pleateaus.

##### October 24th, 2020:

I've successfully implemented a massive 1-minute buffer in which to save data and dump it in a csv file. My main problem was incorporating it into my RingBuffer class, but I essentially re-wrote it so that the total ring buffer size is 1 minute of samples, and the read_all() method is now read_length() with an argument determining what length to read. I also had an issue where data appeared to be mixed around in the csv file, but I believe I fixed that by adding a threading lock on the file itself.

I also confirmed my suspicions about the heart rate algorithm - it is indeed finding multiple peaks on the plateau that the pulse sensor detects. I still don't know why that plateau exists. I may ask Dr. Ghassemi to buy a new pulse sensor, or maybe I'll try to rewrite my algorithm.

##### October 18th, 2020:

I have set up the pulse sensor streaming, however I suspect that I have a faulty pulse sensor, as the data seems to hit some sort of peak value that it cannot go beyond. Theoretically, this shouldn't affect the data as this only happens when a heart beat is detected, but it is messing with my heart beat detecting algorithm, which uses scipy.signal.find_peaks. I will ask Dr. Ghassemi if he has any ideas on a better algorithm.

Setting up the pulse sensor wasn't that bad, although I did have to set a special configuration option on the Cyton board using board.config_board('/2') to send a '/2' byte. This configures the board for 'analog mode' where the AUX channels stream the data from the pulse sensor, whereas normally they would contain the accel data.

I also found a bug that I should have discovered earlier: if two browsers requests data streamed from my RingBuffer at the same time, the data is split between them. This is because the GET function in my handler classes use the same buffer 'ticket' regardless of what browser it comes from. To fix this, I will need to implement the ability to detect whether a request comes from the same browser or not. To solve this, the server will simply have to recognize different browser sessions. The trouble is that I have no at all written the server with this in mind.

In other news, I presented at the MSGC conference yesterday, and I think it went well.

##### October 14th, 2020:

I have finally *actually* fixed the 'chunked data' problem that I've mentioned before. The issue was that the EEG data appeared to be 'chunked', with blank spots between chunks. This was ultimately due to the FTDI driver settings. I fixed this by going to the file `/sys/bus/usb-serial/devices/ttyUSB0/latency_timer` and changing the contents from 16 to 1. I have also added some scripting to perform this automatically in the `install_pi` bash script. 

I have also received another Cyton + Daisy board from Dr. Ghassemi, and I've hooked it up with an OpenBCI pulse sensor. However, the process for streaming the pulse data is a bit more involved, as it is not collected through the same channels as the EEG data. After a lot of digging, I've found that I'll need to reconfigure the Cyton board in order to be able to access the pulse sensor data through the Brainflow APi. That will be a job for tomorrow (or more realistically, this weekend).

##### October 12th, 2020:

Over the last week, I've been struggling to fine-tune the filtering, as well as getting spectrogram head-plots function as Dr. Ghassemi requested.

I have implemented two filters: a bandpass (defaulted to 1-60Hz) and a bandstop (defaulted to 59-61Hz). I originally used a notch filter (scipy.signal.iirnotch) instead of a bandstop, and it didn't quite eliminate the AC main peak at 60Hz. The bandstop can, with only an order 2 butterworth filter.

I got the head plots to work, but there's no visual interpolation right now - just the band power at each electrode location. I downloaded a database of all projected electrode locations (pages/electrodes.json), so any configuration should work. As of right now I am calculating the band power as the mean of the FFT values. Dr. Ghassemi suggested that this might be too sensitive to peaking in the frequency domain, so I will do some research to see what would be best. He suggested using the median, so I'll try that first.

##### September 30th, 2020:

Apparently I was making it more difficult for myself than necessary - the SciPy library can actually solve a lot of the problems I was experiencing. Using scipy.signal.sosfilt, I can keep track of a set of Second Order Sections used to calculate a given filter. I can also keep track of a set of initial conditions that allows data to be chunked and filtered without signal degradation. I'm working on putting this into my program and adding some UI selection tools for it.

##### September 27th, 2020:

In doing research about signal filtering, I have found that the process is a lot more complicated than I thought. In order to filter live data (rather than all at once), one needs to perform a variety of computations. In the coming days I'll be trying to educate myself on the topic. 

A paper I am finding useful: https://www.analog.com/media/en/technical-documentation/dsp-book/dsp_book_Ch18.pdf

##### September 25th/26th, 2020:

I think I've come up with a nice solution to my problem on the 24th. I will keep track of the reader index relative to the head index. For example, a reader of 5 means that the next position to read data is at the head index - 5. That way, a reader of 0 will mean it's all caught up, and a reader equal to the length of data means it's at the very end, even though the two represent the same position in the array. 

I've written this new RingBuffer and am currently working out some bugs, but this coming week I should be able to fit it in nicely to the application.

##### September 24th, 2020:

I've discovered that this ring buffer is a bit more tricky to implement efficiently than I thought. Each last-read position has two edge cases that are indistinguishable. If the cursor is at the very back of the buffer (at the tail index), it is at the same spot as if it were at the very front (the head index). This is because, in either case, the "index that should be read next" is the same index. My only ideas on how to solve this break the conventions on how to implement a ring buffer, and I am afraid that doing so would make it more difficult for others to read.

I also discovered that python has no native shared lock object - everything in the threading module is exclusive. This poses a problem, as I would like all reading processes to freely read without interfering with each other, while being blocked by writes. I believe I have solved this by implementing a custom locking class that allows simultaneous reads but only one write at a time. 

##### September 23rd, 2020:

I discovered that Bokeh can use an AjaxDataSource to accept image patches, but only whole images at a time. Because of this my approach is to have each FFT slice in the spectrogram be a separate image patch. It took me a while to figure out how to do that, but I have a working version now. There are still a couple bugs, like how the spacing between images is just a little off, and there are annoying blank lines that run through the spectrogram. Also occasionally I'll get a completely blank slice altogether and I have no idea why. It might be due to the spec_time counter incrementing when it's not supposed to?

I also had a realization about those EEG plot blank areas that has been on my fix-list for months. I believe it's caused by the browser requesting updates at a slower rate than the stream is sending them, and my GraphRingBuffer only delivers the most recent packet of data. To fix this I need to rewrite my GraphRingBuffer to read data from any process' last read position, essentially using the buffer as a backlog in addition to storing a fixed amount of data. The problem is that each process reading from the buffer is going to have a different last-read position, so it will have to be kept track of separately. The buffer then has to keep track of the read positions of all processes and update them accordingly.

##### September 22th, 2020:

Last week I tried to install Brainflow on the data hub server, but doing so ended up crashing the server and I had to get Dr. Ghassemi to reboot it manually. Instead, I've been looking into using Scipy signal filtering instead. I've written some test programs to figure out the best way to implement filtering, but I've ran into the issue of performing these filters on the live data. Either I filter each data packet as it's received, at the cost of small window sizes, or I use a large window size at the cost of time. I'm not sure what the best option is yet. 

I then got started on implementing a spectrograph of the EEG data. My initial idea is to create a tabbed display that allows the user to switch between each EEG channel to view it's spectrogram. Plotting the spectrogram is proving more difficult than I thought, because the most efficient way (using the Bokeh image() plot) doesn't play nicely with the AjaxDataSource. If I cannot get this to work next time, I may just end up using a massive scatter plot, which is less efficient but more likely to work. Another possibility is a hex plot, but I think I'll run into the same issues as with the image plot.

##### September 15th, 2020:

I've read through a bunch of the Bokeh documentation on Widgets, and added some to the EEG streaming page. I wrote the widget definitions in a standalone file: lib/pages/eeg_widgets
In this file, each widget is given JS code to send a request to the server with the updated widget values. These will be things like the status of a button, position of a slider, selected drop-down item, etc. Right now I just want to use these to add interactive signal filtering selection. The filtering itself hasn't yet been implemented, but Brainflow provides some basic filtering/denoising mechanisms that can easily be applied.

##### September 14th, 2020:

Dr. Ghassemi wants me to implement a spectrographic analysis as well. That shouldn't be too hard - I just have to use my RingBuffer class for the FFT data as well, and display it in a heat map. I fine-tuned the FFT today and committed the latest working version. Before I get started on the spectrograph, though, I want to implement some better filtering and de-noising using Brainflow.

##### September 13th, 2020:

I have implemented the basic structure of a FFT display alongside the EEG stream, and so far it seems to be going well. I had a little trouble getting the Bokeh slider widget to work, but it turns out that you need to import the Bokeh widget JS script separately from the base release in the HTML for the plot. 

##### September 8th, 2020:

I've written a GraphRingBuffer class that will act as a circular buffer for EEG data, keeping a specified amount of data in the buffer to be used for the FFT. I wrote this buffer to be able to dynamically change size, so that I can hopefully add some interactivity to the browser FFT display. I believe that will have to be done using Bokeh widgets, and that's a lower priority then getting the FFT working itself.

##### September 7th, 2020:

Today I've been trying to figure out how to get a Fourier analysis to display along with the EEG data. In principle, I just need to perform an FFT on the server and sent that extra data along with the EEG data to the browser. The problem is that I now need to create a new DataBuffer that holds a specified amount of data (which can ideally be modified in real-time) from which to calculate the FFT. Additionally, I need to rewrite the Bokeh graph streamer class to accommodate more than one AjaxDataSource. I've started working on this today, and should have it done by the end of the week.

##### September 3rd, 2020:

I've done everything I thought of yesterday, and the problem still isn't solved. I also tried to repeat the bug on a different browser, and there was no sign of it. I now think this may be specific to firefox, or at least not all browser. Regardless, this means It may not be a problem with my code. The main reason I was concerned is because I thought it might prevent both a browser stream and a neural network from receiving the data at the same time, but now that the evidence indicated it's just a browser issue, I'm not as worried. This problem is still on my list of bugs to fix, but now is much lower in priority.

Next, I need to be able to test on multiple Pis, so I started trying to figure that out. On my local gateway page, I have the option of forwarding ports on my network, but when I tried to forward port 22, I was given an error saying that I could not forward the same port on multiple devices. I spent the next few hours trying to set a static IP on the Pis with no luck. After giving up on that, I finally went back to the gateway page and settled on forwarding port 2222 and 2223 to port 22 on two different Pis. This is not an idea solution, and I still cannot guarantee static IPs for the Pis, but it will have to do for now.

##### September 1st, 2020:

Today I fixed an issue that I've been having for some time regarding requests being sent unpredictably. I have been looking for a way to control whether a request is sent from the browser on a new socket or an old one, but I still have not identified a way to do that. Today I decided to get around this problem by allowing sockets to travel back from a Worker Node to a Host Node, if need be. That way, a socket can be transferred back and forth between worker and host when needed. I still think this is inefficient, but this transferring doesn't happen as often as, say the EEG stream polling the server 10 times per second. 

I also then tried to tack the issue where two tabs sometimes cannot view the same video stream at once. I thought this this indicated a problem with the DataBuffer holding the images, so I rewrote it so that any number of threads can read from the DataBuffer once each, rather than only one read per write. However this did not solve the problem, and I am getting the same behavior as before: 

Using the "duplicate tab" feature does not work, and the server doesn't even receive a new request. I first suspected that this had to do with cache because reloading the page in the dev console fixes it, but none of the cache-control settings seemed to help. I think there might also be a problem with the way in which a worker node received a socket. Tomorrow I will try running the receive_socket() method on a new thread so hopefully the _run_pipe() method doesn't block, preventing it from getting new sockets (which might be the problem).

When working on this, I also realized that I need to change how the NonBlockingDataBuffer functions. I need to allow multiple reads from different threads for that too, and right now the data is overwritten after each successful read. That should be easier, through.

##### August 31st, 2020:

I was finally able to solve the issue where the EEG stream was losing its column data - took awhile to track down but it turns out it was a simple scope error. I also cleaned up the time axis display on the EEG stream. I ran into an issue where sockets seemed to be shutting themselves down redundantly too many times in a row; even though this wasn't a problem because I have a number of catches for that, it was producing a lot of error messages. Turns out that the problem was actually that some sockets were continuing to run on the main Server host even after they had been passed to the Worker process, and shutting down the connection resulted in both sockets terminating. Once I identified that, it was a easy fix. There are still some situations where a socket might be redundantly shutdown (if the program itself is terminated at the same time as a socket broke off the connection), so I still have catches for redundancy.

##### August 22nd / 23rd, 2020:

This weekend I have been trying to teach myself the bash scripting language. Eventually I will need to write an install script that can get everything set up on the Pi or Server. 

##### August 21st, 2020:

The multiprocessing Pipe() worked like a charm. The Pipe() differs from the Queue() in that is has two distinct "ends". Each has a send() and recv() method that connects to the other end. Queues, on the other hand, have a single entry/exit point which was causing the problem from yesterday. I now have it set up such that a KeyboardInterrupt will trigger in all processes, but the main process can still shut down all worker processes if an error occurs. I have also added an option called automatic_shutdown that will automatically shut down a host node when all workers have terminated. At the moment, this option is only activated on the Client. The Server will still remain active, waiting for new connections. 

Now there is another problem I need to solve: Each process is designed to handle specific requests. This is so that separate data streams have a minimal effect on each other. Some streams, like the EEG and Sense data streams, send numerous requests from the browser to the server per second. Because those requests will be handled on a separate process, none of them go through the main server process which means they shouldn't interfere with any other server operations. However, sometimes a request for a server page is sent on the same socket as a data request. This means that the request gets sent to a process that can't handle it. I seem to have no control over whether a request is sent on a new socket or an old one. Ether I figure out how to do that, or I allow the data-stream process to transfer the socket back to the main process to be handled. The problem with that is it allows for constant transfer back and forth, which is serious speed constraint. 

I know I haven't been pushing my work recently, but that's because I've been writing and re-writing it so much it seemed disingenuous to submit all those in-between stages. For the moment I am happy with it's current structure, so I've committed all the changed made since I started working on multiprocessing.

##### August 20th, 2020:

I ran into some odd behavior that took me nearly the whole day to work out. I have the IPC Queue() set up in both processes such that they both listen for messages, much like a socket. When the main Host process received a KeyboardInterrupt, it sends a SHUTDOWN signal to the Worker, which shuts down then responds with a SHUTDOWN message in return. What I didn't realize is that the thread in which the Worker process is called becomes the new main thread for that process, allowing it to receive KeyboardInterrupts. This was unexpected as I assumed that the MainThread of the MainProcess was the only thread that could receive KeyboardInterrupts. Even more confusingly, that new main thread is not named MainThread, but rather the name of the thread that called the new process. To make this clearer for debugging purposes, I have since called the Process.start() method on it's own thread named NewMainThread. Anyway, because that new main thread can receive KeyboardInterrupts, it also triggered my shutdown() method. This made everything work as I expected it too, but only because both of these things sort of covered for each other, and I didn't realize it until I tested some edge cases. 

It appears that a multiprocessing queue doesn't have two distinct "ends", and my worker process was reading it's own messages. I am going to switch over to a multiprocessing Pipe() and see if that will provide the behavior I want.

##### August 19th, 2020:

I rewrote by Base class to include the basic error-thread functionality that I've been using thus far. Essentially, every class that inherits from Base can call self.run_exit_trigger() to run a separate thread that waits for the self.exit flag to be set. Once triggered, if the self.close flag is also set, then self.cleanup() is called. the cleanup method can be overwritten to do anything that the object needs to before shutting down. The reason I have separate self.exit and self.close flags is because sometimes I need to stop all the running functions in the object, but not completely shut it down. So far this is only used in the SocketHandler class, where I need to be able to stop reading/writing to/from the raw socket, but I don't necessarily want to shutdown the raw socket itself. This is the case for the issue I described yesterday, where I need to be able to send the raw socket through a process pipe. Also, you can call self.run_exit_trigger(block=True) to block the current thread instead of starting a new one. this is necessary when the object in question is running on the MainThread (i.e., a Client or Server). It is important to note that KeyboardInterrupts are ignored on any thread except the main thread, so if you want the exit_trigger() to catch it, one must be run on the main thread.

##### August 18th, 2020:

I found the source of a lot of the weird behavior that I've been seeing. Evidently when I transfer a socket to another process, it continues to run on the first process anyway, resulting in unwanted handling of requests on the first process. The solution would ideally be to stop the SocketHandler but not close the socket itself, remove that SockerHandler from the Host's index, pass the socket through the process Pipe, create a new SocketHandler with the socket, then run the new SocketHandler on the worker process. This would effectively "pause" the socket while it's being transferred. At the moment I have no way to stop the SocketHandler without shutting down the socket completely, so I'll have to re-write some of the SocketHandler. Either that or duplicate the socket, and shutdown the original.

##### August 17th, 2020:

Today was mainly just debugging. I ran across an issue where the initial "sign-on" request from the client streamer was being sent before the streamer itself was run on a new process. I know that's not a huge performance issue, but it feels like evidence of a bigger issue. I re-wrote some of that to give the startup procedure more flexibility. 

I also changed how the "source socket" is dealt with - it should not be necessary to run a handler. I want there to be room for a server handler that doesn't have a data stream (which would mean having a source socket). This kind of handler would be useful for something like a diagnostic stream from the server to the browser - something like a live feed of the memory or CPU usage. Ideally, it shouldn't be difficult to implement given this setup.

##### August 14th, 2020:

I decided that the INIT method problem from yesterday is best solved by just having a completely isolated SIGN_ON method that is called, and any other information can be sent afterward in the INIT method. 

I re-wrote the PickledRequest class (now called SocketPackage because I am never satisfied with a name for more than a day) to try and speed things up, but I suspect that this may be a bottleneck in my program. In order to speed things up, I will need to be able to reduce the amount of socket transfers that occur. This would mean dictating what requests (from a browser) are sent on what sockets. New requests pertaining to a specific client should only send requests to that client, and any other requests (such as for html pages) should be started on new sockets. As far as I know, there is not way to control this, but I will ask Dr. Ghassemi if he has any ideas. If I cannot speed up the process of socket transfer, I am unsure how I will be able to parallelize this program. the only parallelization I would have left would be the separation of the data-stream and the neural network training. So basically I really really hope that this will be fast enough. Hopefully I will have all the bugs worked out and I can start testing it.

##### August 13th, 2020:

Still going strong today. I've now completely redone the way that request methods are called by the Connection. I'm having trouble figuring out a way to call both the default INIT method and a user-defined INIT method. Previously I just had a default _INIT method that also triggered when an INIT request was sent, but with this new structure it just feels wrong to hard code that in. I might have to leave it for now and figure out a nicer way later.

I am also really happy with the way that the process-piping system I've built worked out. Each HostConnection and WorkerConnection has Pipe objects that connect to their respective host/workers. This pipe object has a reference to the process running the other end, that Connection's name, and the shared queue object. 

Another issue at the moment is transferring sockets from one process to another. I've created a PickedRequest class that extracts the raw information from a Request so that it can be send through a process queue. It also takes the raw socket object out of the Request.origin and sends that through alone, because my SocketHandler class cannot be pickled due to the threading Locks it contains. At the other end of the queue, the PickledRequest is re-constructed into a Request object, and the socket is given a new SocketHandler. I hope this process won't be too time-costly.

##### August 12th, 2020:

Alright I've re-written my Server and Client classes. Here's the new structure that I'm working with:

The Connection classes hold an index of SocketHandlers, and each run on a separate Thread. Theses threads run on an isolated Process, which uses a multiprocessing Queue to communicate with the main Connection object, which is the main server. 

The problem I am trying to solve right now is to figure out the best method of shutting everything down. Should the SocketHandlers be trusted to shut themselves down, or should they send a signal to the Connection that forces them to? Same situation for the Server and the Connection, except this time it's between processes not threads. Processes can be easily terminated, but threads cannot. My thought right now is that when the server shuts down, it will give each connection a time window in which to close all it's associated socket handlers. Once that window expires, the main Server will terminate the process, ending all socket handling threads. Which reminds me, I've got to figure out a better way to keep a reference to all the processes and signal queues than just a dictionary of tuples.

##### August 11th, 2020:

(I wrote down the wrong date on yesterday's log entry. It should have been the 10th instead of the 8th. Fixed now.)

I had a conversation with Dr. Ghassemi last night about our plans for the coming semester, and I explained my concerns about whether I should switch over to an API given that my code is a rather naïve implementation of stuff that already exists. We discussed the fact that this code needs to be easy to modify for others to add features, so organization is extremely important. I am finding it incredibly difficult to make this as "general purpose" as possible with the limited knowledge that I have, and an API might be the best way to do that.  Ghassemi also mentioned that it is not at all uncommon to write and re-write code many times over, which was reassuring to hear as I have been doing that frequently. 

I took some time today to go back and re-write some stuff that I think I've been holding onto for too long (another thing Ghassemi and I discussed). I took out some of the multiprocessing and got a working version that I am happy with - I am going to start over with the multiprocessing tomorrow.

##### August 10th, 2020:

Sigh... today was rather unproductive. I encountered an issue with the ServerConnection structure so I rewrote it again, but then later realized that I didn't actually need to do that so I just rolled it back.  I'm still trying to get this program to shut down. I've solved some of it, but the problem is that the SocketHandlers need to communicate to their Connection objects whenever they run into a problem so that the Connection can remove them from it's index of sockets. Right now I'm trying to do that by writing a callback function in the Connections that all sockets are required to call if it exists. This allows the Connection to do whatever it needs to in addition to removing it from it's index. This includes shutting down completely if the socket being shut down is the source socket, which means that the Pi disconnected, which means that any request aimed at that connection should not go through. However I still want the rest of the Connections to remain functional even if a single device disconnects. I'll be working on that tomorrow.

##### August 7th, 2020:

I have not yet mastered the multiprocessing module, it seems.  A major issue is that I can't seem to get the program to shut down properly. I cannot find any processed or threads that are still running. I think I will try to get rid of all threads first and get it working, then add them back if I can find the problem.

Lastly, I changed the SocketHandler so that new commands are no longer called on a separate thread. I am realizing now that this might have been the cause of some problems I experienced awhile back. The idea was to make all commands concurrent, but I am realizing now that the contents of the commands are not necessarily thread-safe, which would be up to the user to implement. For now, if something would take long enough to warrant concurrency, I will leave it up to the user to do that. Keep in mind that at this point the "user" is not an "end-user" in the general use of the word, but rather whoever inherits this code before it's turned into a working application. The main reason I have this concept of a user is because I don't yet know what kinds of data might need to be transferred and visually displayed, and I want my program to be flexible enough to incorporate anything I can think of.

##### August 6th, 2020:

I have decided on a way to deal with that inconsistency between the ServerConnection and the ClientConnection I mentioned yesterday. I am going to have the ClientConnection hold a reference to a source SocketHandler, just like the ServerConnection. From a development standpoint it seems kinda useless, but I think it makes a whole lot more sense from a user standpoint.

Anyway, I also realized that there is another issue with this setup. When a request is sent to the server and handled by one of it's Connections, the method that is called doesn't know which socket the request came from. To solve this I had to pass a reference of the requesting SocketHandler into the request object constructor. This is a *bit* hacky as the Request object is also used by users, and I would rather not confuse the issue by having a constructor argument that isn't used. I am thinking of redesigning the Request object structure into a couple different classes - one that is used by users, and one that is used in the code. I'll work on that tomorrow.

A more general problem I ran into is that the ServerConnections don't have a great way of passing along a request back to the ServerHandler if it can't handle the request. In theory, this should never happen as any request that specifies a connection ID should be intended for the ServerConnection, however I want to have a safety net incase (for some unknown reason) a request is received that has an unnecessary connection ID attached to it. Rather than throw an error, I would much rather the ServerConnection just pass the request along to the server, and only then throw an error if it can't be handled.

I would also like to make it possible to reuse as many ServerHandler objects as possible, swapping out the sockets for new ones as they time out (timeouts is another thing I need to implement). This way I would bog down my program with big objects being created and destroyed all the time.

##### August 5th, 2020:

In light of the information I got yesterday, I have been re-organizing my code to accommodate. The new structure is as follows:

The SocketHandler is essentially a wrapper for a socket, which reads and writes to it as all my Handlers have done previously. This class used to be the one derived and amended by users (to add request methods and such), but now the handler instead calls the request methods of it's parent object, defined by the connection parameter. The connection object can be anything which has methods that should be called in response to requests, but is most notable a ServerConnection object.

The ServerConnection class holds multiple SocketHandlers: The source socket handler is for the socket connected by the Raspberry Pi. There is only one source. All other sockets are stored in ServerConnection.clients, which is a list of SocketHandlers, holding sockets that are connected to anything else (presumably a browser or anything else that wants the data from the source socket). This is the class that should be inherited by users looking to create request methods on the server.

The main Server class, which is created in ingestion.py, holds an index of all the ServerConnection objects. The keys are unique IDs for each ServerConnection, which right now is just the IP:PORT of the source socket. 

The Server also has it's own ServerHandler object, which is a derivative of the SocketHandler. It is special in that it's handle_request() method is overwritten. Instead of simply calling the method specified by the request, this handler first checks to see if an ID was specified in the request query string (like so: 123:456:788/some-page?id=IDHERE). If no ID is specified, then the request is handled as usual (most likely a GET request for a server page). If an ID is specified, it looks for a matching ID in the index of connections. If no such ID exists, the handler then attempts to handle the request while ignoring the ID. If the ID is valid, then the socket sending the request is added to the corresponding ServerConnection's client list, and all further requests from that socket are handled by that connection, instead of the server. This is for cases where a streaming page may need to continually send requests to update the stream, as in the case of the plot streams. These requests on that socket will now go directly to the ServerConnection object, rather than through the ServerHandler first. 

Another special case for the ServerHandler is if the incoming request is an INIT request (reserved method name). In this case, a new ServerConnection object is created with that socket as the source socket, as only a user created client would send an INIT request. The ServerHandler looks for the required headers (class, device, and name), then calls the user defined INIT methods if it exists. Once done, subsequent requests on that source socket will be handled by the new ServerConnection. This, again, is so that the high amount of traffic over that socket does not go through the ServerHandler, but rather directly to the ServerConnection instead.

Essentially, the ServerConnections represent a connection to a single Raspberry Pi, and allows browsers to retrieve the data from it. The ServerHandler is what figures out which ServerConnection any given request should be sent to, and also handles general server unrelated to the data streams.

Finally, the ClientConnection object is the equivalent of the ServerConnection, but on the client side. I am still debating how to structure this, though, because it would make sense for a ClientConnection to hold many sockets, but that doesn't make any sense because any given client connection will only have one socket - to the server. It would then be logical to have the ClientConnection inherit directly from the SocketHandler, but I am afraid that the inconsistency between server and client structure may confuse users. One solution to this would be to rewrite the SocketHandler class to be able to handle multiple sockets (like with the Select module), then there would be no need for a ServerConnection class. The ServerHandler could just pass any number of sockets to another SocketHandler. However I would like to get this version fully working before I do something like that, as SocketHandler is the most difficult to debug.

##### August 4th, 2020:

Did more research to answer my questions from yesterday:

1) Yes. 
	On UNIX:  os.sched_setaffinity(pid, mask). 
	On Windows: ctypes.windll.kernel32.SetProcessAffinityMask(pid, cpu_mask)
	I'm not sure if the pid on the Windows version is actually the pid or something else. Doesn't matter, though, since this is going to be run on Ubuntu. I have also noticed that not manually setting the cpu affinity results in everything being run on one core. Apparently this is because some modules like numpy mess with cpu affinity. I will have to be sure to watch out for this.

2) I can run more processes than CPU cores available, and it looks like the extra processed are just scheduled as if they were threads, but on the machine level rather than on the Python level. I couldn't find a complete answer, but from what I have read I believe that running threads on each process would be less costly that running extra processes.

3) It looks like it is best practice to avoid sharing data between processes, which means that I should most likely find a way to move the socket itself to another process. On Windows, python's socket module provides a way to serialize sockets to be transmitted to another process. On UNIX, sockets can already be passed to child processed due to the fact that os.fork() exists, which allows processes to share file descriptors. I don't *entirely* know why file descriptors are significant in this context, but I will probably learn that as I work on this.

I also got a bit side-tracked today when I realized that the system I have been using to transfer files to the server is basically the same as what PHPStorm does to access a remote host. I spent some time trying to move my project over to PHPStorm, but it turns out PHPStorm doesn't quite have the same level of Python syntax highlighting. Also, since I have to run my programs through the command line, I eventually decided just to keep using SuperPuTTY as I have been. Maybe in the future once the server is meant to run more continuously, I can move everything over to make things easier.

##### August 3rd, 2020:

I spent the day reading the multiprocessing documentation and fiddling with test programs, trying to get a grasp of how this stuff works. I still don't quite understand how I am supposed to control which cores the processes are run on, but it's possible that python just takes care of that automatically. I did learn, though, learned that it is indeed the multiprocessing module that I should use, rather than the subprocess module - I was unsure about that until this afternoon. The subprocess module is for accessing any other type of program from inside python. I also learned that the best practice for this application will be to try and create processes which will run for the longest period of time, rather than starting and stopping processes. Unlike threading, that would be rather costly. The multiprocessing was built with similar usage cases to the threading module which made it easy to run, but I still don't quite grasp the behind-the-scenes conceptual stuff so I don't think I can fully make use of the efficiency it provides.

Questions that I need to figure out:

1) Can I control which CPU core each process runs on? If so, how?
2) If I run more processes than there are CPU cores, do the extra processes run like thread? Would it be better to use the threading module for extra processes instead?
3) Since sockets aren't serializable using pickle, what is the best way to transmit sockets to another process? Is it to duplicate the socket in the new process, or share the data being transferred instead?

##### July 31st, 2020:

Today I spent my time rewriting some of my program in preparation for parallelism. Moving forward, I think it is prudent at this time to implement multiprocessing rather than threading. I think this will take some time for me to figure out, but now that EnSURE is coming to an end I believe I will have the time.

##### July 30th, 2020:

Today I spent most of my time writing up my project outline and putting my poster presentation together, which I will present at MidSURE next week. I got a really nice demonstration of the EEG stream with eye-blinks.

##### July 29th, 20020:

I was finally able to get a working version of the Sense and EEG stream working. It took awhile to get the AjaxDataSource() properly working in Bokeh, but once I did it worked like a charm. I also wrote a wrapper class for the graph that allows for a Handler class to input a Bokeh layout object, containing any kind of plot layout. All the user has to do is create figures and glyphs just like in Bokeh, then pass it all into the GraphStream() object. After that, it's just a matter of writing new data to the GraphStream.buffer, and my program takes care of the rest. 

The biggest changes I made today were actually to the way that my handler responds to browser GET requests. I hadn't anticipated rapid-fire requests from a browser, so I had to throw some 'patchwork' code into my program to get it running. After I finish this poster presentation, I will certainly be going back and refactoring quite a bit.

And best of all, that nasty segfault is nowhere in sight.

##### July 28th, 2020:

Sadly I was not able to solve the segfault problem with matplotlib. 

However, I have discovered my next obsession: Bokeh. The Bokeh module is made specifically for streaming plots and displaying them using JavaScript. No more converting plots into images! It's taken me the better part of the day to acquaint myself with Bokeh, but so far I am loving it. Sometime tomorrow (hopefully) I should have a working version with the EEG stream. 

My main focus today was digging through the Bokeh module and picking out the right stuff that I need to display and update an image in a browser window. Basically it boils down to:

1) Creating the initial HTML for the page that will display the plot, with a designated div tag that Bokeh recognizes

2) writing the necessary JS to send continual requests to my server, which will respond with updated JSON containing the new data to be plotted each time

Bokeh takes care of the rest. I think I'm in love.

##### July 27th, 2020:

Progress was awful today. I tried to find the source of the segfault, only to realize it probably isn't where I thought it was for most of the day - I am no longer convinced that it occurs in the socket module. I  originally thought it was because of the timing of the send/receive cycle of my program, but that was proving less and less an issue as I tweaked various settings. The only think that  made a significant difference was changing when and how often a pyplot figure was being saved as a jpeg. This is simple speculation due to  correlation, of course, but after some searching it appears that segfaults are not uncommon when messing with matplotlib. This would certainly explain why the video stream does not have this  bug. I believe it has something to do with the fact that pyplot implements it's own threading, and it might be interfering with the threading in my program. 

I am going to try out some more stuff tomorrow. If I can't fix it by Thursday, I will put together the poster  regardless. Worst case scenario is that I scrap it all and switch to a different plotting module, but that may be risky given the poster deadline this weekend. 

##### July 23rd / 24th, 2020:

I had quite a lot of trouble setting up the EEG streaming on the Pi. It took me awhile to figure out (and through some help on the OpenBCI forums), that I had to compile the python BrainFlow module from source in order to be able to run it on the Pi's 32 bit system. Once I got over that hurdle, I was able to set up the stream from the EEG device to the Pi, and then incorporate that into my streaming program. I was able to stream the EEG data to the remote server with relative ease (thanks to me preparation with the Sense Hat data), however I encountered a bug that I mentioned on  the 16th: When the plot image is too large, the browser seems to cut off the bottom of it. I cannot for the life of me figure out what is causing this. The plt.savefig method seems to be working fine - the full plot can be saved to an image file on the server. I really need to fix this because I can't ignore it by reducing the image size like I did with the Sense data stream. There is simply too much data, and I need to see the full image. 

##### July 22nd, 2020:

Today I put together the OpenBCI EEG kit and did some tests with the bluetooth stream on my laptop. I had trouble getting the headset to fit correctly - I have quite a lot of hair so I had to mess with the headset a bit to get it to fit. I got it mostly working on my laptop, but have yet to set it up on one of the Pis.

##### July 21st, 2020:

Most importantly, I finally got the "chunked" data method working for the Sense Stream. However, I ran into a segfault about halfway through the day and didn't get much done beyond that. I believe it's coming from the socket module, which is a C extension. It seems to happen when the rate at which Sense data requests are sent to the server get above some threshold. I have no idea why this would be the case because I have the 24 FPS video stream running at the same time. Regardless, the server can't handle generating plot images at a very high rate, so I've sort of avoided both issues with the chunking method. In the StreamHandler Client class, the self.frames attribute controls how many data points are to be sent per request. There is also a time.sleep(0.001) inside the data collection loop to slow it down, but I would really prefer a way to get around the segfault without that. Tomorrow I am going to start putting the EEG kit together.

##### July 20th, 2020:

Today I was able to restructure my Server Handler classes so that each new connection can specify which custom handler should be used for it. I have been thinking about doing this for awhile, and I am glad I finally did it. This allows the user to create a completely separate class in ingestion_lib.py to match the class in collection_lib.py. The INIT request method can now be fully overwritten to include any initial data from the pi. Also, the special HTML() function can now be defined, which returns the HTML for the display page. This is so that each stream type can have a different custom page layout.

This restructure took some doing, but I was able to get it to work by first handling the INIT request on a basic InitServerHandler class which grabs the new Handler class name from the INIT requests and passes the request over to that class. This InitServerHandler class also has a GET method to handle browser requests. 

Tomorrow I will work on figuring out how to chunk the data sent from the pi so that I can increase the amount of data sent without increase the number of images generated, which is very costly.

##### July 17th, 2020:

No luck on the bug from yesterday. I can't figure out where the image is being truncated - it could be anywhere from pyplot.savefig() to being loaded into the browser. I also rewrote my custom graph and matplotlib Axis wrapper class to accommodate different plot layouts, which I hope will be sufficient when we add EEG data. I also tried to make the framerate a little more flexible, but I'm worried that the speed at which EEG data will be streamed is just too high; I may have to send data in chunks. Unless I can find a quicker way to render a plot into a jpeg image, the browser stream is not going to be very fast at all. I suppose that doesn't really matter in the long run since it's just there for observation purposes. With that in mind, I have been trying to find a way for the program to automatically adjust for slow connections. Possibly a flexible framerate? The problem with that is it would require ignoring some number of requests and keeping track of which ones. This is yet another case where the multipart stream seems like a better idea, but I still want to keep my options open.

##### July 16th, 2020:

I got a preliminary version of the SenseHat streaming to work. It's not pretty, but it works well enough. I'll try to polish it up tomorrow. I'd like to write a more general version of this in preparation for the EEG streaming, so I'll work on that if I have time as well.  There is also a minor mystery with pyplot's savefig() function. For some reason when the image being save exceeds 2^16 bytes, the image being sent to the browser gets cut off. I have no explanation as of yet.

##### July 15th, 2020:

Unfortunately that tangle of threading issues I mentioned yesterday has turned into a rather knotted mess. It seems that I had been relying on some rather circumstantial structuring when I had only one type of client handler class to worry about. Since adding the SenseHat handler class, I have been struggling to keep my threads in order and to make sure each handler exits properly. Before, the main thread simply ran the handler's run() method, then waited until an exit condition (keyboard interrupt or disconnection) was caught, then properly closed the socket and terminated nicely. Now, however, I cannot use the main thread to run both connection as they must run concurrently, so I have had to restructure everything such that the Client class can be handed multiple handler classes and create a connection on each. Now the main thread will run both handlers on new threads, then wait for an exit condition to be called. The main problem I am trying to solve right now is how to notify the main thread of said exit condition since it is no longer in the scope of the handlers themselves. My solutions so far have not worked.

I also spent some time putting together a small bit of user interfacing. My idea is that the first time collection.py is run, you will be prompted for the server ip, port, client name, and which Handler classes this particular client will be running. This information will be stored in the config.json file and used on subsequent calls. It took a bit of digging around in python documentation to find out how to pick out the names of particular classes in another file, but I'm happy with how it turned out. My goal was to keep any and all user editing confined to collection_lib.py and ingestion_lib.py. I didn't want for users to have to go into collection.py to add their new class to anything, so it automatically detects new classes added to collection_lib.py. I am considering doing something similar to this and the threading changes I mentioned previously to the ingestion side of things. This would yield the benefit of being able to split of server handlers into separate classes as well, and allow each connection to specify what data it will be transmitting. 

##### July 14th, 2020:

Today I spent most of my time fiddling around with matplotlib and working out how to implement the Handling option #2 that I detailed from yesterday. I wrote a class that implements a graph that can be added to without needing to store old data anywhere else. I haven't yet implemented discarding data that won't be shown, but I'll worry about that after I get it working inside my program. At the moment I'm trying to untangle some threading issues that I've run into - mainly error catching within the graphing class.

#####  July 13th, 2020:

Brainstorming day. Dr. Ghassemi instructed me to begin working on streaming audio and SenseHat data. I first looked into audio, but found that I might need additional hardware - a microphone or something similar. I also have no idea how I would get audio to play in the browser without a few more modules (to handle sycing with the video). Eventually I decided to put the question of audio aside for now. I then started to experiment with the SenseHat module to get a feel for how I might integrate it into my program. In principle it should be easy to make a complete sensor reading at some given frequency and send them in the a data stream, but I need to come up with a game plan before I start working on it. I did some research and have a few implementation ideas:

###### Initiating the stream:

The following options are mostly different in how to set an example for a new user of this program. I want adding a new data stream to be intuitive and only require code in a single file on the server and client.

1) write each Pi to automatically check for an installed SenseHat, and start streaming SenseHat information in separate requests along with the video stream on the same socket, with the method to handle SenseHat data in the main Handler class in collection_lib.py

2) Implement a separate SenseHat streaming method, and leave it up to the user whether to call it on the START signal from the server. 

3) Create a separate Handler class in collection_lib.py that handles only SenseHat data, and run that class in collection.py after the Video Handler. This would also mean that the SenseHat stream would be a completely separate socket connection from the video stream, so it might then be advisable for the server to be able to detect that it's coming from the same device so it could be displayed on the same page as the video  (wouldn't be hard - just have to look at the connecting IP address). I am leaning toward this option, as it would benefit the most from parallelism on both server and client so it might be the first one I try. I think it would also result in the most organized code.

###### Handling the data on the server:

These are separate options regardless of the options above. The main problem for either option is that converting a python plot to a jpeg image for each frame could be costly. It certainly won't be streaming at 10 FPS, much less 24.

1) Store each new data 'frame' to a buffer that keeps some specific length of history, enough to be plotted into and converted into a jpeg image to be streamed to a browser. The problem with this option is that I need to figure out where to store the data that needs to be re-displayed in the browser as time progresses, and the process storing and shifting this data around could be costly. I am debating whether to make the user handle this in collection_lib.py or write a default procedure in lib.py for any kind of pure data stream.

2) Possibly find some method to update and existing plot with new data, not needing to keep any old data. The problem with this method is that I'm not sure there's a nice way to do it using matplotlib. It would, however, eliminate the need to re-plot all the old data each time, which is much more desirable of course.

I began experimenting with ways to implement initiating option #3 with handling option #1, but I'm not liking it so far. I will be doing further research into handling option #2 tomorrow.

##### July 10th, 2020:

I did it. I solved python.

I got the server's CPU usage with one Pi down to 3%, and it scales exactly as expected up to 15% with 5 Pis. It turns out that I was correct in my assessment of by buffer system, and that using the sockets as file objects eliminated the remaining inefficiencies that I had. I am only disappointed that it took me so long to figure it out. Additionally, it seems even though CPU usage is very low now, the lag problem is completely independent; my guess is that 5 pis (and thus 10 streams) at 24 FPS and a resolution of 640x480 is too much to handle on one core. To test this I tried reducing the resolution and framerate, and sure enough all 5 were able to stream with no lag when both were reduced enough. I believe this problem may only be able to be solved using multiprocessing. Again, using a multipart stream instead of separate requests does not seem to make an impact on lag and only a small impact on CPU usage (~2-3%). However I am not ready to completely switch over just yet, as I am not sold on the benefits outweighing the cost, which is that during a multipart stream, nothing else can be sent from the Pi to the server. Maybe it can be that way, but there's a possibility that down the line I might want a Pi to sent variable requests to the server. Who knows.



##### July 9th, 2020:

I've come to the conclusion that my bytes buffers are the cause of all the CPU usage, or at the very least the cause of the lag that I'm seeing. I did a bunch of research on efficient methods of dealing with byte streams, and it looks like the method I was using is just about the least efficient possible. 

I've tried using an io.BytesIO() object, but the trouble there is that there's no clean way to discard data once it's been read. My solution was to inherit the class and implement a maximum size which, when exceeded, triggers a re-allocation - the current remaining bytes get moved to the front and overwrite the old data. I didn't see a big difference, so I scrapped that and moved on.

I also found that the socket object allows you to make a read and writable file object from it, which (I assume) means I wouldn't need an external buffer. The only problem is that reading individual lines becomes a little tricky because it doesn't have a peek() method. I started to figure out a workaround tonight, and I'll continue that tomorrow morning.

##### July 8th, 2020:

I implemented the threading condition on reading from the stream as well as reading from the buffer, and it made a big difference. I am pretty confident that this was the main issue because all of the strange behavior that I could not explain before is gone, for example that changing the framerate or resolution of the images did not affect performance. Now framerate and resolution are the main factors affecting performance, as they should be. I also noticed that increasing the maximum size that the server is allowed to read from the stream at once decreased usage by ~10%, which makes perfect sense. Fewer reads, less time running. To be clear, the CPU usage is still not ideal - a single stream to the server puts it at 80% usage, but this is significantly better than the maxed-out 200% that it was before. Additionally, the Pi that is streaming the data runs at ~10% usage as opposed to ~90% before. There is clearly still room for improvement, but whatever it is, it will be in making the data ingestion process on the server more efficient (for which I already have a few ideas), rather than the program's overall running efficiency. I am going to mess around with it a little more before moving on - hopefully the next step is to start building the EEG kit!

I also rewrote the error-detection system I had in place using threading conditions, which I think is much cleaner than before. 

##### July 7th, 2020:

I finished implementing the multipart stream from the client. CPU usage didn't change as much as I had hoped, but it was noticeable. I still saw the same behavior of additional Pis connecting only changing the CPU usage by a small amount. I then went ahead and switched over to blocking sockets and rewrote the pull/push loop to be on separate threads. I did not notice any change in CPU usage. I believe the problem may actually be with my pull/push loop itself - I may need to manually slow it down while it's not handling any requests. I will be doing this with a threading condition object on the in_buffer, and any process that needs to read from it must wait until there is content. This will require that each pull from the socket notify all waiting conditions.

I also downloaded SuperPuTTY for my laptop, and am extremely happy with it. It allows me to start up my ssh sessions with all the Pis and the Lightsail instance all at once, and I can easily organize the terminal windows however I want. It's built-in file transfer system using PSCP has a GUI that allows me to easily write code from my laptop, then deploy it to the server for testing. No more code deployment through git :)

##### July 6th, 2020

I have two ideas about what I could do to speed things up:

1) Use blocking sockets instead of non-blocking sockets. It's possible that one of the main inefficiencies is that my program runs in a loop until it receives data from the stream, which could be contributing to CPU usage. However it's also possible that blocking sockets just do that anyway to block - only way to find out would be to test it.

2) Rewrite my program so that the client sends image data in a multipart stream, just like how the server sends images to a browser. Currently, images are sent individually in separate requests, each being run on a separate thread on the server (this is intended behavior because I want the server to handle new requests on new threads). However, I think that starting a new thread for each image is likely very costly, especially since it happens 24 times each second at the current framerate. 

I decided to try working on option 2, since I think it is more likely to be the culprit. It turned out to be much harder than I thought, however, as rewriting my code in this manner completely destroys the nice "user experience" I was going for with my class structure. After much deliberation, I decided to just scrap the nice structure I had and try to get the multipart stream working. If it significantly reduces CPU load, I will then try to work out how to rebuild the class structure nicely. If it doesn't, I can revert to my last commit and try option 1. Hopefully I will finish reworking it by tomorrow and will know by then.

##### July 3rd, 2020

Dr. Ghassemi brought 4 Pis to me today, and I went to work setting them up. I had some of the same setup problems as with the first pi - evidently there was a Raspbian update that went out in November last year that caused upgrades to render the file manager non-functional. I had to do a full-upgrade on each of the Pis. After that I had some trouble ssh-ing into them due to a sneaky newline character at the end of the public key files. Took me longer to find than I'd like to admit. After everything was set up, I started up my program and tried adding a second Pi to the stream - it worked on the first try. However I did notice that the CPU usage on the Lightsail instance only went up by about 3%, even when I added the rest of the Pis. I'm not sure what this means, but it might indicate that something about a different part of my program is just really inefficient... I'll have to look further into it. Also At 3 Pis, there was some lag, and with 4 and 5 the feeds just got further and further behind because they couldn't keep up. I'll do some more testing on Monday to see if I can speed that up.

##### July 2nd, 2020:

Today wasn't all that productive in terms of writing as I spent most of it looking into python's multiprocessing module. It appears to be syntactically similar to the threading module, but with some restriction. For example, you can only share objects between processes that are picklable, which excludes some custom classes. There are options to give them the ability to be pickled, but I'm still figuring that out. There doesn't seem to be many examples of socket servers run on multiple cores, and none that I found were as complex as the one I am trying to write. This may be a difficult journey ahead. In the mean-time, Dr. Ghassemi will bring me more RasPis to work with and test my program with streaming multiple video feeds at once. I am a little worried, though, because I noticed that my program was using 99.3% of the AWS server's single thread. I bought it up with Dr. Ghassemi, and he said he was happy to upgrade - I suggested that we just move up to a server with maybe 2 cores. If we want to go all-out, the most cost-efficient option would be to build out own machine on campus.

##### July 1st, 2020:

The issue at the end of yesterday turned out to be a memory leak as a result of one of my connections streaming even when the socket was disconnected. There was an issue with some threads not getting the signal to exit, and I fixed it rather quickly once I figured that out. I would like to figure out how to implement parallel process for different connections rather than just threading, but it looks like the AWS server only has access to one thread. I'll bring it up with Ghassemi when that becomes an issue.

##### June 30th, 2020:

I went thought with most of the ideas I had yesterday - I split the Handler class into a Server and Client version, and moved the GET handler method into the ServerHandler in lib.py. The website now displays a list of links to all the streams currently connected, and each will redirect to a page displaying that individual stream. Eventually there could be a page that displays all of them at once, but I think I'll have to implement parallelism for that - I don't think threading would be enough. At the moment, each stream is identified by the unique ip and port that the pi connected through, and the way that the server knows which stream was requested is by parsing the path, which contains that address. This is a bit unconventional, so I'd like to change this in the future. Perhaps each link will go to the same page but with a different GET query? Really that seems like the same method except a little nicer to look at, but that may be worth it. Also, I changed the pi startup method a bit - the pi now sends an INIT request to the server, giving it some preliminary info like it's 'name' and framerate and such. The server handles this and responds with the START request, which the Pi then handles by streaming. However, I discovered after hours of debugging that the Pi cannot receive data when the server is hosted on the same network (i.e. on my laptop). It can still stream just fine, of course, I had simply never sent more than one request back to the Pi before. I found this out when I used the same code but hosted on the AWS server, and it worked just as expected. I modified the simple echo server to test and demonstrate this and got the same results, so I don't think it has anything to do with my program. I submitted a ticket to MSU IT asking about it, but haven't gotten a response. Also in the middle of working and running the server, I noticed an unexpected connection from an IP I didn't recognize. I looked it up, and it was apparently a common scraping bot. It didn't do anything, but I created a list of banned IPs and run all incoming connections through that list now. Once I got everything working, I tried running the stream for a long time and eventually it slowed down to the point where it was only a frame every few seconds, then the program ended with the message "Killed." This happened once before a couple weeks ago with once of my previous iterations of the program, but I'm not sure what the problem is. Figuring that out will be my task for tomorrow.

##### June 29th, 2020:

I have isolated the issue from Friday. Each new connection was creating a new Handler object, which is where I was initializing the frame buffer. Each time the frame buffer was re-initialized, it cleared the threading condition variables which stopped the stream because there was nothing to release the condition locks. With this in mind as well as my chat with Dr. Ghassemi this morning, I began to restructure a little. I will need to keep an index of all the connections coming in to the server and allow any other connection to access it. That way any incoming connection could read any of the data streams, which is what a browser connection will need to do. Dr. Ghassemi also mentioned that there should be the possibility of running many many video streams at once, so I also want to separate the videos on the website - maybe display links to each stream on a main page? This also made the GET method function in ingestion.py much longer as I have to handle requests for various links. I am considering moving the GET function to lib.py as the user probably won't have a reason to change it. This also brings into question whether I want to split the Handler class into a Client and Server version, as it is getting quite long and complicated. I have also considered making a separate class for each type of streaming connection, but I think that may be a bit much. I'll settle for making the derived ServerHandler class a bit more general, like having a data_buffer which holds the raw payload bytes, and an image_buffer which holds the mjpg images ready to be sent to a browser via multipart stream. These will both be written to in the method defined in the derived class, as it will no doubt be different for every new sensor data stream we add.

##### June 26th, 2020:

Success! I have now gotten the multipart stream to work (on the AWS as well). However I have discovered that Firefox is one of the few browsers that actually supports this particular content type (x-mixed-replace), so it does not work on Chrome, Safari, or Internet Explorer. I spent some time looking to see if there were alternatives to use, but nothing I found was easily compatible with my current program. Some solutions like using HLS or ffmpeg streaming would need a complete overhaul of my code, from what I could tell. Maybe another time. I have also noticed that the stream is only accessible from one browser connection at a time. Any additional connections stops them both. The causes me concern because I thought my program would be able to handle multiple connections - that was the whole point of this threading rewrite. This indicates that there might be something going on that I did not account for, so I will need to fix that before I start anything else. I'll be looking into it on Monday.

##### June 25th, 2020:

Progress! I believe I have fixed that nasty fragmented-data problem. It had to do with the way that python's socket module implements socket.sendall() versus socket.send(). The former calls the latter repeatedly until it is all sent through. Usually this isn't a problem, but I believe that somehow it was sending repeats when the data was too big. I am now using socket.send() exclusively, manually re-sending, and moving down my outgoing data buffer. It seems to have solved the problem so far. I am immeasurably relieved that something finally worked - I didn't end up emailing Ghassemi, so I'll just mention it at our next meeting. Since I got that figured out earlier in the day, I began working on the browser stream.  I have written a method in the Connection class that implements an HTTP multipart stream, but I haven't yet gotten it to work. There seems to be something I don't quite understand about how a browser sends a request and I will start by looking into what that might be tomorrow.

##### June 24th, 2020:

Today was frustrating. I fixed the bug from yesterday, and was finally able to get the "new" program running. The whole reason I rewrote the program was to try to solve the strange problem I mentioned on the 19th. However, even after a complete overhaul, the *same exact* bug still happened. The problem is this: The stream works fine until any single frame size reaches some size threshold, then the read buffer appears to receive the same message over and over again in fragments. I have now sunk more time into debugging it, and I have zeroes in on that threshold - it is exactly 77,500 bytes. If the total size of the data being sent over the connection is even one byte larger, it happens: The receiving end gets a fragmented data stream that contains the same information repeated 10-20 times, overlapping itself. I have no explanation for this, and I will email Ghassemi about it tomorrow. Right now, the only solution I can think of is to manually split the data into smaller chunks. The problem with that is if that particular number (77,500) is unique to my network, operating system, or machine. I will do further testing to figure this out as well.

##### June 23rd, 2020:

Much more progress today. I restructured my classes (yet again) to a state that I am happy with given that shared buffers problem I had yesterday. I was getting some really strange issues that I suspected were due to race conditions on the threads handling requests, so I created a new Request class that each handler method creates an instance of independently. Once a full request is created, it can be sent to the connection's outgoing buffer, which *should* now be thread-safe.  I also rewrote my logging class again so it can handle the same message being sent many times over, and not print it out on a new line every time. Instead it indicates how many times that message has been called to the right of it (super handy when you have a debugging message in a while loop). However there seems to be a problem with interpreters that don't accept the carriage return characters such as python IDLE... it gets messy. I don't know a way around that, but one idea is just to somehow determine whether the interpreter can handle the carriage return, then not print any messages that otherwise would overwrite a previous one if it can't. That's not super important, so it's not my top priority. Right now, I have a bug that is preventing the client from receiving the START request. That is my next task for tomorrow.

##### June 22nd, 2020:

Today was rather unproductive. First, I realized that I had a serious problem with my class structure: Each Connection object is meant to be created when a new connection is found, then run on a separate thread. However, each of those connections must have access to shared buffers so that, for example, one connection can stream in video and another can stream out that same video. As it was this morning, the Connection object were completely independent. The way I am going about fixing this is to pass in a reference to the parent object into the Connection's constructor. This way, each connection has access to the parent's attributes and can share them. This completely throws off the whole way in which I had designed the program to be modified, though, so I experimented with some other ways of re-writing it., but didn't get very far. Another concern that I just realized is that the EEG data will be streaming a much much higher rate than the image data, and I'm worried that my program won't be able to handle it. I looked into implementing parallel processes rather than just threads, but there are some issues with sharing data between processes that I don't know how to get around. I would like to find a better way of structuring my program first before trying anything like that, though. 

##### June 19th, 2020:

Alright so my basic idea is complete - I figured out how to have the server constantly look for new connections and run them on separate threads once they are established. This should theoretically allow any number of connections to the server. Most of my day, however, was spent trying to debug a strange problem where the server was seemingly grabbing too much data from the stream and causing it to lose its 'place' in the data (it would try to read the request line but found itself in the middle of random image data). I have no idea how this was happening. In fact, it shouldn't be - the server was only reading the amount of data specified in the header. I spent hours on this problem, and never quite found the cause but I do have my suspicions. I believe it may have had something to do with the way I had set up the non-blocking socket on the client side when sending data. Regardless, I decided to rewrite the buffer system to make absolutely sure that all the data is sent in the proper order. I implemented a pull-handle-push system with non-blocking read and write operations. The problem now is that doesn't play nicely with the way I want request handling to work - I will need to find a way to efficiently thread that as well. That will be my next task.

##### June 18th, 2020:

Alright so I had to take a step back today and figure out which methods I am going to use to both stream to a browser and stream from multiple devices. One option is to use non-blocking sockets paired with the python select module for multiplexing. The other option is to use blocking sockets but on different threads. After some experimenting in little testing programs, I have decided on the latter. This is for multiple reasons: I understood the threading method better in my testing, threading has a much broader scope of applications, and I need experience with it. The idea is that the server will run a thread that creates new sockets and accepts connections, then delegates a separate thread to handle the stream on that connection. I have begun to write a class to represent that connection, which will then be inherited by the ServerConnection and ClientConnection classes. I also rewrote some of my logging functions because I'm too stubborn to use the logging module. 

##### June 17th, 2020:

I have rewritten my program in terms of HTTP requests, and it is functioning just as it was before. I also added some more error catching functionality, including the ability to catch interrupts and disconnections and exit smoothly. The problem now is to get the browser video feed working, which is not as simple as I originally thought. There was some interactions with the threading and condition modules that I'm not sure about, so tomorrow I will be looking into how those work to make the live feed possible. I have moved the (previously StreamOutput) FrameBuffer class to lib.py, where both the server and client child classes can access it. Previously, only the client class was using it for the picam to write to, but I think I will need it as a buffer from which to stream to a web browser. 

##### June 16th, 2020:

I began writing a new class structure that somewhat imitates the functionality of python's HTTP sertver modules. I didn't understand them well enough to solve this problem so I'm building them from the ground up, albeit a bit simplified. I haven't finished yet, and I will also have to rewrite the data collection program to incorporate standard HTTP request syntax. I had created my own method of sending and parsing data, but it's not compatible with what I need to create the web application with a live feed. I will create a custom HTTP request method (much like I did in my original design with the Pi as the server host) that requests a connection to the server and streams data to it.

##### June 15th, 2020:

Dr. Ghassemi suggested I try to get a live video feed web app running on my working version of the program. The way I got it to work before was by setting up python's HTTPRequestHandler class and rewriting the method that is called for a GET request from a browser, then feeding the camera frames into the webpage. The problem is that the class I wrote for the new program is not based on that handler class, so it isn't contained within its server_forever() method, which is effectively an infinite loop that calls the appropriate request methods. My program does the same thing - I use an infinite loop that constantly receives the stream. I tried rewriting it with the handler class, but I ended up breaking what I had before, so I started over. I looked into the python threading library, and tried a couple things to see if I could get the web app running at the same time as receiving the stream, but I had a hard time getting them to communicate. I have another thing to try tomorrow - trying to rewrite python's handler class (or a simplified version of it) to incorporate my code. All it needs to do is handle GET requests, so it shouldn't be too bad.

##### June 12th, 2020:

I tried the program with the remote server, and was unable to make a connection. I then went back to the small "echo server" program I write awhile ago to make sure it still worked, and it did.  I also noticed that when running my video streaming program the ingestion client (using my laptop on the same network) was connecting through a different port every time, even through the server was hosted through port 8000 as I specified - I am fairly certain that this may be what was preventing the remote VM from connecting. I suspected that this was because the echo server project used the remote VM to host the server, to which the Pi connected and sent data to. With the video stream, the Pi was used as the host for the server instead. I wrote it this way so that either a web browser or the ingestion client could connect to the Pi from different machines and receive the stream. To test that this might be the problem, I rewrote the echo server program and switched the client/server roles, making the pi host the server to which the VM connected just like the streaming program. This version of the echo server failed, which was good because it meant that I had potentially found the problem. For the remainder of the day I rewrote the video streaming program (in /testing/live_video_stream) such that VM hosted the server, and the pi streamed directly to it. This version worked, to my immense delight. However, I was not able to figure out a way to get the web browser live feed working, which is why I wrote it in the original configuration to begin with.  

My next steps will be to teach myself how to use python's threading module so that I will be able to stream the image data to whatever neural networking program we will be using and run it concurrently (problem #4 I made note of on the 10th). I have never worked with parallel programming before so this will also be new to me.

##### June 11th, 2020: 

I accomplished more than I thought I would today. The Pi can now stream to a web browser and the client at the same time. I spent a lot of time digging through the source code of python's HTTPRequestHandler and figured out how to communicate a custom HTTPS request method ("CLIENTSTREAM") from the client to the pi, which allows it to stream separately to both a web browser and the client (this solved problem #1 I had yesterday). To the client, it first sends a 2-byte mini-header (referred to as the proto-header in the code) that denotes the length of the actual header. The actual header can potentially contain whatever information I want, but for now just sends the length of the image and the number of images sent from the pi thus far (solving problem #2 from yesterday). The TCP class I wrote yesterday is designed to parse this header style. Also, I discovered that the client is not lagging behind like I thought - the lag that I observed yesterday was from saving the images to the local machine, which is a time consuming process. When the images aren't saved, there is no problem. This solves problem #3. As of yet, I have not figured out problem #4, although it's not really urgent as of yet. Tomorrow I plan to try this code on the remote server, and update Dr. Ghassemi of my progress.

##### June 10th, 2020:

I have made a lot of progress! I wrote a class on the data-ingestion side that receives and parses the TCP data stream from the pi, including header data with any extra information I want to send along. Problems that I still need to solve:

1) Figure out how to send the image data through the connection without overwriting BaseHTTPRequestHandler.handle(). The problem is that, for some reason, I couldn't receive data sent to the server in order to start the stream. The handle() method starts immediately, but overwriting it means that the do_GET() never gets called, but that's what I use to stream to the browser. Essentially right now it's one or the other, and I want to be able to stream to a browser AND to the data-ingestion server.

2) Figure out how to track and report how many images were sent on the pi-side (I can already track how many were received on the server side). This would allow me to asses how many of the images I'm losing between the two.

3) Need to figure out if the server is able to keep up with the pi. I suspect that there may be a back-log of image data that isn't being processed, which would result in an increasingly out-of-date image feed. If that is the case, I would need to be able to detect that from the server and intentionally skip frames to keep up. That or just reduce the framerate on the pi until they are synchronized, but that is less adaptable to different hardware.

4) Need to figure out how to send the received image stream to a program that can process it (like the neural network). Will I need to use Threading in order to do these simultaneously? I don't have enough experience with this kind of thing to know.

##### June 9th, 2020:    

I have finished my draft of the annotated bibliography with 12 sources. I have also made some progress with the video streaming - I learned a lot today, actually. I have been experimenting with TCP streaming and parsing data sent back and forth between a server and client. I was able to modify the code from the picam documentation to additionally provide a handler for direct socket connection requests, and send individual frames through the connection. I am currently struggling with syncronizing the connections so that I know exactly what it is that the server is receiving, along with being able to send additional information along with the frames. Online research has suggested that sending a header along with the main payload is the preferred method, but I can't seem to find what the conventional method is - I will update Dr. Ghassemi and ask about it.

##### June 8th, 2020: 

I began looking for sources for the annotated bibliography, and found a number that I thought were relevant. I have read through 15 papers today, and wrote annotations for 6 of them as of yet. I will finish the rest tomorrow and notify Dr. Ghassemi - this will be my first official research paper so no doubt he will have some feedback. I did not work on video streaming today - I would like to make sure I have time to finalize the bibliography by Friday (June 12th).

##### June 5th, 2020:   

I tried a new method of setting up live streaming from the Raspberry Pi Camera documentation (data-hub/testing/live_video_stream/client_1.py). This method was successful, and had much less latency than the method used by the baby monitor guide. I was still unable to access the stream remotetly from the virtual machine, however this time I was able to access from a machine on the same network. I tried using port 80 instead, as well as moving one or both ends onto my phone's wifi hotspot to no effect. This would indicate that the error is on the end of the Raspberry Pi (An hypothesis that Dr. Ghassemi also posed after I was unable to establish an FTPS link). Even so, I think this new approach is much cleaner as it required no setup on the end of the machine reading the data. Of course once we want to analyze the data, I will have to figure out how to connect to the signal being sent rather than just viewing it through a browser. I experimented with this by trying different ways in which to parse the stream data. I was able to convert individual images into numpy arrays that could then be fed into a neural network, but I was unable to do it "live." It was only after they had all been transmitted. My next step is to try and do that in "batches." If I were able to make these batches shorter and shorter, I believe it would constitute being "live," however I have no idea whether it would be efficient enough to consider.

##### June 4th, 2020: 

No luck on the FTPS. I tried Filezilla, which claims to support SSL, but all documentation on the matter is extremelt outdated and I simply could not figure out how to do it on the most recent installtion. SSL/TLS is nowhere to be seen in the filezilla client. Dr. Ghassemi opened ports 21 and 990 on his end, but it did not seem to help. Here are some examples of what I tried that actuallt produced a result, but simply failed to connect:

> $lftp -c 'open -e "set ftps:initial-prot ""; set ftp:ssl-force true; set ftp:ssl-protect-data true; put newfile.txt; " -u "USERNAME","PASSWORD" ftps://HOSTNAME:990 '

> $lftp -c 'open -e "set ftps:initial-prot ""; set ftp:ssl-force true; set ftp:ssl-protect-data true; put newfile.txt; " ftps://ubuntu@3.136.140.191:990 '

(those two commands required GnuTLS)

I also followed this guide: http://www.yourownlinux.com/2015/06/how-to-set-up-ftps-ftp-over-ssl-server-on-linux.html

Dr. Ghassemi reminded me of something I wanted to try with the testing programs I wrote last week, so I've started experimenting with getting live video data from the Pi to the server over a TCP connection. I have been having some trouble with the PIL library, and after scouring the documentation I've fixed a few of those issues. I am making slow progress, but progress nonetheless. I also contacted MSGC and the responded saying that they require no action on my part.

##### June 3rd, 2020:  

I was having a number of connection issues, and it turns out that I actually did not succeed in getting local streaming to work. I had assumed that I did, as I was able to connect to the Picam from the Pi's browser. However, I am unable to connect from my laptop on the same network, or even when both are on my phone's hotspot (as suggested by Dr. Ghassemi as a test). I could not ping the Pi's hostname either, although I can still ping its public ip. My attempts to find the cause of the issue thus far have failed. I asked Dr. Ghassemi for advice, and I was instructed to test the following:
1) Whether the Pi can connect to the internet: Yes
2) Whether the Pi can SSH into the remote virtual machine: Yes
3) Whether the Pi can send files to the VM via SFTP: Yes (I was successful in using the linux sftp command)
4) Whether the Pi can send files to the VM via FTPS: Not yet. 

I have tried unsuccessfully to use FTPS using linux's lftp command, however I believe there is a significant possibility that I simply have not done it right. I did not know there was a difference between the two before today. Online guides seem to indicate that this process is much more involved than with SFTP, so I will need to try more methods. In other news, I have received a response from MSGC about my last inquiry, but they haven't mentioned everything being "finalized." If I do not hear from them tomorrow morning, I will contact them again.

##### June 2nd, 2020:

I have continued the setup process, and was able to get streaming to work on the local network. I ran into a few problems but I was able to solve them with some researching. Next step is to set up remote streaming to the server. Even then, I am not sure how I will download the video data for storage/analysis, as I reallt don't know how the image data is processed. I may have to learn a lot more to be able to do that. I have still not heard back from MSGC about my questions.

##### June 1st, 2020: 

Dr. Ghassemi discovered the connection issue that I have been having, and I can now access the data hub web application remotely - Yay! It was because the ports (8000 and 5000) needed to be forwarded from the his end, not just mine. I feel vindicated in my identification of the problem, at least. Even so, the redirection loop I encountered last time was still present, which I suspected might be the case. Ben has not responded to my last three emails, and I am not optimistic. Dr. Ghassemi has encouraged me to try my own method if video streaming using the data-hub (and another similar project he found about building a baby monitor) as a reference. I have begun following the instruction for the baby monitor project, but I am struggling to decipher the bash script that it is having me write. It's not really necessary, but I would like to learn as much as I can in the process. Our project has been accepted by the MSGC for funding - I have filled out the requested paperwork and am waiting to hear back about a few questions I had.

##### May 29th, 2020: 

The Netsurf browser could not be found for install by apt, and once I installed it from source, it refused to be recognized by the command like. Same deal with Qupzilla, and I couldn't install midori at all.. Finally I was able to install Epiphany (Gnome Web) through Snap:
$sudo apt install snapd
$sudo snap install epiphany

Then I was able to run epiphany remotely and view it's graphic environment on my laptop. Success! However when I tried to run the data-hub web application (using sh run.sh, as I have successully done many times), I got the following error: "ImportError: libmysqlclient.so.20: cannot open shared object file: No such file or directory." Sigh. Online solutions all seemed to suggest reinstalling the package. I uninstalled mysql, mysql-server, and mysql-client, then I went back to the team's seup guide. When I ran the $wget command (which worked just fine before), I got the following error: "https://repo.mysql.com/mysql-apt-config_0.8.13-1_all.deb: Scheme missing." This is the exact same command I used during the initial setup. Online solutions suggested using the "-A" flag with $wget" I did this, and was able to install the mysql packages again. This did not solve the original error. I navigated to the directory specified in the error (~/data-hub/venv/lib/python3.6/site-packages/MySQLdb), and it apparently has not been modified since last week when I installed it. I was completelty mystified for couple hours, but all it took was to run $sudo apt-get install libmysqlclient20. How this library was deleted, I do not know. The server now runs properly (as far as I can tell). I opened up epiphany and navigated to 172.26.0.249:5000, and was greeted with the login page. YAY!
I went to the registration page and created an account, username Aven, password "zitzelbe_password." After logging in, it took me to a page that had a button to start a new session, but upon clicking it, I was redirected to the login page. The process repeated. I checked the user table in the database, and my account was indeed entered. I will send another email to Ben.

##### May 28th, 2020: 

Today I wanted to try to access the remote server with a graphical interface so that I could attempt to visit the web application on its local network. This would allow me to continue with the project I was assigned, even though I have not been able to resolve the networking issues. I made numerous attempts to access the server with a remote desktop utility (X2GO) with SSH, but I can't seem to get through. I can get through via CLI just fine, but it seems X2Go just isn't having it. According to everything I know (which isn't much, granted) it should be working, however I get an ambiguous authentication error that no amount of googling has resolved. I spent way too much time trying to debug this, but I had to cut my losses and give up eventually. However, I did manage to get an Xserver client going and was able to verify that I can access some graphical interfaces on the server from my laptop. With this, I will see if I can get a lightweight web browser to access the web application on the server's network. It's not ideal, but it should get the result I want.

##### May 27th, 2020: 

I've been trying to reach the website with UFW running with no success. I first did a trial with UFW on port 80 to make sure it was allowing the port properly, then tried with 8000: With UFW enabled, only default settings, my test program was not able to get through (as expected). After running "sudo ufw allow 80", my test program was able to get through (as expected). Then I set my test program to use port 8000, and it was not able to get through (as expected). After running "sudo ufw allow 8000", my test program was not able to get through (unexpected). It seems that using UFW did not have an effect on the connection issue with port 8000. Also there's something I noticed - Gunicorn says that it's listening at *.5000. Where as everywhere else in the code, port 8000 is specified. I tried allowing port 5000 as well, but that didn't help. I have been consulting with Ben, and he cannot say why this is happeneing. He said he may have to look at the code himself.

##### May 26th, 2020: 

Today I've managed to write a program that is able to sent a stream of images captured from the pi to the remote server. I have moved all of my "experimenting" scripts to the directory "testing" in the root directory. I wasn't able to save the images on the server, but I did confirm that they were being sent. I've been trying to figure out a way to view the images on the server, but I have been having a lot of trouble with X server authentication and couldn't get it to work. I see why a web app is the preferred method. My goal with this was to write my own program to stream video to the capstone's database and use their web app to view it, however I have absolutely no idea how long it would take me to figure that out. I've learned a lot in the last two days, but not quite enough to do it from scratch I'm afraid. Ben has been replying to my emails and given me some useful information, however. He said that the team used UFW to manage the ports they needed to go through. My goal for tomorrow will be to follow the steps he layed out and see where it takes me - looks promising so far.

##### May 25th, 2020:

I was able to confirm that the data from the collection layer is not being sent to the ingestion layer. After fiddling around with the python socket library for awhile, I was able to write a program that can communicate between the raspberry pi and the remote server. They are in the root directory: testing_server.py and testing_client.py (EDIT June 4th - moved to /testing/basic: client.py, server.py). I noticed, however, that I was not able to use ports other than the default ports, so I used TCP port 80 for my script. I suspect that one of the reasons the data-hub is unable to connect is because they may be using a port that is unavailable. I made an attempt to change the port they were using (8000 for video, it looks like), however it did not help. There might be a particular reason for using port 8000, in which case I will need to contact the IT department and ask if they would forward that port on the MSU network - they would undoubtedly require confirmation from Dr. Ghassemi. I have sent another email to the team asking about this. One team member has replied, saying only that he is unable to help.

##### May 22nd, 2020:

Still no response from the team. I was able to track down some of the errors to the pyaudio library, and some others seem to be related to the Sense HAT software. I disabled the audio and Sense HAT programs, leaving only video, and was able to get rid of all the errors I was seeing. However I still don't know if that ultimately affects anything as there is no instruction on what should happen after I run the main programs, as I mentioned in my last email to the team. I would like to test one thing at a time - my next step will be to see if the data-collection layer is working properly.

##### May 21st, 2020:

I have successfully completed the steps in the instructions to set up the data-ingestion layer on the Lightsail instance, but it seems to fail to start. I have no idea what it should look like when it does start, actually. I also was able to finally get the ffmpeg configuration to run without errors on the Pi by installing LAME from sourceforge (https://sourceforge.net/projects/lame/files/lame/3.100/). I haven't heard back yet from the team, and I sent them another email. I mentioned that I thought the extra installs I had to perform should be in the instructions, and asked about the data-ingestion layer problems. I don't think I can proceed further without help from the team, but I will continue to look for solutions. And by that I mean I will read documentation and google error messages until the stackoverflow logo is burned into my retinas.

##### May 20th, 2020:

After receiving a solution from one of the previous team members, I continued with the rest of the setup. Very few steps given in the instructions worked as intended, and numerous packages had to be installed in alternative ways. I emailed the previous group to ask whether my process was adequate or if it might cause further problems. I also asked whether a Sense HAT was necessary for the program to work. Dr. Ghassemi created an AWS Lightsail instance on which I will initialize the data-ingenstion layer - I'll begin working on that tomorrow.

##### May 19th, 2020:

I received the go-ahead from Dr. Ayres to submit our project proposal to the MSGC, along with some additional specifications which I am currently working on. I have also received the raspberry pi. I initially had some issues getting it to recognize MSU's network, but I eventually got it to work via ethernet. I also encountered a problem causing the file system to crash after restarting the system, but I believe I have it working after a full-upgrade. I now have another problem installing MySQL due to my Raspbian Buster installation not being supported. I have found no solution and will contact Dr. Ghassemi.

##### May 18th, 2020:

I have not received the raspberry pi yet. I have been going over the code given to me by Dr. Ghassemi.

##### May 16th, 2020:

I received the first amazon package - the keyboard and mouse. The raspberry pi and camera was due to arrive, but appears to have been delayed. 

##### May 15th, 2020: 

I received the email from Dr. Ayres, and submitted the proposal to the D2L Dropbox as instructed.

##### May 14th, 2020:

I continued work on the application. Dr. Ghassemi gave me feedback and revised it, after which I emailed Dr. Ayres for information on how to apply.

##### May 13th, 2020:

Dr. Ghassemi ordered a bundle from OpenBCI which includes the items I listed plus some wet electrode equipment. He also informed me of an opportunity for us to apply for the NASA Michigan Space Grant Consortium. I began work on the application, and found an interesting paper from 2018 about similar research (https://ieeexplore.ieee.org/document/8396819).

##### May 12th, 2020:

Sent an application to join the OpenBCI forums, and emailed OpenBCI asking whether their software is capable of simultaneously steaming over 16 channels of data at once. I received a response to both today. Evidently the answer to my quetion was definitely 'No', but I was accepted into their forum. 
I suggested to Dr. Ghassemi that we still purchase the 16 channel head set, as it would give us the option of both monitoring the brain fully, or various muscle groups with fewer brain channels. 
Received an update from Amazon saying that the keyboard and mouse has been shipped.

##### May 11th, 2020:

Discussed amazon purchases with Dr. Ghassemi, and placed an order for a Raspberry Pi, an accompanying camera, and a cheap keyboard & mouse, totalling $153.60.
Researched necessary parts for the project from OpenBCI, and compiled a preliminary list of possible purchases.
Emailed OpenBCI with questions, and will modify the list before purchase if necessary.

Preliminary OpenBCI Purchase List:
* Ultracortex Mark IV EEG Headset (Unassembled, Large, 16 channels): $700
* Cyton + Daisy Biosensing Board  (16 channels): $950
* Pulse Sensor: $25
* Dry EEG Comb Electrodes (30/pack): $30
* EMG/ECG Foam Solid Gel Electrodes (30/pack) (2x): $13 x2
* EMG/ECG Snap Electrode Cables: (x2) $40 x2
* Total cost: ~$1,811
