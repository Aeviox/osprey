<html>
    <head>
        <title>Audio Live Streaming Test</title>
        <script>

        async function init() {
            // Since most MP3 encoders store the gapless metadata in binary, we'll need a
            // method for turning bytes into integers.  Note: This doesn't work for values
            // larger than 2^30 since we'll overflow the signed integer type when shifting.
            function ReadInt(buffer) {
                var result = buffer.charCodeAt(0);
                for (var i = 1; i < buffer.length; ++i) {
                    result <<= 8;
                    result += buffer.charCodeAt(i);
                }
                return result;
            }

            var SECONDS_PER_SAMPLE = 1 / 44100

            function ParseGaplessData(arrayBuffer) {
                // Gapless data is generally within the first 512 bytes, so limit parsing.
                var byteStr = new TextDecoder().decode(arrayBuffer.slice(0, 512));

                var frontPadding = 0, endPadding = 0, realSamples = 0;

                var iTunesDataIndex = byteStr.indexOf('iTunSMPB');
                if (iTunesDataIndex != -1) {
                    var frontPaddingIndex = iTunesDataIndex + 34;
                    frontPadding = parseInt(byteStr.substr(frontPaddingIndex, 8), 16);

                    var endPaddingIndex = frontPaddingIndex + 9;
                    endPadding = parseInt(byteStr.substr(endPaddingIndex, 8), 16);

                    var sampleCountIndex = endPaddingIndex + 9;
                    realSamples = parseInt(byteStr.substr(sampleCountIndex, 16), 16);
                }


                var xingDataIndex = byteStr.indexOf('Xing');
                if (xingDataIndex == -1) xingDataIndex = byteStr.indexOf('Info');
                if (xingDataIndex != -1) {
                    // See section 2.3.1 in the link above for the specifics on parsing the Xing
                    // frame count.
                    var frameCountIndex = xingDataIndex + 8;
                    var frameCount = ReadInt(byteStr.substr(frameCountIndex, 4));

                    // For Layer3 Version 1 and Layer2 there are 1152 samples per frame.  See
                    // section 2.1.5 in the link above for more details.
                    var paddedSamples = frameCount * 1152;

                    xingDataIndex = byteStr.indexOf('LAME');
                    if (xingDataIndex == -1) xingDataIndex = byteStr.indexOf('Lavf');
                    if (xingDataIndex != -1) {
                    // See http://gabriel.mp3-tech.org/mp3infotag.html#delays for details of
                    // how this information is encoded and parsed.
                    var gaplessDataIndex = xingDataIndex + 21;
                    var gaplessBits = ReadInt(byteStr.substr(gaplessDataIndex, 3));

                    // Upper 12 bits are the front padding, lower are the end padding.
                    frontPadding = gaplessBits >> 12;
                    endPadding = gaplessBits & 0xFFF;
                    }

                    realSamples = paddedSamples - (frontPadding + endPadding);
                }

                return {
                    audioDuration: realSamples * SECONDS_PER_SAMPLE,
                    frontPaddingDuration: frontPadding * SECONDS_PER_SAMPLE
                };
            }


            function sleep(ms) {
                return new Promise(resolve => setTimeout(resolve, ms));
            }
            await sleep(5000);

            var audio = document.getElementById("audio");
            var mediaSource = new MediaSource();
            var SEGMENTS = 5;

            mediaSource.addEventListener('sourceopen', function() {
                var sourceBuffer = mediaSource.addSourceBuffer('audio/mpeg');

                function onAudioLoaded(data, index) {
                    // Parsing gapless metadata is unfortunately non trivial and a bit messy, so
                    // we'll glaze over it here; see the appendix for details.
                    // ParseGaplessData() will return a dictionary with two elements:
                    //
                    //    audioDuration: Duration in seconds of all non-padding audio.
                    //    frontPaddingDuration: Duration in seconds of the front padding.
                    //
                    var gaplessMetadata = ParseGaplessData(data);

                    // Each appended segment must be appended relative to the next.  To avoid any
                    // overlaps, we'll use the end timestamp of the last append as the starting
                    // point for our next append or zero if we haven't appended anything yet.
                    var appendTime = index > 0 ? sourceBuffer.buffered.end(0) : 0;

                    // Simply put, an append window allows you to trim off audio (or video) frames
                    // which fall outside of a specified time range.  Here, we'll use the end of
                    // our last append as the start of our append window and the end of the real
                    // audio data for this segment as the end of our append window.
                    sourceBuffer.appendWindowStart = appendTime;
                    sourceBuffer.appendWindowEnd = appendTime + gaplessMetadata.audioDuration;

                    // The timestampOffset field essentially tells MediaSource where in the media
                    // timeline the data given to appendBuffer() should be placed.  I.e., if the
                    // timestampOffset is 1 second, the appended data will start 1 second into
                    // playback.
                    //
                    // MediaSource requires that the media timeline starts from time zero, so we
                    // need to ensure that the data left after filtering by the append window
                    // starts at time zero.  We'll do this by shifting all of the padding we want
                    // to discard before our append time (and thus, before our append window).
                    sourceBuffer.timestampOffset =
                        appendTime - gaplessMetadata.frontPaddingDuration;

                    // When appendBuffer() completes, it will fire an updateend event signaling
                    // that it's okay to append another segment of media.  Here, we'll chain the
                    // append for the next segment to the completion of our current append.
                    /*
                    if (index == 0) {
                        sourceBuffer.addEventListener('updateend', function() {
                        if (++index < SEGMENTS) {
                            GET('sintel/sintel_' + index + '.mp3',
                                function(data) { onAudioLoaded(data, index); });
                        } else {
                            // We've loaded all available segments, so tell MediaSource there are no
                            // more buffers which will be appended.
                            mediaSource.endOfStream();
                            URL.revokeObjectURL(audio.src);
                        }
                        });
                    }
                    */

                    // appendBuffer() will now use the timestamp offset and append window settings
                    // to filter and timestamp the data we're appending.
                    //
                    // Note: While this demo uses very little memory, more complex use cases need
                    // to be careful about memory usage or garbage collection may remove ranges of
                    // media in unexpected places.
                    sourceBuffer.appendBuffer(data);
                }

                var base_request_url = "{{url_for('filefetchaudio', filename='REPLACEME')}}"
                console.log("Base request url: ");
                console.log(base_request_url);
                var current_segment_number = 1;
                function requestSegment(segment_number) {
                    var segment_request_url = base_request_url.replace("REPLACEME", "mp3_" + segment_number + ".mp3");

                    fetch(segment_request_url).then(response => {
                        return response.arrayBuffer();
                    }).then(function(buffer) {
                        if (buffer.byteLength > 1000){
                            onAudioLoaded(buffer, segment_number - 1);
                        }
                        else {
                            requestSegment(segment_number);
                        }
                    });
                }

                // gets called when we're done updating the SourceBuffer (after appending a segment)
                sourceBuffer.addEventListener('updateend', function() {
                    current_segment_number = current_segment_number + 1;

                    //if (current_segment_number <= 7) {
                    requestSegment(current_segment_number); 
                    //}
                });

                requestSegment(1); // start requesting segments
                
                // Retrieve an audio segment via XHR.  For simplicity, we're retrieving the
                // entire segment at once, but we could also retrieve it in chunks and append
                // each chunk separately.  MSE will take care of assembling the pieces.
                /*
                var audio_request_url = "{{url_for('filefetchaudio', filename='mp3_1.mp3')}}"
                console.log("full request url: " + audio_request_url);
                //GET(audio_request_url, function(data) { onAudioLoaded(data, 0); } );
                fetch(audio_request_url).then(response => {
                    return response.arrayBuffer();
                }).then(function(buffer) {
                    onAudioLoaded(buffer, 0);
                });
                */

            });

            audio.src = URL.createObjectURL(mediaSource);
          }

          document.addEventListener('DOMContentLoaded', init);
        </script>
    </head>
    <body>
        <audio id="audio" controls autoplay>
            Your browser doesn't support HTML audio element.
        </audio>
    </body>
</html>



